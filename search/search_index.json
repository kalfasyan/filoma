{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"filoma","text":"<p>Fast, multi-backend directory analysis &amp; file/image profiling with a tiny API surface.</p> <pre><code>from filoma import probe, probe_to_df, probe_file\n\nfilo = probe_file('README.md')   # single file metadata\nprint(filo.size)\n\nanalysis = probe('.')            # directory summary\nanalysis.print_summary()\n\ndf = probe_to_df('.')            # filoma.DataFrame wrapper containing a Polars DataFrame of paths\ndf.add_path_components()         # add columns for e.g. parent, stem, suffix\ndf.add_file_stats_cols()         # add file stats columns (like size, mtime, etc.)\ndf.add_depth_col()               # add depth column (file nesting level)\ndf.add_filename_features()       # instance method: discover filename tokens (see Demo)\n\n# ML-ready splits\ntrain, val, test = df.split_data(seed=42, train_val_test=(70,20,10), feature='XYZ')\n</code></pre>"},{"location":"#why-filoma","title":"Why filoma?","text":"<ul> <li>Automatic speed: Rust / fd / Python backend selection</li> <li>DataFrame-first: Direct Polars integration + enrichment helpers</li> <li>One-liners: <code>probe</code>, <code>probe_to_df</code>, <code>probe_file</code>, <code>probe_image</code></li> <li>Deterministic ML splits: Group-aware, leakage-resistant</li> <li>Extensible: Low-level profilers still accessible</li> </ul>"},{"location":"#start-here","title":"Start here","text":"<p>Best place to begin is the Demo notebook (see the <code>Demo</code> page in the docs) 1. Read the Quickstart 2. Learn Core Concepts 3. Explore the DataFrame Workflow 4. Browse recipes in the Cookbook 5. Dive into the API Reference </p>"},{"location":"#common-tasks-tldr","title":"Common Tasks (TL;DR)","text":"Task Snippet Scan dir <code>probe('.')</code> DataFrame <code>probe_to_df('.')</code> Largest N files see Cookbook Filter extension <code>df.filter_by_extension('.py')</code> Add stats <code>df.add_file_stats_cols()</code> ML split <code>ml.split_data(df)</code>"},{"location":"#installation-uv","title":"Installation (uv)","text":"<pre><code>uv add filoma\n</code></pre> <p>Want performance? Install Rust (for fastest backend) or fd.</p> <p>Need something else? Check the Cookbook or jump to the API.</p>"},{"location":"advanced-usage/","title":"Advanced Usage","text":""},{"location":"advanced-usage/#profiler-quick-reference","title":"Profiler Quick Reference","text":"<p>This section shows the three main profilers (<code>DirectoryProfiler</code>, <code>FileProfiler</code>, <code>ImageProfiler</code>), short examples, and the most important constructor/probe arguments with notes about which backend(s) honor them.</p> <p>DirectoryProfiler \u2014 high-level directory analysis (counts, extensions, empty folders, timing)</p> <pre><code>from filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\n\nprofiler = DirectoryProfiler(DirectoryProfilerConfig(\n    search_backend='auto',   # 'auto'|'rust'|'fd'|'python'\n    use_async=False,         # Rust async scanner (network-optimized)\n    build_dataframe=True,    # collect paths into a DataFrame (Polars)\n    show_progress=True,\n))\nresult = profiler.probe('.')\nprofiler.print_summary(result)\n</code></pre> <p>Key arguments (what they do &amp; which backend(s) support them): - <code>search_backend</code> \u2014 choose preferred backend. Supported values: <code>rust</code>, <code>fd</code>, <code>python</code>, <code>auto</code> (default). All profilers use this to decide implementation. - <code>use_async</code> \u2014 enable Rust async scanner (when <code>search_backend</code> allows Rust and tokio-enabled build). Backend: Rust (async only). - <code>use_parallel</code> / <code>parallel_threshold</code> \u2014 prefer parallel Rust scanning when available; adjusts parallel decision heuristics. Backend: Rust (parallel only). - <code>build_dataframe</code> \u2014 collect discovered paths into a Polars DataFrame for downstream analysis. Backend: works with any discovery backend; building is done in Python when using Rust/fd. - <code>max_depth</code> \u2014 limit recursion depth. Honored by all backends. - <code>follow_links</code> \u2014 whether to follow symlinks. Backend support: Rust (explicit flag), fd (discovery flag), Python (depends on os.walk behaviour but passed through by the profiler). - <code>search_hidden</code> \u2014 include hidden files/dirs. Backend support: Rust, fd, Python (profiler passes preference). - <code>no_ignore</code> \u2014 ignore .gitignore and similar ignore files (fd/Rust option). Backend support: fd, Rust. - <code>threads</code> \u2014 number of threads forwarded to <code>fd</code> (if used). Backend: fd. - <code>fast_path_only</code> \u2014 Rust-only mode to skip expensive metadata collection and only gather file paths (useful for very large trees).</p> <p>Notes: when <code>search_backend='auto'</code> filoma chooses the most efficient backend available and applies fd-like defaults (follow hidden, do not respect ignore files) unless you explicitly override flags.</p> <p>FileProfiler \u2014 probe a single file for metadata and optional hash</p> <pre><code>from filoma.files import FileProfiler\n\nfilo = FileProfiler().probe('README.md', compute_hash=False)\nprint(filo.to_dict())\n</code></pre> <p>Key arguments: - <code>compute_hash</code> (bool) \u2014 compute content hash (sha256). Supported by: FileProfiler (Python implementation) and internal Rust file profilers when enabled; computing a hash may be slower for large files. - <code>follow_links</code> \u2014 when probing a path that is a symlink, whether to resolve it. Supported by: FileProfiler (behavior depends on implementation; FileProfiler forwards to low-level routines).</p> <p>ImageProfiler \u2014 high-level entry point that dispatches to specialized image profilers (PNG, TIF, NPY, ZARR or in-memory numpy arrays)</p> <pre><code>from filoma.images import ImageProfiler\n\n# File path\nimg_report = ImageProfiler().probe('images/logo.png')\n\n# Or pass a numpy array directly\nimport numpy as np\narr = np.zeros((64,64), dtype=np.uint8)\nimg_report2 = ImageProfiler().probe(arr)\n</code></pre> <p>Key arguments &amp; notes: - <code>path</code> or numpy array input \u2014 ImageProfiler accepts either a path-like (dispatches by extension) or an ndarray directly. - <code>compute_stats</code> \u2014 compute pixel-level statistics (min/max/mean/std) and simple histograms. Supported by: image profilers implemented in Python; some heavy operations may call compiled helpers. - <code>load_lazy</code> / <code>fast</code> \u2014 some backends/profilers may provide a fast/low-memory mode for very large images (TIF/ZARR). Backend support: varies by specific image profiler (Tif/Zarr profilers often support chunked/lazy reading).</p> <p>Assumptions &amp; compatibility - The doc lists commonly available options; exact flag names and behavior are implemented in the specific profiler classes. When unspecified, <code>DirectoryProfiler</code> attempts to forward preferences to the chosen backend (<code>rust</code>/<code>fd</code>/<code>python</code>). - If you'd like, I can add a small matrix table (argument vs backend) documenting the precise per-backend support for each flag.</p>"},{"location":"advanced-usage/#smart-file-discovery","title":"Smart File Discovery","text":""},{"location":"advanced-usage/#fdfinder-interface","title":"FdFinder Interface","text":"<pre><code>from filoma.directories import FdFinder\n\n# Create searcher (automatically uses fd if available)\nsearcher = FdFinder()\n\n# Find Python files\npython_files = searcher.find_files(pattern=r\"\\.py$\", path=\".\", max_depth=3)\nprint(f\"Found {len(python_files)} Python files\")\n\n# Find files by extension\ncode_files = searcher.find_by_extension(['py', 'rs', 'js'], path=\".\")\nimage_files = searcher.find_by_extension(['.jpg', '.png', '.tif'], path=\".\")\n\n# Find directories\ntest_dirs = searcher.find_directories(pattern=\"test\", max_depth=2)\n</code></pre>"},{"location":"advanced-usage/#advanced-search-patterns","title":"Advanced Search Patterns","text":"<pre><code># Search with glob patterns\nconfig_files = searcher.find_files(pattern=\"*.config.*\", use_glob=True)\n\n# Search hidden files\nhidden_files = searcher.find_files(pattern=\".*\", hidden=True)\n\n# Case-insensitive search\nreadme_files = searcher.find_files(pattern=\"readme\", case_sensitive=False)\n\n# Recent files (if fd supports time filters)\nrecent_files = searcher.find_recent_files(changed_within=\"1d\", path=\"/logs\")\n\n# Large files\nlarge_files = searcher.find_large_files(min_size=\"1M\", path=\"/data\")\n</code></pre>"},{"location":"advanced-usage/#direct-fd-integration","title":"Direct fd Integration","text":"<pre><code>from filoma.core import FdIntegration\n\n# Low-level fd access\nfd = FdIntegration()\nif fd.is_available():\n    print(f\"fd version: {fd.get_version()}\")\n\n    # Regex pattern search\n    py_files = fd.find(pattern=r\"\\.py$\", path=\"/src\", max_depth=2)\n\n    # Glob pattern search  \n    config_files = fd.find(pattern=\"*.json\", use_glob=True, max_results=10)\n\n    # Files only\n    files = fd.find(file_types=[\"f\"], max_depth=3)\n\n    # Directories only\n    dirs = fd.find(file_types=[\"d\"], search_hidden=True)\n</code></pre>"},{"location":"advanced-usage/#dataframe-analysis","title":"DataFrame Analysis","text":""},{"location":"advanced-usage/#basic-dataframe-usage","title":"Basic DataFrame Usage","text":"<pre><code>from filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\n\n# Enable DataFrame building for advanced analysis\nprofiler = DirectoryProfiler(DirectoryProfilerConfig(build_dataframe=True))\nresult = profiler.probe(\".\")\n\n# Get the DataFrame with all file paths\ndf = profiler.get_dataframe(result)\nprint(f\"Found {len(df)} paths\")\n\n# Add path components (parent, name, stem, suffix)\ndf_enhanced = df.add_path_components()\nprint(df_enhanced.head())\n</code></pre>"},{"location":"advanced-usage/#advanced-dataframe-operations","title":"Advanced DataFrame Operations","text":"<pre><code># Filter by file type\npython_files = df.filter_by_extension('.py')\nimage_files = df.filter_by_extension(['.jpg', '.png', '.tif'])\n\n# Group and probe\nextension_counts = df.group_by_extension()\ndirectory_counts = df.group_by_directory()\n\n# Add file statistics\ndf = df.add_file_stats_cols()  # size, timestamps, etc.\n\n# Add depth information\ndf = df.add_depth_col()\n\n# Export for further analysis\ndf.save_csv(\"file_analysis.csv\")\ndf.save_parquet(\"file_analysis.parquet\")\n</code></pre>"},{"location":"advanced-usage/#dataframe-api-reference","title":"DataFrame API Reference","text":"<pre><code># Path manipulation\ndf.add_path_components()     # Add parent, name, stem, suffix columns\ndf.add_depth_col()        # Add directory depth column\ndf.add_file_stats_cols()          # Add size, timestamps, file type info\n\n# Filtering\ndf.filter_by_extension('.py')              # Filter by single extension\ndf.filter_by_extension(['.jpg', '.png'])   # Filter by multiple extensions\ndf.filter_by_pattern('test')               # Filter by path pattern\n\n# Analysis\ndf.group_by_extension()      # Group and count by file extension\ndf.group_by_directory()      # Group and count by parent directory\n\n# Export\ndf.save_csv(\"analysis.csv\")           # Export to CSV\ndf.save_parquet(\"analysis.parquet\")   # Export to Parquet\ndf.to_polars()                        # Get underlying Polars DataFrame\n</code></pre>"},{"location":"advanced-usage/#backend-control-comparison","title":"Backend Control &amp; Comparison","text":"<pre><code>from filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\nimport time\n\n# Test all available backends\nbackends = [\"python\", \"rust\", \"fd\"]\nresults = {}\n\nfor backend in backends:\n    try:\n    profiler = DirectoryProfiler(DirectoryProfilerConfig(search_backend=backend))\n        # Check if the specific backend is available\n        available = ((backend == \"rust\" and profiler.is_rust_available()) or\n                    (backend == \"fd\" and profiler.is_fd_available()) or\n                    (backend == \"python\"))  # Python always available\n        if available:\n            start = time.time()\n            result = profiler.probe(\"/test/directory\")\n            elapsed = time.time() - start\n            results[backend] = {\n                'time': elapsed,\n                'files': result['summary']['total_files'],\n                'available': True\n            }\n            print(f\"\u2705 {backend}: {elapsed:.3f}s - {result['summary']['total_files']} files\")\n        else:\n            print(f\"\u274c {backend}: Not available\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f {backend}: Error - {e}\")\n\n# Find the fastest\nif results:\n    fastest = min(results.keys(), key=lambda k: results[k]['time'])\n    print(f\"\ud83c\udfc6 Fastest backend: {fastest}\")\n</code></pre>"},{"location":"advanced-usage/#manual-backend-selection","title":"Manual Backend Selection","text":"<pre><code># Force specific backends\nprofiler_python = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"python\", show_progress=False))\nprofiler_rust = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"rust\", show_progress=False))  \nprofiler_fd = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"fd\", show_progress=False))\n\n# Disable progress for pure benchmarking\nprofiler_benchmark = DirectoryProfiler(DirectoryProfilerConfig(show_progress=False, fast_path_only=True))\n\n# Check which backend is actually being used\nprint(f\"Python backend available: True\")  # Always available\nprint(f\"Rust backend available: {profiler_rust.is_rust_available()}\")\nprint(f\"fd backend available: {profiler_fd.is_fd_available()}\")\n</code></pre>"},{"location":"advanced-usage/#complex-fd-search-patterns","title":"Complex fd Search Patterns","text":"<pre><code>from filoma.core import FdIntegration\n\nfd = FdIntegration()\n\nif fd.is_available():\n    # Complex regex patterns\n    test_files = fd.find(\n        pattern=r\"test.*\\.py$\",\n        path=\"/src\",\n        max_depth=3,\n        case_sensitive=False\n    )\n\n    # Glob patterns with exclusions\n    source_files = fd.find(\n        pattern=\"*.{py,rs,js}\",\n        use_glob=True,\n        exclude_patterns=[\"*test*\", \"*__pycache__*\"],\n        max_depth=5\n    )\n\n    # Find large files\n    large_files = fd.find(\n        pattern=\".\",\n        file_types=[\"f\"],\n        absolute_paths=True\n    )\n\n    # Search hidden files\n    hidden_files = fd.find(\n        pattern=\".*\",\n        search_hidden=True,\n        max_results=100\n    )\n</code></pre>"},{"location":"advanced-usage/#progress-performance-features","title":"Progress &amp; Performance Features","text":"<pre><code>from filoma.directories import DirectoryProfiler\n\n# Most profilers support progress bars via `show_progress=True` (behavior may\n# differ depending on backend availability and interactive environment)\nprofiler = DirectoryProfiler(DirectoryProfilerConfig(show_progress=True))\nresult = profiler.probe(\"/path/to/large/directory\")\nprofiler.print_summary(result)\n\n# Fast path only mode (just finds file paths, no metadata)\nprofiler_fast = DirectoryProfiler(DirectoryProfilerConfig(show_progress=True, fast_path_only=True))\nresult_fast = profiler_fast.probe(\"/path/to/large/directory\")\nprint(f\"Found {result_fast['summary']['total_files']} files (fast path only)\")\n\n# Disable progress for benchmarking\nprofiler_benchmark = DirectoryProfiler(DirectoryProfilerConfig(show_progress=False))\n</code></pre>"},{"location":"advanced-usage/#analysis-output-structure","title":"Analysis Output Structure","text":"<pre><code>{\n    \"path\": \"/probed/path\",\n    \"summary\": {\n        \"total_files\": 150,\n        \"total_folders\": 25,\n        \"total_size_bytes\": 1048576,\n        \"total_size_mb\": 1.0,\n        \"avg_files_per_folder\": 6.0,\n        \"max_depth\": 3,\n        \"empty_folder_count\": 2\n    },\n    \"file_extensions\": {\".py\": 45, \".txt\": 30, \".md\": 10},\n    \"common_folder_names\": {\"src\": 3, \"tests\": 2, \"docs\": 1},\n    \"empty_folders\": [\"/path/to/empty1\", \"/path/to/empty2\"],\n    \"top_folders_by_file_count\": [(\"/path/with/most/files\", 25)],\n    \"depth_distribution\": {0: 1, 1: 5, 2: 12, 3: 7},\n    \"dataframe\": filoma.DataFrame  # When build_dataframe=True\n}\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>This page uses <code>mkdocstrings</code> to generate an API reference from the <code>src/filoma</code> package.</p> <p>filoma: filesystem profiling and directory analysis.</p> <p>A modular Python tool for profiling files, analyzing directory structures, and inspecting image data.</p> <p>This module exposes a tiny, ergonomic public surface while importing heavy optional dependencies lazily (Polars, Pillow, Rust extension, etc.). Accessing convenience classes like :class:<code>DataFrame</code> or subpackages like <code>filoma.directories</code> will import the underlying modules on-demand.</p>"},{"location":"api/#filoma.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Lazy import and attribute resolution for top-level names.</p> <p>Implements PEP 562: import submodules or attributes on demand.</p> Source code in <code>src/filoma/__init__.py</code> <pre><code>def __getattr__(name: str):\n    \"\"\"Lazy import and attribute resolution for top-level names.\n\n    Implements PEP 562: import submodules or attributes on demand.\n    \"\"\"\n    mapping = {\n        # top-level subpackages\n        \"core\": \"filoma.core\",\n        \"directories\": \"filoma.directories\",\n        \"files\": \"filoma.files\",\n        \"images\": \"filoma.images\",\n        \"ml\": \"filoma.ml\",\n        # common classes placed in submodules (module:attr)\n        \"DataFrame\": \"filoma.dataframe:DataFrame\",\n        \"DirectoryProfiler\": \"filoma.directories.directory_profiler:DirectoryProfiler\",\n        \"FileProfiler\": \"filoma.files.file_profiler:FileProfiler\",\n        \"ImageProfiler\": \"filoma.images.image_profiler:ImageProfiler\",\n    }\n\n    if name in mapping:\n        target = mapping[name]\n        if \":\" in target:\n            module_name, attr = target.split(\":\", 1)\n            mod = importlib.import_module(module_name)\n            value = getattr(mod, attr)\n        else:\n            value = importlib.import_module(target)\n\n        globals()[name] = value\n        return value\n\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n</code></pre>"},{"location":"api/#filoma.probe","title":"<code>probe(path, **kwargs)</code>","text":"<p>Quick helper: probe a directory path and return a DirectoryAnalysis.</p> <p>This wrapper accepts probe-specific keyword arguments such as <code>max_depth</code> and <code>threads</code> and forwards them to :class:<code>DirectoryProfiler.probe</code>. Other kwargs are used to configure the :class:<code>DirectoryProfiler</code> constructor.</p> Source code in <code>src/filoma/__init__.py</code> <pre><code>def probe(path: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Quick helper: probe a directory path and return a DirectoryAnalysis.\n\n    This wrapper accepts probe-specific keyword arguments such as\n    ``max_depth`` and ``threads`` and forwards them to\n    :class:`DirectoryProfiler.probe`. Other kwargs are used to configure the\n    :class:`DirectoryProfiler` constructor.\n    \"\"\"\n    # Extract probe-only parameters so they are not passed to the\n    # DirectoryProfiler constructor (which doesn't accept them).\n    max_depth = kwargs.pop(\"max_depth\", None)\n    threads = kwargs.pop(\"threads\", None)\n\n    # If the provided path points to a file, dispatch to FileProfiler.probe\n    try:\n        from pathlib import Path\n\n        p = Path(path)\n        if p.exists() and p.is_file():\n            # Forward any file-specific kwargs (e.g., compute_hash) via kwargs\n            from .files.file_profiler import FileProfiler\n\n            return FileProfiler().probe(path, **kwargs)\n    except Exception:\n        # If any checks fail, fall back to directory probing behaviour and\n        # let the underlying profiler raise appropriate errors.\n        pass\n\n    # Local import to ensure the class is available without forcing it at\n    # module import time.\n    from .directories import DirectoryProfiler, DirectoryProfilerConfig\n\n    # Build a typed config from remaining kwargs and instantiate the profiler\n    config = DirectoryProfilerConfig(**kwargs)\n    profiler = DirectoryProfiler(config)\n    return profiler.probe(path, max_depth=max_depth, threads=threads)\n</code></pre>"},{"location":"api/#filoma.probe_file","title":"<code>probe_file(path, **kwargs)</code>","text":"<p>Quick helper: probe a single file and return a Filo dataclass.</p> Source code in <code>src/filoma/__init__.py</code> <pre><code>def probe_file(path: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Quick helper: probe a single file and return a Filo dataclass.\"\"\"\n    from .files.file_profiler import FileProfiler\n\n    return FileProfiler().probe(path, **kwargs)\n</code></pre>"},{"location":"api/#filoma.probe_image","title":"<code>probe_image(arg, **kwargs)</code>","text":"<p>Analyze an image.</p> <p>If <code>arg</code> is a numpy array, :class:<code>ImageProfiler.probe</code> is used; if it's path-like, attempt to locate an image-specific profiler or load it to numpy and analyze.</p> <p>This wrapper favors simplicity for interactive use; for advanced control instantiate profilers directly.</p> Source code in <code>src/filoma/__init__.py</code> <pre><code>def probe_image(arg: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Analyze an image.\n\n    If ``arg`` is a numpy array, :class:`ImageProfiler.probe` is used; if\n    it's path-like, attempt to locate an image-specific profiler or load it\n    to numpy and analyze.\n\n    This wrapper favors simplicity for interactive use; for advanced\n    control instantiate profilers directly.\n    \"\"\"\n    # Local imports; keep them inside the function to avoid heavy deps at\n    # module import time.\n    from pathlib import Path\n\n    try:\n        import numpy as _np\n    except Exception:\n        _np = None\n\n    # If it's a numpy array, use ImageProfiler directly\n    if _np is not None and hasattr(_np, \"ndarray\") and isinstance(arg, _np.ndarray):\n        from .images.image_profiler import ImageProfiler\n\n        return ImageProfiler().probe(arg)\n\n    # Treat as path-like\n    p = Path(arg)\n    suffix = p.suffix.lower() if p.suffix else \"\"\n\n    try:\n        # Use images package specializers when available\n        from .images import NpyProfiler, PngProfiler, TifProfiler, ZarrProfiler\n\n        if suffix == \".png\":\n            return PngProfiler().probe(p)\n        if suffix == \".npy\":\n            return NpyProfiler().probe(p)\n        if suffix in (\".tif\", \".tiff\"):\n            return TifProfiler().probe(p)\n        if suffix == \".zarr\":\n            return ZarrProfiler().probe(p)\n    except Exception:\n        # If specialist creation fails, fall back to generic loader below\n        pass\n\n    # Generic fallback: try Pillow + numpy loader\n    try:\n        # Third-party import\n        from PIL import Image as _PILImage\n\n        # Local import\n        from .images.image_profiler import ImageProfiler\n\n        img = _PILImage.open(p)\n        arr = _np.array(img) if _np is not None else None\n        if arr is not None:\n            return ImageProfiler().probe(arr)\n    except Exception:\n        pass\n\n    # Last resort: return an ImageReport with status explaining failure\n    from .images.image_profiler import ImageReport\n\n    return ImageReport(path=str(p), status=\"failed_to_load_or_unsupported_format\")\n</code></pre>"},{"location":"api/#filoma.probe_to_df","title":"<code>probe_to_df(path, to_pandas=False, enrich=True, **kwargs)</code>","text":"<p>Return a Polars DataFrame (or pandas if to_pandas=True).</p> <p>Force DataFrame building on the profiler and optionally run a small enrichment chain: .add_depth_col(path).add_path_components().add_file_stats_cols().</p> Source code in <code>src/filoma/__init__.py</code> <pre><code>def probe_to_df(path: str, to_pandas: bool = False, enrich: bool = True, **kwargs: Any) -&gt; Any:\n    \"\"\"Return a Polars DataFrame (or pandas if to_pandas=True).\n\n    Force DataFrame building on the profiler and optionally run a small\n    enrichment chain: .add_depth_col(path).add_path_components().add_file_stats_cols().\n    \"\"\"\n    # Extract probe-only parameters\n    max_depth = kwargs.pop(\"max_depth\", None)\n    threads = kwargs.pop(\"threads\", None)\n\n    # Lazy import to avoid heavy deps at module import time\n    from .directories import DirectoryProfiler, DirectoryProfilerConfig\n\n    # Force DataFrame building and construct a typed config\n    kwargs[\"build_dataframe\"] = True\n    config = DirectoryProfilerConfig(**kwargs)\n    profiler = DirectoryProfiler(config)\n    analysis = profiler.probe(path, max_depth=max_depth, threads=threads)\n\n    df_wrapper = analysis.to_df()\n    if df_wrapper is None:\n        raise RuntimeError(\"DataFrame was not built. Ensure 'polars' is installed and that DataFrame building is enabled (build_dataframe=True).\")\n\n    # Optionally enrich the DataFrame wrapper with useful columns/stats\n    df_enriched = df_wrapper\n    if enrich:\n        try:\n            df_enriched = df_enriched.add_depth_col(path).add_path_components().add_file_stats_cols()\n        except Exception:\n            # If enrichment fails for any reason, fall back to the raw DataFrame\n            pass\n\n    # Return requested format: filoma.DataFrame wrapper (default) or pandas\n    # Keep the `to_pandas` convenience for callers that explicitly want pandas\n    if to_pandas:\n        try:\n            return df_enriched.df.to_pandas()\n        except Exception as e:\n            raise RuntimeError(f\"Failed to convert Polars DataFrame to pandas: {e}\")\n\n    return df_enriched\n</code></pre>"},{"location":"api/#package-overview","title":"Package overview","text":"<p>The top-level package docstring is rendered above. Below are some focused sections for important modules and classes.</p>"},{"location":"api/#dataframe-wrapper","title":"DataFrame wrapper","text":"<p>The <code>filoma.DataFrame</code> wrapper provides convenience enrichers and helpers that operate on a Polars DataFrame internally.</p> <p>A wrapper around Polars DataFrame for enhanced file and directory analysis.</p> <p>This class provides a specialized interface for working with file path data, allowing for easy manipulation and analysis of filesystem information.</p> <p>All standard Polars DataFrame methods and properties are available through attribute delegation, so you can use this like a regular Polars DataFrame with additional file-specific functionality.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>class DataFrame:\n    \"\"\"A wrapper around Polars DataFrame for enhanced file and directory analysis.\n\n    This class provides a specialized interface for working with file path data,\n    allowing for easy manipulation and analysis of filesystem information.\n\n    All standard Polars DataFrame methods and properties are available through\n    attribute delegation, so you can use this like a regular Polars DataFrame\n    with additional file-specific functionality.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[pl.DataFrame, List[str], List[Path], Dict[str, Any]]] = None,\n    ):\n        \"\"\"Initialize a DataFrame.\n\n        Args:\n        ----\n            data: Initial data. Can be:\n                - A Polars DataFrame\n                - A dictionary mapping column names to sequences (all same length)\n                - A list of string paths\n                - A list of Path objects\n                - None for an empty DataFrame\n\n        \"\"\"\n        if data is None:\n            self._df = pl.DataFrame({\"path\": []}, schema={\"path\": pl.String})\n        elif isinstance(data, pl.DataFrame):\n            self._df = data\n        elif isinstance(data, dict):\n            if not data:\n                self._df = pl.DataFrame()\n            else:\n                processed: Dict[str, List[Any]] = {}\n                expected_len: Optional[int] = None\n                for col, values in data.items():\n                    if not isinstance(values, (list, tuple)):\n                        raise ValueError(\"Dictionary values must be list or tuple sequences\")\n                    seq = [str(x) if isinstance(x, Path) else x for x in values]\n                    if expected_len is None:\n                        expected_len = len(seq)\n                    elif len(seq) != expected_len:\n                        raise ValueError(\"All dictionary value sequences must have the same length\")\n                    processed[col] = seq\n                self._df = pl.DataFrame(processed)\n        elif isinstance(data, list):\n            paths = [str(path) for path in data]\n            self._df = pl.DataFrame({\"path\": paths})\n        else:\n            raise ValueError(\"data must be a Polars DataFrame, dict of columns, list of paths, or None\")\n        self._pd_cache = None\n        self.with_enrich = False\n        self.with_filename_features = False\n\n    def _ensure_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Ensure the internal `_df` is a Polars DataFrame.\n\n        If the underlying object is not a Polars DataFrame attempt to convert\n        it (via pandas conversion if available or `pl.DataFrame(...)`). This\n        prevents AttributeError when methods expect Polars APIs like\n        `with_columns` or `map_elements`.\n        \"\"\"\n        # Fast path\n        if isinstance(self._df, pl.DataFrame):\n            return self._df\n\n        # Try pandas conversion first if pandas is present and this looks like\n        # a pandas DataFrame\n        try:\n            if pd is not None and isinstance(self._df, pd.DataFrame):\n                self._df = pl.from_pandas(self._df)\n                # Invalidate any cached pandas view since we've converted\n                self.invalidate_pandas_cache()\n                return self._df\n        except Exception:\n            # fall through to generic conversion\n            pass\n\n        # Generic attempt to coerce into a Polars DataFrame\n        try:\n            self._df = pl.DataFrame(self._df)\n            self.invalidate_pandas_cache()\n            return self._df\n        except Exception as exc:\n            raise RuntimeError(f\"Unable to coerce internal DataFrame to polars.DataFrame: {exc}\")\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Delegate attribute access to the underlying Polars DataFrame.\n\n        This allows direct access to all Polars DataFrame methods and properties\n        like columns, dtypes, shape, select, filter, group_by, etc.\n        \"\"\"\n        # Directly return the attribute from the underlying Polars DataFrame.\n        # NOTE: We intentionally do NOT wrap returned Polars DataFrames anymore.\n        # This makes filoma.DataFrame behave like a Polars DataFrame by default\n        # (calls like df.head(), df.select(...), etc. return native Polars\n        # objects). This is a breaking change compared to previously wrapping\n        # Polars results in filoma.DataFrame.\n        try:\n            attr = getattr(self._df, name)\n        except AttributeError:\n            # Preserve the original error semantics\n            raise\n\n        # If the attribute is callable, return a wrapper that conditionally\n        # wraps returned polars.DataFrame objects into filoma.DataFrame\n        if callable(attr):\n\n            def wrapper(*args, **kwargs):\n                result = attr(*args, **kwargs)\n                # If the underlying call mutated the Polars DataFrame in-place,\n                # Polars often returns None or the same object reference. In\n                # that case invalidate the cached pandas conversion so future\n                # .pandas/.pandas_cached calls reflect the mutation.\n                if result is None or result is self._df:\n                    try:\n                        self.invalidate_pandas_cache()\n                    except Exception:\n                        # Best-effort: do not let cache invalidation break calls\n                        pass\n                    return result\n\n                # If wrapping is enabled and result is a Polars DataFrame,\n                # wrap it back into filoma.DataFrame for compatibility.\n                if get_default_wrap_polars() and isinstance(result, pl.DataFrame):\n                    return DataFrame(result)\n\n                return result\n\n            return wrapper\n\n        # Non-callable attributes (properties) \u2014 if it's a Polars DataFrame and\n        # wrapping is requested, wrap it; otherwise return as-is.\n        if get_default_wrap_polars() and isinstance(attr, pl.DataFrame):\n            return DataFrame(attr)\n\n        return attr\n\n    def __dir__(self) -&gt; List[str]:\n        \"\"\"Expose both wrapper and underlying Polars attributes in interactive help.\"\"\"\n        attrs = set(super().__dir__())\n        try:\n            attrs.update(dir(self._df))\n        except Exception:\n            pass\n        return sorted(list(attrs))\n\n    def __getitem__(self, key):\n        \"\"\"Forward subscription (e.g., df['path']) to the underlying Polars DataFrame.\n\n        Returns native Polars objects (Series or DataFrame) to match the default\n        Polars-first behavior of this wrapper.\n        \"\"\"\n        return self._df.__getitem__(key)\n\n    def __setitem__(self, key, value):\n        \"\"\"Forward item assignment to the underlying Polars DataFrame.\"\"\"\n        # Polars DataFrame supports column assignment via df[key] = value\n        # Try to support common user-friendly patterns: assigning a Python\n        # sequence or a Series to create/replace a column. Polars' native\n        # __setitem__ may raise TypeError in some versions, so handle that\n        # explicitly and fall back to with_columns.\n        try:\n            if isinstance(key, str):\n                # Accept polars Series, pandas Series, or Python sequences\n                if isinstance(value, pl.Series):\n                    series = value\n                else:\n                    try:\n                        # pandas Series -&gt; polars Series\n                        if pd is not None and hasattr(value, \"__array__\") and not isinstance(value, (list, tuple)):\n                            series = pl.Series(value)\n                        elif isinstance(value, (list, tuple)):\n                            series = pl.Series(key, list(value))\n                        else:\n                            # Scalar value: repeat across rows\n                            series = pl.Series(key, [value] * len(self._df))\n                    except Exception:\n                        series = None\n\n                if \"series\" in locals() and series is not None:\n                    # Use with_columns to add/replace the column\n                    self._df = self._df.with_columns(series.alias(key))\n                    self.invalidate_pandas_cache()\n                    return\n\n            # Fallback to delegating to Polars __setitem__ for other patterns\n            self._df.__setitem__(key, value)\n            # Underlying data has changed; invalidate any cached pandas conversion\n            self.invalidate_pandas_cache()\n        except TypeError:\n            # Polars raises TypeError for some unsupported assignment forms\n            # (e.g., assigning a Series by index). Re-raise a clearer message\n            msg = \"DataFrame object does not support `Series` assignment by index\\n\\nUse `DataFrame.with_columns`.\"\n            raise TypeError(msg)\n\n    def invalidate_pandas_cache(self) -&gt; None:\n        \"\"\"Clear the cached pandas conversion created by `to_pandas()`.\n\n        Call this after mutating the underlying Polars DataFrame to ensure\n        subsequent `pandas` accesses reflect the latest data.\n        \"\"\"\n        self._pd_cache = None\n\n    @property\n    def df(self) -&gt; pl.DataFrame:\n        \"\"\"Get the underlying Polars DataFrame.\"\"\"\n        return self._df\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of rows in the DataFrame.\"\"\"\n        # polars.DataFrame supports len(), but some wrapped/native objects\n        # (for example older PyArrow-backed objects) may not implement __len__.\n        # Try common fallbacks in order of preference.\n        try:\n            return len(self._df)\n        except Exception:\n            # polars exposes `.height` as row count and `.shape[0]` as rows\n            try:\n                return int(getattr(self._df, \"height\"))\n            except Exception:\n                try:\n                    return int(self._df.shape[0])\n                except Exception:\n                    # Last resort: convert to pandas if available (cheap for small frames)\n                    if pd is not None:\n                        try:\n                            return int(self._df.to_pandas().shape[0])\n                        except Exception:\n                            return 0\n                    return 0\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DataFrame.\"\"\"\n        # Avoid calling the underlying object's __str__/__repr__ if it may\n        # raise TypeError (observed with some PyDataFrame wrappers). Use\n        # safe fallbacks for a short textual preview.\n        row_count = len(self)\n        # Try polars' to_string-like rendering if available\n        try:\n            # Polars DataFrame implements __str__/__repr__; prefer repr()\n            df_preview = repr(self._df)\n        except Exception:\n            try:\n                # Try to convert to pandas for a safer repr\n                if pd is not None:\n                    df_preview = repr(self._df.to_pandas())\n                else:\n                    df_preview = \"&lt;unrepresentable DataFrame&gt;\"\n            except Exception:\n                df_preview = \"&lt;unrepresentable DataFrame&gt;\"\n\n        return f\"filoma.DataFrame with {row_count} rows\\n{df_preview}\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the DataFrame.\"\"\"\n        return self.__repr__()\n\n    def head(self, n: int = 5) -&gt; pl.DataFrame:\n        \"\"\"Get the first n rows.\"\"\"\n        return DataFrame(self._df.head(n))\n\n    def tail(self, n: int = 5) -&gt; pl.DataFrame:\n        \"\"\"Get the last n rows.\"\"\"\n        return DataFrame(self._df.tail(n))\n\n    def add_path_components(self, inplace: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Add columns for path components (parent, name, stem, suffix).\n\n        Returns\n        -------\n            New DataFrame with additional path component columns\n\n        \"\"\"\n        df_with_components = self._df.with_columns(\n            [\n                pl.col(\"path\").map_elements(lambda x: str(Path(x).parent), return_dtype=pl.String).alias(\"parent\"),\n                pl.col(\"path\").map_elements(lambda x: Path(x).name, return_dtype=pl.String).alias(\"name\"),\n                pl.col(\"path\").map_elements(lambda x: Path(x).stem, return_dtype=pl.String).alias(\"stem\"),\n                pl.col(\"path\").map_elements(lambda x: Path(x).suffix, return_dtype=pl.String).alias(\"suffix\"),\n            ]\n        )\n        if inplace:\n            self._df = df_with_components\n            self.invalidate_pandas_cache()\n            return self\n\n        return DataFrame(df_with_components)\n\n    def add_file_stats_cols(\n        self,\n        path: str = \"path\",\n        base_path: Optional[Union[str, Path]] = None,\n        inplace: bool = False,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Add file statistics columns (size, modified time, etc.) based on a column containing filesystem paths.\n\n        Args:\n            path: Name of the column containing file system paths.\n            base_path: Optional base path. If provided, any non-absolute paths in the\n                path column are resolved relative to this base.\n            inplace: If True, modify this DataFrame in-place and return ``self``.\n\n        Returns:\n            New DataFrame with file statistics columns added, or ``self`` when\n            ``inplace=True``.\n\n        Raises:\n            ValueError: If the specified path column does not exist.\n\n        \"\"\"\n        if path not in self._df.columns:\n            raise ValueError(f\"Column '{path}' not found in DataFrame\")\n\n        # Resolve base path if provided\n        base = Path(base_path) if base_path is not None else None\n\n        # Use filoma's FileProfiler to collect rich file metadata\n        profiler = FileProfiler()\n\n        def get_file_stats(path_str: str) -&gt; Dict[str, Any]:\n            try:\n                p = Path(path_str)\n                if base is not None and not p.is_absolute():\n                    p = base / p\n                full_path = str(p)\n                if not p.exists():\n                    logger.warning(f\"Path does not exist: {full_path}\")\n                    return {\n                        \"size_bytes\": None,\n                        \"modified_time\": None,\n                        \"created_time\": None,\n                        \"is_file\": None,\n                        \"is_dir\": None,\n                        \"owner\": None,\n                        \"group\": None,\n                        \"mode_str\": None,\n                        \"inode\": None,\n                        \"nlink\": None,\n                        \"sha256\": None,\n                        \"xattrs\": \"{}\",\n                    }\n\n                # Use the profiler; let it handle symlinks and permissions\n                filo = profiler.probe(full_path, compute_hash=False)\n                row = filo.as_dict()\n\n                # Normalize keys to a stable schema used by this helper\n                return {\n                    \"size_bytes\": row.get(\"size\"),\n                    \"modified_time\": row.get(\"modified\"),\n                    \"created_time\": row.get(\"created\"),\n                    \"is_file\": row.get(\"is_file\"),\n                    \"is_dir\": row.get(\"is_dir\"),\n                    \"owner\": row.get(\"owner\"),\n                    \"group\": row.get(\"group\"),\n                    \"mode_str\": row.get(\"mode_str\"),\n                    \"inode\": row.get(\"inode\"),\n                    \"nlink\": row.get(\"nlink\"),\n                    \"sha256\": row.get(\"sha256\"),\n                    \"xattrs\": json.dumps(row.get(\"xattrs\") or {}),\n                }\n            except Exception:\n                # On any error, return a row of Nones/empties preserving schema\n                return {\n                    \"size_bytes\": None,\n                    \"modified_time\": None,\n                    \"created_time\": None,\n                    \"is_file\": None,\n                    \"is_dir\": None,\n                    \"owner\": None,\n                    \"group\": None,\n                    \"mode_str\": None,\n                    \"inode\": None,\n                    \"nlink\": None,\n                    \"sha256\": None,\n                    \"xattrs\": \"{}\",\n                }\n\n        stats_data = [get_file_stats(p) for p in self._df[path].to_list()]\n\n        stats_df = pl.DataFrame(\n            stats_data,\n            schema={\n                \"size_bytes\": pl.Int64,\n                \"modified_time\": pl.String,\n                \"created_time\": pl.String,\n                \"is_file\": pl.Boolean,\n                \"is_dir\": pl.Boolean,\n                \"owner\": pl.String,\n                \"group\": pl.String,\n                \"mode_str\": pl.String,\n                \"inode\": pl.Int64,\n                \"nlink\": pl.Int64,\n                \"sha256\": pl.String,\n                \"xattrs\": pl.String,\n            },\n        )\n\n        df_with_stats = pl.concat([self._df, stats_df], how=\"horizontal\")\n        if inplace:\n            self._df = df_with_stats\n            self.invalidate_pandas_cache()\n            return self\n\n        return DataFrame(df_with_stats)\n\n    def add_depth_col(self, path: Optional[Union[str, Path]] = None, inplace: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Add a depth column showing the nesting level of each path.\n\n        Args:\n        ----\n            path: The path to calculate depth from. If None, uses the common root.\n            inplace: If True, modify this DataFrame in-place and return ``self``.\n\n        Returns:\n        -------\n            New DataFrame with depth column\n\n        \"\"\"\n        if path is None:\n            # Find the common root path\n            paths = [Path(p) for p in self._df[\"path\"].to_list()]\n            if not paths:\n                path = Path()\n            else:\n                # Find common parent\n                common_parts = []\n                first_parts = paths[0].parts\n                for i, part in enumerate(first_parts):\n                    if all(len(p.parts) &gt; i and p.parts[i] == part for p in paths):\n                        common_parts.append(part)\n                    else:\n                        break\n                path = Path(*common_parts) if common_parts else Path()\n        else:\n            path = Path(path)\n\n        # Use a different local name to avoid shadowing the parameter inside calculate_depth\n        path_root = path\n\n        def calculate_depth(path_str: str) -&gt; int:\n            \"\"\"Calculate the depth of a path relative to the provided root path.\"\"\"\n            try:\n                p = Path(path_str)\n                relative_path = p.relative_to(path_root)\n                return len(relative_path.parts)\n            except ValueError:\n                # Path is not relative to the provided root path\n                return len(Path(path_str).parts)\n\n        df_with_depth = self._df.with_columns([pl.col(\"path\").map_elements(calculate_depth, return_dtype=pl.Int64).alias(\"depth\")])\n        if inplace:\n            self._df = df_with_depth\n            self.invalidate_pandas_cache()\n            return self\n\n        return DataFrame(df_with_depth)\n\n    def filter_by_extension(self, extensions: Union[str, List[str]]) -&gt; \"DataFrame\":\n        \"\"\"Filter the DataFrame to only include files with specific extensions.\n\n        Args:\n        ----\n            extensions: File extension(s) to filter by (with or without leading dot)\n\n        Returns:\n        -------\n            Filtered DataFrame\n\n        \"\"\"\n        if isinstance(extensions, str):\n            extensions = [extensions]\n\n        # Normalize extensions (ensure they start with a dot)\n        normalized_extensions = []\n        for ext in extensions:\n            if not ext.startswith(\".\"):\n                ext = \".\" + ext\n            normalized_extensions.append(ext.lower())\n\n        filtered_df = self._df.filter(\n            pl.col(\"path\").map_elements(\n                lambda x: Path(x).suffix.lower() in normalized_extensions,\n                return_dtype=pl.Boolean,\n            )\n        )\n        return DataFrame(filtered_df)\n\n    def filter_by_pattern(self, pattern: str) -&gt; \"DataFrame\":\n        \"\"\"Filter the DataFrame by path pattern.\n\n        Args:\n        ----\n            pattern: Pattern to match (uses Polars string contains)\n\n        Returns:\n        -------\n            Filtered DataFrame\n\n        \"\"\"\n        filtered_df = self._df.filter(pl.col(\"path\").str.contains(pattern))\n        return DataFrame(filtered_df)\n\n    def group_by_extension(self) -&gt; pl.DataFrame:\n        \"\"\"Group files by extension and count them.\n\n        Returns\n        -------\n            Polars DataFrame with extension counts\n\n        \"\"\"\n        # underlying `_df` is expected to be a Polars DataFrame\n        df_with_ext = self._df.with_columns(\n            [\n                pl.col(\"path\")\n                .map_elements(\n                    lambda x: (Path(x).suffix.lower() if Path(x).suffix else \"&lt;no extension&gt;\"),\n                    return_dtype=pl.String,\n                )\n                .alias(\"extension\")\n            ]\n        )\n        result = df_with_ext.group_by(\"extension\").len().sort(\"len\", descending=True)\n        return DataFrame(result)\n\n    def group_by_directory(self) -&gt; pl.DataFrame:\n        \"\"\"Group files by their parent directory and count them.\n\n        Returns\n        -------\n            Polars DataFrame with directory counts\n\n        \"\"\"\n        # underlying `_df` is expected to be a Polars DataFrame\n        df_with_parent = self._df.with_columns(\n            [pl.col(\"path\").map_elements(lambda x: str(Path(x).parent), return_dtype=pl.String).alias(\"parent_dir\")]\n        )\n        result = df_with_parent.group_by(\"parent_dir\").len().sort(\"len\", descending=True)\n        return DataFrame(result)\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Get the underlying Polars DataFrame.\"\"\"\n        return self._df\n\n    def to_pandas(self, force: bool = False) -&gt; Any:\n        \"\"\"Convert to a pandas DataFrame.\n\n        By default this method will return a cached pandas conversion if one\n        exists (for performance). Set ``force=True`` to reconvert from the\n        current Polars DataFrame and update the cache.\n        \"\"\"\n        if pd is None:\n            raise ImportError(\"pandas is not installed. Please install it to use to_pandas().\")\n        # Convert and cache on first access or when forced\n        if force or self._pd_cache is None:\n            # Use Polars' to_pandas conversion for consistency\n            self._pd_cache = self._df.to_pandas()\n        return self._pd_cache\n\n    @property\n    def polars(self) -&gt; pl.DataFrame:\n        \"\"\"Property access for the underlying Polars DataFrame (convenience).\"\"\"\n        return self.to_polars()\n\n    @property\n    def pandas(self) -&gt; Any:\n        \"\"\"Return a fresh pandas DataFrame conversion (not the cached object).\n\n        This is intentionally a fresh conversion so callers who expect an\n        up-to-date pandas view can access it directly. Use ``pandas_cached`` or\n        ``to_pandas(force=False)`` to access the cached conversion for repeated\n        reads, or ``to_pandas(force=True)`` to reconvert and update the cache.\n\n        Raises\n        ------\n            ImportError: if pandas is not installed.\n\n        \"\"\"\n        if pd is None:\n            raise ImportError(\"pandas is not installed. Please install it to use pandas property.\")\n        return self._df.to_pandas()\n\n    @property\n    def pandas_cached(self) -&gt; Any:\n        \"\"\"Return a cached pandas DataFrame, converting once if needed.\n\n        This is useful when repeated conversions would be expensive and the\n        caller is comfortable with an explicit cache that can be invalidated\n        with ``invalidate_pandas_cache()`` or by calling ``to_pandas(force=True)``.\n        \"\"\"\n        return self.to_pandas(force=False)\n\n    @property\n    def native(self):\n        \"\"\"Return the dataframe in the module-wide default backend.\n\n        If `get_default_dataframe_backend()` is 'polars' this returns a Polars\n        DataFrame, otherwise it returns a pandas DataFrame.\n        \"\"\"\n        if get_default_dataframe_backend() == \"polars\":\n            return self.polars\n        return self.pandas\n\n    @classmethod\n    def from_pandas(cls, df: Any) -&gt; \"DataFrame\":\n        \"\"\"Construct a filoma.DataFrame from a pandas DataFrame.\n\n        This is a convenience wrapper that converts the pandas DataFrame into\n        a Polars DataFrame and wraps it. Requires pandas to be installed.\n        \"\"\"\n        if pd is None:\n            raise RuntimeError(\"pandas is not available in this environment\")\n        # Convert via Polars for internal consistency\n        pl_df = pl.from_pandas(df)\n        return cls(pl_df)\n\n    def to_dict(self) -&gt; Dict[str, List]:\n        \"\"\"Convert to a dictionary.\"\"\"\n        return self._df.to_dict(as_series=False)\n\n    def save_csv(self, path: Union[str, Path]) -&gt; None:\n        \"\"\"Save the DataFrame to CSV.\"\"\"\n        self._df.write_csv(str(path))\n\n    def save_parquet(self, path: Union[str, Path]) -&gt; None:\n        \"\"\"Save the DataFrame to Parquet format.\"\"\"\n        self._df.write_parquet(str(path))\n\n    # Convenience methods for common Polars operations that users expect\n    @property\n    def columns(self) -&gt; List[str]:\n        \"\"\"Get column names.\"\"\"\n        return self._df.columns\n\n    @property\n    def dtypes(self) -&gt; List[pl.DataType]:\n        \"\"\"Get column data types.\"\"\"\n        return self._df.dtypes\n\n    @property\n    def shape(self) -&gt; tuple:\n        \"\"\"Get DataFrame shape (rows, columns).\"\"\"\n        # Attempt to return a (rows, cols) tuple even if the underlying\n        # object doesn't expose .shape or len(). Use the same fallbacks as\n        # in __len__ for rows and inspect columns for width.\n        try:\n            rows, cols = self._df.shape\n            return (int(rows), int(cols))\n        except Exception:\n            # Rows fallback\n            try:\n                rows = len(self)\n            except Exception:\n                rows = 0\n            # Columns fallback: try .columns or pandas conversion\n            try:\n                cols = len(getattr(self._df, \"columns\"))\n            except Exception:\n                try:\n                    if pd is not None:\n                        cols = self._df.to_pandas().shape[1]\n                    else:\n                        cols = 0\n                except Exception:\n                    cols = 0\n            return (int(rows), int(cols))\n\n    def describe(self, percentiles: Optional[List[float]] = None) -&gt; pl.DataFrame:\n        \"\"\"Generate descriptive statistics.\n\n        Args:\n        ----\n            percentiles: List of percentiles to include (default: [0.25, 0.5, 0.75])\n\n        \"\"\"\n        # Polars' describe returns a new DataFrame summarizing columns; wrap it\n        return DataFrame(self._df.describe(percentiles=percentiles))\n\n    def info(self) -&gt; None:\n        \"\"\"Print concise summary of the DataFrame.\"\"\"\n        print(\"filoma.DataFrame\")\n        print(f\"Shape: {self.shape}\")\n        print(f\"Columns: {len(self.columns)}\")\n        print()\n\n        # Column info\n        print(\"Column details:\")\n        for i, (col, dtype) in enumerate(zip(self.columns, self.dtypes)):\n            null_count = self._df[col].null_count()\n            print(f\"  {i:2d}  {col:15s} {str(dtype):15s} {null_count:8d} nulls\")\n\n        # Memory usage approximation\n        memory_mb = sum(self._df[col].estimated_size(\"mb\") for col in self.columns)\n        print(f\"\\nEstimated memory usage: {memory_mb:.2f} MB\")\n\n    def unique(self, subset: Optional[Union[str, List[str]]] = None) -&gt; \"DataFrame\":\n        \"\"\"Get unique rows.\n\n        Args:\n        ----\n            subset: Column name(s) to consider for uniqueness\n\n        \"\"\"\n        if subset is None:\n            result = self._df.unique()\n        else:\n            result = self._df.unique(subset=subset)\n        return DataFrame(result)\n\n    def sort(self, by: Union[str, List[str]], descending: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Sort the DataFrame.\n\n        Args:\n        ----\n            by: Column name(s) to sort by\n            descending: Sort in descending order\n\n        \"\"\"\n        result = self._df.sort(by, descending=descending)\n        return DataFrame(result)\n\n    # -------------------- ML convenience API -------------------- #\n    def split_data(\n        self,\n        train_val_test: Tuple[float, float, float] = (80, 10, 10),\n        feature: Union[str, Sequence[str]] = \"path_parts\",\n        path_parts: Optional[Iterable[int]] = (-1,),\n        seed: Optional[int] = None,\n        discover: bool = False,\n        sep: str = \"_\",\n        feat_prefix: str = \"feat\",\n        max_tokens: Optional[int] = None,\n        include_parent: bool = False,\n        include_all_parts: bool = False,\n        token_names: Optional[Union[str, Sequence[str]]] = None,\n        path_col: str = \"path\",\n        verbose: bool = True,\n        return_type: str = \"filoma\",\n    ):\n        \"\"\"Deterministically split this filoma DataFrame into train/val/test.\n\n        This is a thin wrapper around ``filoma.ml.split_data`` so you can call\n        ``df.split_data(...)`` directly on a filoma DataFrame instance.\n\n        Args mirror :func:`filoma.ml.split_data` except ``df`` is implicit.\n\n        By default ``return_type='filoma'`` so the three returned objects are\n        filoma.DataFrame wrappers.\n        \"\"\"\n        # Local import to avoid loading ml utilities unless used\n        from . import ml  # type: ignore\n\n        return ml.split_data(\n            self,\n            train_val_test=train_val_test,\n            feature=feature,\n            path_parts=path_parts,\n            seed=seed,\n            discover=discover,\n            sep=sep,\n            feat_prefix=feat_prefix,\n            max_tokens=max_tokens,\n            include_parent=include_parent,\n            include_all_parts=include_all_parts,\n            token_names=token_names,\n            path_col=path_col,\n            verbose=verbose,\n            return_type=return_type,\n        )\n\n    def enrich(self, inplace: bool = False):\n        \"\"\"Enrich the DataFrame by adding features like path components, file stats, and depth.\n\n        Args:\n        ----\n            inplace: If True, perform the operation in-place and return self.\n                     If False (default), return a new DataFrame with the changes.\n\n        \"\"\"\n        # Chain the enrichment methods; this produces a new DataFrame wrapper\n        enriched_wrapper = self.add_path_components().add_file_stats_cols().add_depth_col()\n        enriched_wrapper.with_enrich = True\n\n        if inplace:\n            # Update the internal state of the current object\n            self._df = enriched_wrapper._df\n            self.with_enrich = True\n            self.invalidate_pandas_cache()\n            return self\n\n        # Return the new, enriched DataFrame instance\n        return enriched_wrapper\n\n    def evaluate_duplicates(\n        self,\n        path_col: str = \"path\",\n        text_threshold: float = 0.8,\n        image_max_distance: int = 5,\n        text_k: int = 3,\n        show_table: bool = True,\n    ) -&gt; dict:\n        \"\"\"Evaluate duplicates among files in the DataFrame.\n\n        Scans the `path_col` column, runs exact, text and image duplicate\n        detectors and prints a small Rich table summarizing counts.\n\n        Returns the raw dict produced by `filoma.dedup.find_duplicates`.\n        \"\"\"\n        if path_col not in self._df.columns:\n            raise ValueError(f\"Column '{path_col}' not found in DataFrame\")\n\n        paths = [str(p) for p in self._df[path_col].to_list()]\n        res = _dedup.find_duplicates(\n            paths,\n            text_k=text_k,\n            text_threshold=text_threshold,\n            image_max_distance=image_max_distance,\n        )\n\n        # Summarize counts\n        exact_groups = res.get(\"exact\", [])\n        text_groups = res.get(\"text\", [])\n        image_groups = res.get(\"image\", [])\n\n        console = Console()\n        if show_table:\n            table = Table(title=\"Duplicate Summary\")\n            table.add_column(\"Type\", style=\"bold cyan\")\n            table.add_column(\"Groups\", style=\"white\")\n            table.add_column(\"Files In Groups\", style=\"white\")\n            table.add_row(\n                \"exact\",\n                str(len(exact_groups)),\n                str(sum(len(g) for g in exact_groups) if exact_groups else 0),\n            )\n            table.add_row(\n                \"text\",\n                str(len(text_groups)),\n                str(sum(len(g) for g in text_groups) if text_groups else 0),\n            )\n            table.add_row(\n                \"image\",\n                str(len(image_groups)),\n                str(sum(len(g) for g in image_groups) if image_groups else 0),\n            )\n            console.print(table)\n\n        logger.info(\n            f\"Duplicate summary: exact={len(exact_groups)} groups \"\n            f\"({sum(len(g) for g in exact_groups) if exact_groups else 0} files), \"\n            f\"text={len(text_groups)} groups \"\n            f\"({sum(len(g) for g in text_groups) if text_groups else 0} files), \"\n            f\"image={len(image_groups)} groups \"\n            f\"({sum(len(g) for g in image_groups) if image_groups else 0} files)\"\n        )\n\n        return res\n\n    def add_filename_features(\n        self,\n        path_col: str = \"path\",\n        sep: str = \"_\",\n        prefix: Optional[str] = \"feat\",\n        max_tokens: Optional[int] = None,\n        include_parent: bool = False,\n        include_all_parts: bool = False,\n        token_names: Optional[Union[str, Sequence[str]]] = None,\n        enrich: bool = False,\n        inplace: bool = False,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Discover filename features and add them as columns on this DataFrame.\n\n        This instance method discovers separator-based tokens from filename\n        stems and adds columns (e.g., `feat1`, `feat2` or `token1`, ...).\n\n        Args:\n        ----\n            path_col: Column containing path strings to analyze (default: 'path').\n            sep: Separator used to split filename stems (default: '_').\n            prefix: Column name prefix for discovered tokens (default: 'feat').\n            max_tokens: Optional cap on extracted tokens; by default uses observed max.\n            include_parent: If True, add a `parent` column containing immediate parent folder name.\n            include_all_parts: If True, add `path_part0`, `path_part1`, ... for all Path.parts.\n            token_names: Optional list of token column names or 'auto' to generate readable names.\n            enrich: If True, automatically enrich the DataFrame with path components and file stats before discovery.\n            inplace: If True, perform the operation in-place and return self. Otherwise returns a new `filoma.DataFrame`.\n\n        Returns:\n        -------\n            A new or modified `filoma.DataFrame` with discovered filename features.\n\n        \"\"\"\n        # Determine the base Polars DataFrame for feature discovery\n        base_df = self\n        if enrich and not self.with_enrich:\n            logger.info(\"Enriching DataFrame before discovering filename features\")\n            base_df = self.enrich(inplace=False)\n\n        # Polars-native implementation inlined here (formerly a top-level helper).\n        pl_df = base_df._df\n        if path_col not in pl_df.columns:\n            raise ValueError(f\"DataFrame must have a '{path_col}' column\")\n\n        stems = [Path(s).stem for s in pl_df[path_col].to_list()]\n        split_tokens = [stem.split(sep) if stem is not None else [\"\"] for stem in stems]\n        observed_max = max((len(t) for t in split_tokens), default=0)\n        if max_tokens is None:\n            eff_max = observed_max\n        else:\n            eff_max = max_tokens\n\n        # Normalize token_names\n        if token_names == \"auto\":\n            token_names_seq = None\n            auto_mode = True\n        elif isinstance(token_names, (list, tuple)):\n            token_names_seq = list(token_names)\n            auto_mode = False\n        else:\n            token_names_seq = None\n            auto_mode = False\n\n        new_cols = []\n        for i in range(eff_max):\n            if token_names_seq is not None and i &lt; len(token_names_seq) and token_names_seq[i]:\n                col_name = token_names_seq[i]\n            elif auto_mode:\n                base = prefix if prefix else \"token\"\n                col_name = f\"{base}{i + 1}\"\n            else:\n                if prefix:\n                    col_name = f\"{prefix}{i + 1}\"\n                else:\n                    col_name = f\"token{i + 1}\"\n\n            def pick_token(s: str, idx=i):\n                st = Path(s).stem\n                parts = st.split(sep) if st is not None else [\"\"]\n                try:\n                    return parts[idx]\n                except Exception:\n                    return \"\"\n\n            new_cols.append(pl.col(path_col).map_elements(pick_token, return_dtype=pl.Utf8).alias(col_name))\n\n        if include_parent:\n            new_cols.append(pl.col(path_col).map_elements(lambda s: Path(s).parent.name, return_dtype=pl.Utf8).alias(\"parent\"))\n\n        if include_all_parts:\n            parts_lists = [list(Path(s).parts) for s in pl_df[path_col].to_list()]\n            max_parts = max((len(p) for p in parts_lists), default=0)\n            for i in range(max_parts):\n                col_name = f\"path_part{i}\"\n\n                def pick_part(s: str, idx=i):\n                    try:\n                        parts = list(Path(s).parts)\n                        return parts[idx]\n                    except Exception:\n                        return \"\"\n\n                new_cols.append(pl.col(path_col).map_elements(pick_part, return_dtype=pl.Utf8).alias(col_name))\n\n        pl_result = pl_df.with_columns(new_cols)\n\n        # Wrap the result in a filoma.DataFrame\n        enriched_wrapper = DataFrame(pl_result)\n        enriched_wrapper.with_filename_features = True\n\n        if inplace:\n            self._df = enriched_wrapper._df\n            self.with_filename_features = True\n            if enrich and not self.with_enrich:\n                self.with_enrich = True\n            self.invalidate_pandas_cache()\n            return self\n\n        return enriched_wrapper\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Get column names.</p>"},{"location":"api/#filoma.dataframe.DataFrame.df","title":"<code>df</code>  <code>property</code>","text":"<p>Get the underlying Polars DataFrame.</p>"},{"location":"api/#filoma.dataframe.DataFrame.dtypes","title":"<code>dtypes</code>  <code>property</code>","text":"<p>Get column data types.</p>"},{"location":"api/#filoma.dataframe.DataFrame.native","title":"<code>native</code>  <code>property</code>","text":"<p>Return the dataframe in the module-wide default backend.</p> <p>If <code>get_default_dataframe_backend()</code> is 'polars' this returns a Polars DataFrame, otherwise it returns a pandas DataFrame.</p>"},{"location":"api/#filoma.dataframe.DataFrame.pandas","title":"<code>pandas</code>  <code>property</code>","text":"<p>Return a fresh pandas DataFrame conversion (not the cached object).</p> <p>This is intentionally a fresh conversion so callers who expect an up-to-date pandas view can access it directly. Use <code>pandas_cached</code> or <code>to_pandas(force=False)</code> to access the cached conversion for repeated reads, or <code>to_pandas(force=True)</code> to reconvert and update the cache.</p>"},{"location":"api/#filoma.dataframe.DataFrame.pandas--raises","title":"Raises","text":"<pre><code>ImportError: if pandas is not installed.\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.pandas_cached","title":"<code>pandas_cached</code>  <code>property</code>","text":"<p>Return a cached pandas DataFrame, converting once if needed.</p> <p>This is useful when repeated conversions would be expensive and the caller is comfortable with an explicit cache that can be invalidated with <code>invalidate_pandas_cache()</code> or by calling <code>to_pandas(force=True)</code>.</p>"},{"location":"api/#filoma.dataframe.DataFrame.polars","title":"<code>polars</code>  <code>property</code>","text":"<p>Property access for the underlying Polars DataFrame (convenience).</p>"},{"location":"api/#filoma.dataframe.DataFrame.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Get DataFrame shape (rows, columns).</p>"},{"location":"api/#filoma.dataframe.DataFrame.__dir__","title":"<code>__dir__()</code>","text":"<p>Expose both wrapper and underlying Polars attributes in interactive help.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __dir__(self) -&gt; List[str]:\n    \"\"\"Expose both wrapper and underlying Polars attributes in interactive help.\"\"\"\n    attrs = set(super().__dir__())\n    try:\n        attrs.update(dir(self._df))\n    except Exception:\n        pass\n    return sorted(list(attrs))\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Delegate attribute access to the underlying Polars DataFrame.</p> <p>This allows direct access to all Polars DataFrame methods and properties like columns, dtypes, shape, select, filter, group_by, etc.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Delegate attribute access to the underlying Polars DataFrame.\n\n    This allows direct access to all Polars DataFrame methods and properties\n    like columns, dtypes, shape, select, filter, group_by, etc.\n    \"\"\"\n    # Directly return the attribute from the underlying Polars DataFrame.\n    # NOTE: We intentionally do NOT wrap returned Polars DataFrames anymore.\n    # This makes filoma.DataFrame behave like a Polars DataFrame by default\n    # (calls like df.head(), df.select(...), etc. return native Polars\n    # objects). This is a breaking change compared to previously wrapping\n    # Polars results in filoma.DataFrame.\n    try:\n        attr = getattr(self._df, name)\n    except AttributeError:\n        # Preserve the original error semantics\n        raise\n\n    # If the attribute is callable, return a wrapper that conditionally\n    # wraps returned polars.DataFrame objects into filoma.DataFrame\n    if callable(attr):\n\n        def wrapper(*args, **kwargs):\n            result = attr(*args, **kwargs)\n            # If the underlying call mutated the Polars DataFrame in-place,\n            # Polars often returns None or the same object reference. In\n            # that case invalidate the cached pandas conversion so future\n            # .pandas/.pandas_cached calls reflect the mutation.\n            if result is None or result is self._df:\n                try:\n                    self.invalidate_pandas_cache()\n                except Exception:\n                    # Best-effort: do not let cache invalidation break calls\n                    pass\n                return result\n\n            # If wrapping is enabled and result is a Polars DataFrame,\n            # wrap it back into filoma.DataFrame for compatibility.\n            if get_default_wrap_polars() and isinstance(result, pl.DataFrame):\n                return DataFrame(result)\n\n            return result\n\n        return wrapper\n\n    # Non-callable attributes (properties) \u2014 if it's a Polars DataFrame and\n    # wrapping is requested, wrap it; otherwise return as-is.\n    if get_default_wrap_polars() and isinstance(attr, pl.DataFrame):\n        return DataFrame(attr)\n\n    return attr\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Forward subscription (e.g., df['path']) to the underlying Polars DataFrame.</p> <p>Returns native Polars objects (Series or DataFrame) to match the default Polars-first behavior of this wrapper.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Forward subscription (e.g., df['path']) to the underlying Polars DataFrame.\n\n    Returns native Polars objects (Series or DataFrame) to match the default\n    Polars-first behavior of this wrapper.\n    \"\"\"\n    return self._df.__getitem__(key)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__init__","title":"<code>__init__(data=None)</code>","text":"<p>Initialize a DataFrame.</p> <pre><code>data: Initial data. Can be:\n    - A Polars DataFrame\n    - A dictionary mapping column names to sequences (all same length)\n    - A list of string paths\n    - A list of Path objects\n    - None for an empty DataFrame\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[pl.DataFrame, List[str], List[Path], Dict[str, Any]]] = None,\n):\n    \"\"\"Initialize a DataFrame.\n\n    Args:\n    ----\n        data: Initial data. Can be:\n            - A Polars DataFrame\n            - A dictionary mapping column names to sequences (all same length)\n            - A list of string paths\n            - A list of Path objects\n            - None for an empty DataFrame\n\n    \"\"\"\n    if data is None:\n        self._df = pl.DataFrame({\"path\": []}, schema={\"path\": pl.String})\n    elif isinstance(data, pl.DataFrame):\n        self._df = data\n    elif isinstance(data, dict):\n        if not data:\n            self._df = pl.DataFrame()\n        else:\n            processed: Dict[str, List[Any]] = {}\n            expected_len: Optional[int] = None\n            for col, values in data.items():\n                if not isinstance(values, (list, tuple)):\n                    raise ValueError(\"Dictionary values must be list or tuple sequences\")\n                seq = [str(x) if isinstance(x, Path) else x for x in values]\n                if expected_len is None:\n                    expected_len = len(seq)\n                elif len(seq) != expected_len:\n                    raise ValueError(\"All dictionary value sequences must have the same length\")\n                processed[col] = seq\n            self._df = pl.DataFrame(processed)\n    elif isinstance(data, list):\n        paths = [str(path) for path in data]\n        self._df = pl.DataFrame({\"path\": paths})\n    else:\n        raise ValueError(\"data must be a Polars DataFrame, dict of columns, list of paths, or None\")\n    self._pd_cache = None\n    self.with_enrich = False\n    self.with_filename_features = False\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of rows in the DataFrame.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of rows in the DataFrame.\"\"\"\n    # polars.DataFrame supports len(), but some wrapped/native objects\n    # (for example older PyArrow-backed objects) may not implement __len__.\n    # Try common fallbacks in order of preference.\n    try:\n        return len(self._df)\n    except Exception:\n        # polars exposes `.height` as row count and `.shape[0]` as rows\n        try:\n            return int(getattr(self._df, \"height\"))\n        except Exception:\n            try:\n                return int(self._df.shape[0])\n            except Exception:\n                # Last resort: convert to pandas if available (cheap for small frames)\n                if pd is not None:\n                    try:\n                        return int(self._df.to_pandas().shape[0])\n                    except Exception:\n                        return 0\n                return 0\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DataFrame.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DataFrame.\"\"\"\n    # Avoid calling the underlying object's __str__/__repr__ if it may\n    # raise TypeError (observed with some PyDataFrame wrappers). Use\n    # safe fallbacks for a short textual preview.\n    row_count = len(self)\n    # Try polars' to_string-like rendering if available\n    try:\n        # Polars DataFrame implements __str__/__repr__; prefer repr()\n        df_preview = repr(self._df)\n    except Exception:\n        try:\n            # Try to convert to pandas for a safer repr\n            if pd is not None:\n                df_preview = repr(self._df.to_pandas())\n            else:\n                df_preview = \"&lt;unrepresentable DataFrame&gt;\"\n        except Exception:\n            df_preview = \"&lt;unrepresentable DataFrame&gt;\"\n\n    return f\"filoma.DataFrame with {row_count} rows\\n{df_preview}\"\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Forward item assignment to the underlying Polars DataFrame.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __setitem__(self, key, value):\n    \"\"\"Forward item assignment to the underlying Polars DataFrame.\"\"\"\n    # Polars DataFrame supports column assignment via df[key] = value\n    # Try to support common user-friendly patterns: assigning a Python\n    # sequence or a Series to create/replace a column. Polars' native\n    # __setitem__ may raise TypeError in some versions, so handle that\n    # explicitly and fall back to with_columns.\n    try:\n        if isinstance(key, str):\n            # Accept polars Series, pandas Series, or Python sequences\n            if isinstance(value, pl.Series):\n                series = value\n            else:\n                try:\n                    # pandas Series -&gt; polars Series\n                    if pd is not None and hasattr(value, \"__array__\") and not isinstance(value, (list, tuple)):\n                        series = pl.Series(value)\n                    elif isinstance(value, (list, tuple)):\n                        series = pl.Series(key, list(value))\n                    else:\n                        # Scalar value: repeat across rows\n                        series = pl.Series(key, [value] * len(self._df))\n                except Exception:\n                    series = None\n\n            if \"series\" in locals() and series is not None:\n                # Use with_columns to add/replace the column\n                self._df = self._df.with_columns(series.alias(key))\n                self.invalidate_pandas_cache()\n                return\n\n        # Fallback to delegating to Polars __setitem__ for other patterns\n        self._df.__setitem__(key, value)\n        # Underlying data has changed; invalidate any cached pandas conversion\n        self.invalidate_pandas_cache()\n    except TypeError:\n        # Polars raises TypeError for some unsupported assignment forms\n        # (e.g., assigning a Series by index). Re-raise a clearer message\n        msg = \"DataFrame object does not support `Series` assignment by index\\n\\nUse `DataFrame.with_columns`.\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the DataFrame.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the DataFrame.\"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.add_depth_col","title":"<code>add_depth_col(path=None, inplace=False)</code>","text":"<p>Add a depth column showing the nesting level of each path.</p> <pre><code>path: The path to calculate depth from. If None, uses the common root.\ninplace: If True, modify this DataFrame in-place and return ``self``.\n</code></pre> <pre><code>New DataFrame with depth column\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def add_depth_col(self, path: Optional[Union[str, Path]] = None, inplace: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Add a depth column showing the nesting level of each path.\n\n    Args:\n    ----\n        path: The path to calculate depth from. If None, uses the common root.\n        inplace: If True, modify this DataFrame in-place and return ``self``.\n\n    Returns:\n    -------\n        New DataFrame with depth column\n\n    \"\"\"\n    if path is None:\n        # Find the common root path\n        paths = [Path(p) for p in self._df[\"path\"].to_list()]\n        if not paths:\n            path = Path()\n        else:\n            # Find common parent\n            common_parts = []\n            first_parts = paths[0].parts\n            for i, part in enumerate(first_parts):\n                if all(len(p.parts) &gt; i and p.parts[i] == part for p in paths):\n                    common_parts.append(part)\n                else:\n                    break\n            path = Path(*common_parts) if common_parts else Path()\n    else:\n        path = Path(path)\n\n    # Use a different local name to avoid shadowing the parameter inside calculate_depth\n    path_root = path\n\n    def calculate_depth(path_str: str) -&gt; int:\n        \"\"\"Calculate the depth of a path relative to the provided root path.\"\"\"\n        try:\n            p = Path(path_str)\n            relative_path = p.relative_to(path_root)\n            return len(relative_path.parts)\n        except ValueError:\n            # Path is not relative to the provided root path\n            return len(Path(path_str).parts)\n\n    df_with_depth = self._df.with_columns([pl.col(\"path\").map_elements(calculate_depth, return_dtype=pl.Int64).alias(\"depth\")])\n    if inplace:\n        self._df = df_with_depth\n        self.invalidate_pandas_cache()\n        return self\n\n    return DataFrame(df_with_depth)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.add_file_stats_cols","title":"<code>add_file_stats_cols(path='path', base_path=None, inplace=False)</code>","text":"<p>Add file statistics columns (size, modified time, etc.) based on a column containing filesystem paths.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Name of the column containing file system paths.</p> <code>'path'</code> <code>base_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional base path. If provided, any non-absolute paths in the path column are resolved relative to this base.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modify this DataFrame in-place and return <code>self</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>New DataFrame with file statistics columns added, or <code>self</code> when</p> <code>DataFrame</code> <p><code>inplace=True</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified path column does not exist.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def add_file_stats_cols(\n    self,\n    path: str = \"path\",\n    base_path: Optional[Union[str, Path]] = None,\n    inplace: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Add file statistics columns (size, modified time, etc.) based on a column containing filesystem paths.\n\n    Args:\n        path: Name of the column containing file system paths.\n        base_path: Optional base path. If provided, any non-absolute paths in the\n            path column are resolved relative to this base.\n        inplace: If True, modify this DataFrame in-place and return ``self``.\n\n    Returns:\n        New DataFrame with file statistics columns added, or ``self`` when\n        ``inplace=True``.\n\n    Raises:\n        ValueError: If the specified path column does not exist.\n\n    \"\"\"\n    if path not in self._df.columns:\n        raise ValueError(f\"Column '{path}' not found in DataFrame\")\n\n    # Resolve base path if provided\n    base = Path(base_path) if base_path is not None else None\n\n    # Use filoma's FileProfiler to collect rich file metadata\n    profiler = FileProfiler()\n\n    def get_file_stats(path_str: str) -&gt; Dict[str, Any]:\n        try:\n            p = Path(path_str)\n            if base is not None and not p.is_absolute():\n                p = base / p\n            full_path = str(p)\n            if not p.exists():\n                logger.warning(f\"Path does not exist: {full_path}\")\n                return {\n                    \"size_bytes\": None,\n                    \"modified_time\": None,\n                    \"created_time\": None,\n                    \"is_file\": None,\n                    \"is_dir\": None,\n                    \"owner\": None,\n                    \"group\": None,\n                    \"mode_str\": None,\n                    \"inode\": None,\n                    \"nlink\": None,\n                    \"sha256\": None,\n                    \"xattrs\": \"{}\",\n                }\n\n            # Use the profiler; let it handle symlinks and permissions\n            filo = profiler.probe(full_path, compute_hash=False)\n            row = filo.as_dict()\n\n            # Normalize keys to a stable schema used by this helper\n            return {\n                \"size_bytes\": row.get(\"size\"),\n                \"modified_time\": row.get(\"modified\"),\n                \"created_time\": row.get(\"created\"),\n                \"is_file\": row.get(\"is_file\"),\n                \"is_dir\": row.get(\"is_dir\"),\n                \"owner\": row.get(\"owner\"),\n                \"group\": row.get(\"group\"),\n                \"mode_str\": row.get(\"mode_str\"),\n                \"inode\": row.get(\"inode\"),\n                \"nlink\": row.get(\"nlink\"),\n                \"sha256\": row.get(\"sha256\"),\n                \"xattrs\": json.dumps(row.get(\"xattrs\") or {}),\n            }\n        except Exception:\n            # On any error, return a row of Nones/empties preserving schema\n            return {\n                \"size_bytes\": None,\n                \"modified_time\": None,\n                \"created_time\": None,\n                \"is_file\": None,\n                \"is_dir\": None,\n                \"owner\": None,\n                \"group\": None,\n                \"mode_str\": None,\n                \"inode\": None,\n                \"nlink\": None,\n                \"sha256\": None,\n                \"xattrs\": \"{}\",\n            }\n\n    stats_data = [get_file_stats(p) for p in self._df[path].to_list()]\n\n    stats_df = pl.DataFrame(\n        stats_data,\n        schema={\n            \"size_bytes\": pl.Int64,\n            \"modified_time\": pl.String,\n            \"created_time\": pl.String,\n            \"is_file\": pl.Boolean,\n            \"is_dir\": pl.Boolean,\n            \"owner\": pl.String,\n            \"group\": pl.String,\n            \"mode_str\": pl.String,\n            \"inode\": pl.Int64,\n            \"nlink\": pl.Int64,\n            \"sha256\": pl.String,\n            \"xattrs\": pl.String,\n        },\n    )\n\n    df_with_stats = pl.concat([self._df, stats_df], how=\"horizontal\")\n    if inplace:\n        self._df = df_with_stats\n        self.invalidate_pandas_cache()\n        return self\n\n    return DataFrame(df_with_stats)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.add_filename_features","title":"<code>add_filename_features(path_col='path', sep='_', prefix='feat', max_tokens=None, include_parent=False, include_all_parts=False, token_names=None, enrich=False, inplace=False)</code>","text":"<p>Discover filename features and add them as columns on this DataFrame.</p> <p>This instance method discovers separator-based tokens from filename stems and adds columns (e.g., <code>feat1</code>, <code>feat2</code> or <code>token1</code>, ...).</p> <pre><code>path_col: Column containing path strings to analyze (default: 'path').\nsep: Separator used to split filename stems (default: '_').\nprefix: Column name prefix for discovered tokens (default: 'feat').\nmax_tokens: Optional cap on extracted tokens; by default uses observed max.\ninclude_parent: If True, add a `parent` column containing immediate parent folder name.\ninclude_all_parts: If True, add `path_part0`, `path_part1`, ... for all Path.parts.\ntoken_names: Optional list of token column names or 'auto' to generate readable names.\nenrich: If True, automatically enrich the DataFrame with path components and file stats before discovery.\ninplace: If True, perform the operation in-place and return self. Otherwise returns a new `filoma.DataFrame`.\n</code></pre> <pre><code>A new or modified `filoma.DataFrame` with discovered filename features.\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def add_filename_features(\n    self,\n    path_col: str = \"path\",\n    sep: str = \"_\",\n    prefix: Optional[str] = \"feat\",\n    max_tokens: Optional[int] = None,\n    include_parent: bool = False,\n    include_all_parts: bool = False,\n    token_names: Optional[Union[str, Sequence[str]]] = None,\n    enrich: bool = False,\n    inplace: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Discover filename features and add them as columns on this DataFrame.\n\n    This instance method discovers separator-based tokens from filename\n    stems and adds columns (e.g., `feat1`, `feat2` or `token1`, ...).\n\n    Args:\n    ----\n        path_col: Column containing path strings to analyze (default: 'path').\n        sep: Separator used to split filename stems (default: '_').\n        prefix: Column name prefix for discovered tokens (default: 'feat').\n        max_tokens: Optional cap on extracted tokens; by default uses observed max.\n        include_parent: If True, add a `parent` column containing immediate parent folder name.\n        include_all_parts: If True, add `path_part0`, `path_part1`, ... for all Path.parts.\n        token_names: Optional list of token column names or 'auto' to generate readable names.\n        enrich: If True, automatically enrich the DataFrame with path components and file stats before discovery.\n        inplace: If True, perform the operation in-place and return self. Otherwise returns a new `filoma.DataFrame`.\n\n    Returns:\n    -------\n        A new or modified `filoma.DataFrame` with discovered filename features.\n\n    \"\"\"\n    # Determine the base Polars DataFrame for feature discovery\n    base_df = self\n    if enrich and not self.with_enrich:\n        logger.info(\"Enriching DataFrame before discovering filename features\")\n        base_df = self.enrich(inplace=False)\n\n    # Polars-native implementation inlined here (formerly a top-level helper).\n    pl_df = base_df._df\n    if path_col not in pl_df.columns:\n        raise ValueError(f\"DataFrame must have a '{path_col}' column\")\n\n    stems = [Path(s).stem for s in pl_df[path_col].to_list()]\n    split_tokens = [stem.split(sep) if stem is not None else [\"\"] for stem in stems]\n    observed_max = max((len(t) for t in split_tokens), default=0)\n    if max_tokens is None:\n        eff_max = observed_max\n    else:\n        eff_max = max_tokens\n\n    # Normalize token_names\n    if token_names == \"auto\":\n        token_names_seq = None\n        auto_mode = True\n    elif isinstance(token_names, (list, tuple)):\n        token_names_seq = list(token_names)\n        auto_mode = False\n    else:\n        token_names_seq = None\n        auto_mode = False\n\n    new_cols = []\n    for i in range(eff_max):\n        if token_names_seq is not None and i &lt; len(token_names_seq) and token_names_seq[i]:\n            col_name = token_names_seq[i]\n        elif auto_mode:\n            base = prefix if prefix else \"token\"\n            col_name = f\"{base}{i + 1}\"\n        else:\n            if prefix:\n                col_name = f\"{prefix}{i + 1}\"\n            else:\n                col_name = f\"token{i + 1}\"\n\n        def pick_token(s: str, idx=i):\n            st = Path(s).stem\n            parts = st.split(sep) if st is not None else [\"\"]\n            try:\n                return parts[idx]\n            except Exception:\n                return \"\"\n\n        new_cols.append(pl.col(path_col).map_elements(pick_token, return_dtype=pl.Utf8).alias(col_name))\n\n    if include_parent:\n        new_cols.append(pl.col(path_col).map_elements(lambda s: Path(s).parent.name, return_dtype=pl.Utf8).alias(\"parent\"))\n\n    if include_all_parts:\n        parts_lists = [list(Path(s).parts) for s in pl_df[path_col].to_list()]\n        max_parts = max((len(p) for p in parts_lists), default=0)\n        for i in range(max_parts):\n            col_name = f\"path_part{i}\"\n\n            def pick_part(s: str, idx=i):\n                try:\n                    parts = list(Path(s).parts)\n                    return parts[idx]\n                except Exception:\n                    return \"\"\n\n            new_cols.append(pl.col(path_col).map_elements(pick_part, return_dtype=pl.Utf8).alias(col_name))\n\n    pl_result = pl_df.with_columns(new_cols)\n\n    # Wrap the result in a filoma.DataFrame\n    enriched_wrapper = DataFrame(pl_result)\n    enriched_wrapper.with_filename_features = True\n\n    if inplace:\n        self._df = enriched_wrapper._df\n        self.with_filename_features = True\n        if enrich and not self.with_enrich:\n            self.with_enrich = True\n        self.invalidate_pandas_cache()\n        return self\n\n    return enriched_wrapper\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.add_path_components","title":"<code>add_path_components(inplace=False)</code>","text":"<p>Add columns for path components (parent, name, stem, suffix).</p>"},{"location":"api/#filoma.dataframe.DataFrame.add_path_components--returns","title":"Returns","text":"<pre><code>New DataFrame with additional path component columns\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def add_path_components(self, inplace: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Add columns for path components (parent, name, stem, suffix).\n\n    Returns\n    -------\n        New DataFrame with additional path component columns\n\n    \"\"\"\n    df_with_components = self._df.with_columns(\n        [\n            pl.col(\"path\").map_elements(lambda x: str(Path(x).parent), return_dtype=pl.String).alias(\"parent\"),\n            pl.col(\"path\").map_elements(lambda x: Path(x).name, return_dtype=pl.String).alias(\"name\"),\n            pl.col(\"path\").map_elements(lambda x: Path(x).stem, return_dtype=pl.String).alias(\"stem\"),\n            pl.col(\"path\").map_elements(lambda x: Path(x).suffix, return_dtype=pl.String).alias(\"suffix\"),\n        ]\n    )\n    if inplace:\n        self._df = df_with_components\n        self.invalidate_pandas_cache()\n        return self\n\n    return DataFrame(df_with_components)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.describe","title":"<code>describe(percentiles=None)</code>","text":"<p>Generate descriptive statistics.</p> <pre><code>percentiles: List of percentiles to include (default: [0.25, 0.5, 0.75])\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def describe(self, percentiles: Optional[List[float]] = None) -&gt; pl.DataFrame:\n    \"\"\"Generate descriptive statistics.\n\n    Args:\n    ----\n        percentiles: List of percentiles to include (default: [0.25, 0.5, 0.75])\n\n    \"\"\"\n    # Polars' describe returns a new DataFrame summarizing columns; wrap it\n    return DataFrame(self._df.describe(percentiles=percentiles))\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.enrich","title":"<code>enrich(inplace=False)</code>","text":"<p>Enrich the DataFrame by adding features like path components, file stats, and depth.</p> <pre><code>inplace: If True, perform the operation in-place and return self.\n         If False (default), return a new DataFrame with the changes.\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def enrich(self, inplace: bool = False):\n    \"\"\"Enrich the DataFrame by adding features like path components, file stats, and depth.\n\n    Args:\n    ----\n        inplace: If True, perform the operation in-place and return self.\n                 If False (default), return a new DataFrame with the changes.\n\n    \"\"\"\n    # Chain the enrichment methods; this produces a new DataFrame wrapper\n    enriched_wrapper = self.add_path_components().add_file_stats_cols().add_depth_col()\n    enriched_wrapper.with_enrich = True\n\n    if inplace:\n        # Update the internal state of the current object\n        self._df = enriched_wrapper._df\n        self.with_enrich = True\n        self.invalidate_pandas_cache()\n        return self\n\n    # Return the new, enriched DataFrame instance\n    return enriched_wrapper\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.evaluate_duplicates","title":"<code>evaluate_duplicates(path_col='path', text_threshold=0.8, image_max_distance=5, text_k=3, show_table=True)</code>","text":"<p>Evaluate duplicates among files in the DataFrame.</p> <p>Scans the <code>path_col</code> column, runs exact, text and image duplicate detectors and prints a small Rich table summarizing counts.</p> <p>Returns the raw dict produced by <code>filoma.dedup.find_duplicates</code>.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def evaluate_duplicates(\n    self,\n    path_col: str = \"path\",\n    text_threshold: float = 0.8,\n    image_max_distance: int = 5,\n    text_k: int = 3,\n    show_table: bool = True,\n) -&gt; dict:\n    \"\"\"Evaluate duplicates among files in the DataFrame.\n\n    Scans the `path_col` column, runs exact, text and image duplicate\n    detectors and prints a small Rich table summarizing counts.\n\n    Returns the raw dict produced by `filoma.dedup.find_duplicates`.\n    \"\"\"\n    if path_col not in self._df.columns:\n        raise ValueError(f\"Column '{path_col}' not found in DataFrame\")\n\n    paths = [str(p) for p in self._df[path_col].to_list()]\n    res = _dedup.find_duplicates(\n        paths,\n        text_k=text_k,\n        text_threshold=text_threshold,\n        image_max_distance=image_max_distance,\n    )\n\n    # Summarize counts\n    exact_groups = res.get(\"exact\", [])\n    text_groups = res.get(\"text\", [])\n    image_groups = res.get(\"image\", [])\n\n    console = Console()\n    if show_table:\n        table = Table(title=\"Duplicate Summary\")\n        table.add_column(\"Type\", style=\"bold cyan\")\n        table.add_column(\"Groups\", style=\"white\")\n        table.add_column(\"Files In Groups\", style=\"white\")\n        table.add_row(\n            \"exact\",\n            str(len(exact_groups)),\n            str(sum(len(g) for g in exact_groups) if exact_groups else 0),\n        )\n        table.add_row(\n            \"text\",\n            str(len(text_groups)),\n            str(sum(len(g) for g in text_groups) if text_groups else 0),\n        )\n        table.add_row(\n            \"image\",\n            str(len(image_groups)),\n            str(sum(len(g) for g in image_groups) if image_groups else 0),\n        )\n        console.print(table)\n\n    logger.info(\n        f\"Duplicate summary: exact={len(exact_groups)} groups \"\n        f\"({sum(len(g) for g in exact_groups) if exact_groups else 0} files), \"\n        f\"text={len(text_groups)} groups \"\n        f\"({sum(len(g) for g in text_groups) if text_groups else 0} files), \"\n        f\"image={len(image_groups)} groups \"\n        f\"({sum(len(g) for g in image_groups) if image_groups else 0} files)\"\n    )\n\n    return res\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.filter_by_extension","title":"<code>filter_by_extension(extensions)</code>","text":"<p>Filter the DataFrame to only include files with specific extensions.</p> <pre><code>extensions: File extension(s) to filter by (with or without leading dot)\n</code></pre> <pre><code>Filtered DataFrame\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def filter_by_extension(self, extensions: Union[str, List[str]]) -&gt; \"DataFrame\":\n    \"\"\"Filter the DataFrame to only include files with specific extensions.\n\n    Args:\n    ----\n        extensions: File extension(s) to filter by (with or without leading dot)\n\n    Returns:\n    -------\n        Filtered DataFrame\n\n    \"\"\"\n    if isinstance(extensions, str):\n        extensions = [extensions]\n\n    # Normalize extensions (ensure they start with a dot)\n    normalized_extensions = []\n    for ext in extensions:\n        if not ext.startswith(\".\"):\n            ext = \".\" + ext\n        normalized_extensions.append(ext.lower())\n\n    filtered_df = self._df.filter(\n        pl.col(\"path\").map_elements(\n            lambda x: Path(x).suffix.lower() in normalized_extensions,\n            return_dtype=pl.Boolean,\n        )\n    )\n    return DataFrame(filtered_df)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.filter_by_pattern","title":"<code>filter_by_pattern(pattern)</code>","text":"<p>Filter the DataFrame by path pattern.</p> <pre><code>pattern: Pattern to match (uses Polars string contains)\n</code></pre> <pre><code>Filtered DataFrame\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def filter_by_pattern(self, pattern: str) -&gt; \"DataFrame\":\n    \"\"\"Filter the DataFrame by path pattern.\n\n    Args:\n    ----\n        pattern: Pattern to match (uses Polars string contains)\n\n    Returns:\n    -------\n        Filtered DataFrame\n\n    \"\"\"\n    filtered_df = self._df.filter(pl.col(\"path\").str.contains(pattern))\n    return DataFrame(filtered_df)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.from_pandas","title":"<code>from_pandas(df)</code>  <code>classmethod</code>","text":"<p>Construct a filoma.DataFrame from a pandas DataFrame.</p> <p>This is a convenience wrapper that converts the pandas DataFrame into a Polars DataFrame and wraps it. Requires pandas to be installed.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>@classmethod\ndef from_pandas(cls, df: Any) -&gt; \"DataFrame\":\n    \"\"\"Construct a filoma.DataFrame from a pandas DataFrame.\n\n    This is a convenience wrapper that converts the pandas DataFrame into\n    a Polars DataFrame and wraps it. Requires pandas to be installed.\n    \"\"\"\n    if pd is None:\n        raise RuntimeError(\"pandas is not available in this environment\")\n    # Convert via Polars for internal consistency\n    pl_df = pl.from_pandas(df)\n    return cls(pl_df)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.group_by_directory","title":"<code>group_by_directory()</code>","text":"<p>Group files by their parent directory and count them.</p>"},{"location":"api/#filoma.dataframe.DataFrame.group_by_directory--returns","title":"Returns","text":"<pre><code>Polars DataFrame with directory counts\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def group_by_directory(self) -&gt; pl.DataFrame:\n    \"\"\"Group files by their parent directory and count them.\n\n    Returns\n    -------\n        Polars DataFrame with directory counts\n\n    \"\"\"\n    # underlying `_df` is expected to be a Polars DataFrame\n    df_with_parent = self._df.with_columns(\n        [pl.col(\"path\").map_elements(lambda x: str(Path(x).parent), return_dtype=pl.String).alias(\"parent_dir\")]\n    )\n    result = df_with_parent.group_by(\"parent_dir\").len().sort(\"len\", descending=True)\n    return DataFrame(result)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.group_by_extension","title":"<code>group_by_extension()</code>","text":"<p>Group files by extension and count them.</p>"},{"location":"api/#filoma.dataframe.DataFrame.group_by_extension--returns","title":"Returns","text":"<pre><code>Polars DataFrame with extension counts\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def group_by_extension(self) -&gt; pl.DataFrame:\n    \"\"\"Group files by extension and count them.\n\n    Returns\n    -------\n        Polars DataFrame with extension counts\n\n    \"\"\"\n    # underlying `_df` is expected to be a Polars DataFrame\n    df_with_ext = self._df.with_columns(\n        [\n            pl.col(\"path\")\n            .map_elements(\n                lambda x: (Path(x).suffix.lower() if Path(x).suffix else \"&lt;no extension&gt;\"),\n                return_dtype=pl.String,\n            )\n            .alias(\"extension\")\n        ]\n    )\n    result = df_with_ext.group_by(\"extension\").len().sort(\"len\", descending=True)\n    return DataFrame(result)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.head","title":"<code>head(n=5)</code>","text":"<p>Get the first n rows.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def head(self, n: int = 5) -&gt; pl.DataFrame:\n    \"\"\"Get the first n rows.\"\"\"\n    return DataFrame(self._df.head(n))\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.info","title":"<code>info()</code>","text":"<p>Print concise summary of the DataFrame.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"Print concise summary of the DataFrame.\"\"\"\n    print(\"filoma.DataFrame\")\n    print(f\"Shape: {self.shape}\")\n    print(f\"Columns: {len(self.columns)}\")\n    print()\n\n    # Column info\n    print(\"Column details:\")\n    for i, (col, dtype) in enumerate(zip(self.columns, self.dtypes)):\n        null_count = self._df[col].null_count()\n        print(f\"  {i:2d}  {col:15s} {str(dtype):15s} {null_count:8d} nulls\")\n\n    # Memory usage approximation\n    memory_mb = sum(self._df[col].estimated_size(\"mb\") for col in self.columns)\n    print(f\"\\nEstimated memory usage: {memory_mb:.2f} MB\")\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.invalidate_pandas_cache","title":"<code>invalidate_pandas_cache()</code>","text":"<p>Clear the cached pandas conversion created by <code>to_pandas()</code>.</p> <p>Call this after mutating the underlying Polars DataFrame to ensure subsequent <code>pandas</code> accesses reflect the latest data.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def invalidate_pandas_cache(self) -&gt; None:\n    \"\"\"Clear the cached pandas conversion created by `to_pandas()`.\n\n    Call this after mutating the underlying Polars DataFrame to ensure\n    subsequent `pandas` accesses reflect the latest data.\n    \"\"\"\n    self._pd_cache = None\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.save_csv","title":"<code>save_csv(path)</code>","text":"<p>Save the DataFrame to CSV.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def save_csv(self, path: Union[str, Path]) -&gt; None:\n    \"\"\"Save the DataFrame to CSV.\"\"\"\n    self._df.write_csv(str(path))\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.save_parquet","title":"<code>save_parquet(path)</code>","text":"<p>Save the DataFrame to Parquet format.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def save_parquet(self, path: Union[str, Path]) -&gt; None:\n    \"\"\"Save the DataFrame to Parquet format.\"\"\"\n    self._df.write_parquet(str(path))\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.sort","title":"<code>sort(by, descending=False)</code>","text":"<p>Sort the DataFrame.</p> <pre><code>by: Column name(s) to sort by\ndescending: Sort in descending order\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def sort(self, by: Union[str, List[str]], descending: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Sort the DataFrame.\n\n    Args:\n    ----\n        by: Column name(s) to sort by\n        descending: Sort in descending order\n\n    \"\"\"\n    result = self._df.sort(by, descending=descending)\n    return DataFrame(result)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.split_data","title":"<code>split_data(train_val_test=(80, 10, 10), feature='path_parts', path_parts=(-1,), seed=None, discover=False, sep='_', feat_prefix='feat', max_tokens=None, include_parent=False, include_all_parts=False, token_names=None, path_col='path', verbose=True, return_type='filoma')</code>","text":"<p>Deterministically split this filoma DataFrame into train/val/test.</p> <p>This is a thin wrapper around <code>filoma.ml.split_data</code> so you can call <code>df.split_data(...)</code> directly on a filoma DataFrame instance.</p> <p>Args mirror :func:<code>filoma.ml.split_data</code> except <code>df</code> is implicit.</p> <p>By default <code>return_type='filoma'</code> so the three returned objects are filoma.DataFrame wrappers.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def split_data(\n    self,\n    train_val_test: Tuple[float, float, float] = (80, 10, 10),\n    feature: Union[str, Sequence[str]] = \"path_parts\",\n    path_parts: Optional[Iterable[int]] = (-1,),\n    seed: Optional[int] = None,\n    discover: bool = False,\n    sep: str = \"_\",\n    feat_prefix: str = \"feat\",\n    max_tokens: Optional[int] = None,\n    include_parent: bool = False,\n    include_all_parts: bool = False,\n    token_names: Optional[Union[str, Sequence[str]]] = None,\n    path_col: str = \"path\",\n    verbose: bool = True,\n    return_type: str = \"filoma\",\n):\n    \"\"\"Deterministically split this filoma DataFrame into train/val/test.\n\n    This is a thin wrapper around ``filoma.ml.split_data`` so you can call\n    ``df.split_data(...)`` directly on a filoma DataFrame instance.\n\n    Args mirror :func:`filoma.ml.split_data` except ``df`` is implicit.\n\n    By default ``return_type='filoma'`` so the three returned objects are\n    filoma.DataFrame wrappers.\n    \"\"\"\n    # Local import to avoid loading ml utilities unless used\n    from . import ml  # type: ignore\n\n    return ml.split_data(\n        self,\n        train_val_test=train_val_test,\n        feature=feature,\n        path_parts=path_parts,\n        seed=seed,\n        discover=discover,\n        sep=sep,\n        feat_prefix=feat_prefix,\n        max_tokens=max_tokens,\n        include_parent=include_parent,\n        include_all_parts=include_all_parts,\n        token_names=token_names,\n        path_col=path_col,\n        verbose=verbose,\n        return_type=return_type,\n    )\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.tail","title":"<code>tail(n=5)</code>","text":"<p>Get the last n rows.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def tail(self, n: int = 5) -&gt; pl.DataFrame:\n    \"\"\"Get the last n rows.\"\"\"\n    return DataFrame(self._df.tail(n))\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to a dictionary.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, List]:\n    \"\"\"Convert to a dictionary.\"\"\"\n    return self._df.to_dict(as_series=False)\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.to_pandas","title":"<code>to_pandas(force=False)</code>","text":"<p>Convert to a pandas DataFrame.</p> <p>By default this method will return a cached pandas conversion if one exists (for performance). Set <code>force=True</code> to reconvert from the current Polars DataFrame and update the cache.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def to_pandas(self, force: bool = False) -&gt; Any:\n    \"\"\"Convert to a pandas DataFrame.\n\n    By default this method will return a cached pandas conversion if one\n    exists (for performance). Set ``force=True`` to reconvert from the\n    current Polars DataFrame and update the cache.\n    \"\"\"\n    if pd is None:\n        raise ImportError(\"pandas is not installed. Please install it to use to_pandas().\")\n    # Convert and cache on first access or when forced\n    if force or self._pd_cache is None:\n        # Use Polars' to_pandas conversion for consistency\n        self._pd_cache = self._df.to_pandas()\n    return self._pd_cache\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.to_polars","title":"<code>to_polars()</code>","text":"<p>Get the underlying Polars DataFrame.</p> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Get the underlying Polars DataFrame.\"\"\"\n    return self._df\n</code></pre>"},{"location":"api/#filoma.dataframe.DataFrame.unique","title":"<code>unique(subset=None)</code>","text":"<p>Get unique rows.</p> <pre><code>subset: Column name(s) to consider for uniqueness\n</code></pre> Source code in <code>src/filoma/dataframe.py</code> <pre><code>def unique(self, subset: Optional[Union[str, List[str]]] = None) -&gt; \"DataFrame\":\n    \"\"\"Get unique rows.\n\n    Args:\n    ----\n        subset: Column name(s) to consider for uniqueness\n\n    \"\"\"\n    if subset is None:\n        result = self._df.unique()\n    else:\n        result = self._df.unique(subset=subset)\n    return DataFrame(result)\n</code></pre>"},{"location":"api/#ml-helpers","title":"ML helpers","text":"<p>Utilities used for dataset splitting and feature handling.</p> <p>Simple ML-style utilities for filoma DataFrame splitting.</p> <p>Provides an intuitive split_data API to split a filoma.DataFrame into train/val/test based on filename/path-derived features. The goal is a tiny, dependency-free, user-friendly interface using pathlib.Path to select path parts.</p>"},{"location":"api/#filoma.ml.split_data","title":"<code>split_data(data, train_val_test=(80, 10, 10), feature='path_parts', path_parts=(-1,), seed=None, random_state=None, discover=False, sep='_', feat_prefix='feat', max_tokens=None, include_parent=False, include_all_parts=False, token_names=None, path_col='path', verbose=True, validate_counts=True, return_type='filoma')</code>","text":"<p>Split a filoma DataFrame into train/val/test based on filename/path-derived features.</p>"},{"location":"api/#filoma.ml.split_data--parameters","title":"Parameters","text":"<p>data : Union[pl.DataFrame, Any]     A Polars DataFrame or filoma.DataFrame wrapper containing a 'path' column. train_val_test : tuple[float, float, float]     Three integers or ratios for train/val/test; they will be normalized to fractions. feature : Union[str, Sequence[str]]     Which feature to use for grouping. May be a sequence of column names     (group by existing columns), a single column name string, or one of     'path_parts', 'filename', 'stem', 'parent', 'suffix' to derive the feature from <code>path_col</code>. path_parts : Optional[Iterable[int]]     Iterable selecting indices in Path.parts (supports negative indices). Only used     when <code>feature=='path_parts'</code>. Default picks -1 (filename). seed : Optional[int]     Optional integer to alter hashing for reproducible, different splits. random_state : Optional[int]     Alias for <code>seed</code> (if provided it takes precedence). discover : bool     If True, automatically discover filename tokens and add columns named     <code>prefix1</code>, <code>prefix2</code>, ... (or <code>token1</code>... if prefix=None). sep : str     Separator used to split filename stems when <code>discover=True</code>. feat_prefix : str     Prefix to use for discovered token column names. If None, names will be     <code>token1</code>, <code>token2</code>, ... token_names : Optional[Union[str, Sequence[str]]]     Optional list of column names to use for tokens, or 'auto' to automatically     generate readable names (uses prefix if set). max_tokens : Optional[int]     Maximum number of tokens to extract when discovering. include_parent : bool     If True, add a <code>parent</code> column with the immediate parent folder name. include_all_parts : bool     If True, add columns <code>path_part0</code>, <code>path_part1</code>, ... for all Path.parts. verbose : bool     If True (default) log a short warning when achieved split counts differ noticeably     from requested ratios (common with small datasets or grouped features). validate_counts : bool     If True, log a warning when the set of unique feature values (or combined-column     feature) is not identical across the train/val/test splits. return_type : str     One of 'polars' (default), 'filoma' (wrap Polars into filoma.DataFrame), or     'pandas' (convert to pandas.DataFrame). If 'pandas' is chosen, pandas must be available. path_col : str     Column name in the input DataFrame containing file paths used for deriving features.</p>"},{"location":"api/#filoma.ml.split_data--returns","title":"Returns","text":"<pre><code>    tuple: (train_df, val_df, test_df) as Polars DataFrames.\n\nNote:\n    Splits are deterministic and grouped by the chosen feature to avoid\n    leaking similar files into multiple sets when they share the same feature.\n    The method uses sha256 hashing of the feature string to map to [0,1).\n</code></pre> Source code in <code>src/filoma/ml.py</code> <pre><code>def split_data(\n    data: Union[pl.DataFrame, Any],\n    train_val_test: Tuple[float, float, float] = (80, 10, 10),\n    feature: Union[str, Sequence[str]] = \"path_parts\",\n    path_parts: Optional[Iterable[int]] = (-1,),\n    seed: Optional[int] = None,\n    random_state: Optional[int] = None,\n    discover: bool = False,\n    sep: str = \"_\",\n    feat_prefix: str = \"feat\",\n    max_tokens: Optional[int] = None,\n    include_parent: bool = False,\n    include_all_parts: bool = False,\n    token_names: Optional[Union[str, Sequence[str]]] = None,\n    path_col: str = \"path\",\n    verbose: bool = True,\n    validate_counts: bool = True,\n    return_type: str = \"filoma\",\n) -&gt; Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Split a filoma DataFrame into train/val/test based on filename/path-derived features.\n\n    Parameters\n    ----------\n    data : Union[pl.DataFrame, Any]\n        A Polars DataFrame or filoma.DataFrame wrapper containing a 'path' column.\n    train_val_test : tuple[float, float, float]\n        Three integers or ratios for train/val/test; they will be normalized to fractions.\n    feature : Union[str, Sequence[str]]\n        Which feature to use for grouping. May be a sequence of column names\n        (group by existing columns), a single column name string, or one of\n        'path_parts', 'filename', 'stem', 'parent', 'suffix' to derive the feature from `path_col`.\n    path_parts : Optional[Iterable[int]]\n        Iterable selecting indices in Path.parts (supports negative indices). Only used\n        when `feature=='path_parts'`. Default picks -1 (filename).\n    seed : Optional[int]\n        Optional integer to alter hashing for reproducible, different splits.\n    random_state : Optional[int]\n        Alias for `seed` (if provided it takes precedence).\n    discover : bool\n        If True, automatically discover filename tokens and add columns named\n        `prefix1`, `prefix2`, ... (or `token1`... if prefix=None).\n    sep : str\n        Separator used to split filename stems when `discover=True`.\n    feat_prefix : str\n        Prefix to use for discovered token column names. If None, names will be\n        `token1`, `token2`, ...\n    token_names : Optional[Union[str, Sequence[str]]]\n        Optional list of column names to use for tokens, or 'auto' to automatically\n        generate readable names (uses prefix if set).\n    max_tokens : Optional[int]\n        Maximum number of tokens to extract when discovering.\n    include_parent : bool\n        If True, add a `parent` column with the immediate parent folder name.\n    include_all_parts : bool\n        If True, add columns `path_part0`, `path_part1`, ... for all Path.parts.\n    verbose : bool\n        If True (default) log a short warning when achieved split counts differ noticeably\n        from requested ratios (common with small datasets or grouped features).\n    validate_counts : bool\n        If True, log a warning when the set of unique feature values (or combined-column\n        feature) is not identical across the train/val/test splits.\n    return_type : str\n        One of 'polars' (default), 'filoma' (wrap Polars into filoma.DataFrame), or\n        'pandas' (convert to pandas.DataFrame). If 'pandas' is chosen, pandas must be available.\n    path_col : str\n        Column name in the input DataFrame containing file paths used for deriving features.\n\n    Returns\n    -------\n            tuple: (train_df, val_df, test_df) as Polars DataFrames.\n\n        Note:\n            Splits are deterministic and grouped by the chosen feature to avoid\n            leaking similar files into multiple sets when they share the same feature.\n            The method uses sha256 hashing of the feature string to map to [0,1).\n\n    \"\"\"\n    assert train_val_test is not None and len(train_val_test) == 3, \"train_val_test must be a tuple of three numbers\"\n\n    # Accept filoma.DataFrame wrapper or raw Polars DataFrame; discovery\n    # (if requested) will wrap raw frames into filoma.DataFrame. Defer the\n    # `path_col` existence check until after discovery to avoid unwrapping\n    # the filoma.DataFrame more than once.\n\n    ratios = _normalize_ratios(train_val_test)\n\n    # Discovery: return a filoma.DataFrame wrapper (or wrap the input if not discovering)\n    df_work = _maybe_discover(\n        data,\n        discover=discover,\n        sep=sep,\n        feat_prefix=feat_prefix,\n        max_tokens=max_tokens,\n        include_parent=include_parent,\n        include_all_parts=include_all_parts,\n        token_names=token_names,\n        path_col=path_col,\n    )\n\n    # Extract the underlying Polars DataFrame for downstream processing\n    pl_work = df_work.df\n\n    if path_col not in pl_work.columns:\n        raise ValueError(f\"DataFrame must have a '{path_col}' column\")\n\n    # Feature grouping &amp; assignment\n    feature_to_idxs, paths = _build_feature_index(pl_work, path_col=path_col, feature=feature, path_parts=path_parts)\n    # Determine effective seed: prefer `random_state` if provided for sklearn-like API\n    effective_seed = random_state if random_state is not None else seed\n    feature_assignment = _assign_features(feature_to_idxs, ratios=ratios, seed=effective_seed)\n    mask = _mask_from_assignment(feature_to_idxs, feature_assignment, total=len(paths))\n    tmp = pl_work.with_columns([pl.Series(\"_split\", mask)])\n\n    # Feature column for user convenience\n    tmp = _add_feature_column(tmp, path_col=path_col, feature=feature, path_parts=path_parts)\n\n    # Split\n    train_df = tmp.filter(pl.col(\"_split\") == \"train\").drop(\"_split\")\n    val_df = tmp.filter(pl.col(\"_split\") == \"val\").drop(\"_split\")\n    test_df = tmp.filter(pl.col(\"_split\") == \"test\").drop(\"_split\")\n\n    # Validate that the unique feature values are represented equally across splits\n    if validate_counts:\n        PATH_MODES = {\"path_parts\", \"filename\", \"stem\", \"parent\", \"suffix\"}\n        if isinstance(feature, (list, tuple)) or (isinstance(feature, str) and feature not in PATH_MODES and feature in pl_work.columns):\n            feat_col = \"_feat_group\"\n        else:\n            feat_col = \"_feat_path_parts\" if feature == \"path_parts\" else f\"_feat_{feature}\"\n\n        try:\n            train_set = set(train_df[feat_col].to_list())\n            val_set = set(val_df[feat_col].to_list())\n            test_set = set(test_df[feat_col].to_list())\n        except Exception:\n            # If something unexpected happens (missing column), skip validation\n            train_set = val_set = test_set = set()\n\n        if not (train_set == val_set == test_set):\n            union = train_set | val_set | test_set\n            missing_in_train = list((union - train_set))[:5]\n            missing_in_val = list((union - val_set))[:5]\n            missing_in_test = list((union - test_set))[:5]\n            logger.warning(\n                (\n                    \"filoma.ml.split_data: unique feature values differ across splits for '{}' -\"\n                    \" counts train={}, val={}, test={}; examples missing_in_train={},\"\n                    \" missing_in_val={}, missing_in_test={}\"\n                ),\n                feat_col,\n                len(train_set),\n                len(val_set),\n                len(test_set),\n                missing_in_train,\n                missing_in_val,\n                missing_in_test,\n            )\n\n    _maybe_log_ratio_drift(len(train_df), len(val_df), len(test_df), len(paths), ratios, verbose)\n\n    # Return requested type (default: filoma.DataFrame wrappers)\n    if return_type == \"filoma\" or return_type is None:\n        # Lazy import filoma.DataFrame wrapper to avoid heavy imports at module import time\n        try:\n            from .dataframe import DataFrame as FDataFrame\n        except Exception:\n            from filoma.dataframe import DataFrame as FDataFrame\n\n        return FDataFrame(train_df), FDataFrame(val_df), FDataFrame(test_df)\n\n    if return_type == \"polars\":\n        return train_df, val_df, test_df\n\n    if return_type == \"pandas\":\n        try:\n            return train_df.to_pandas(), val_df.to_pandas(), test_df.to_pandas()\n        except Exception as e:\n            raise RuntimeError(f\"Failed to convert to pandas DataFrame: {e}\")\n\n    raise ValueError(f\"Unknown return_type='{return_type}'\")\n</code></pre>"},{"location":"api/#directory-profiler","title":"Directory profiler","text":"<p>The directory profiling API and configuration helpers.</p> <p>Directory profiling utilities.</p> <p>This module provides :class:<code>DirectoryProfiler</code> which analyzes directory trees and returns a :class:<code>DirectoryAnalysis</code> dataclass with summary statistics and optional DataFrame support.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis","title":"<code>DirectoryAnalysis</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Mapping</code></p> <p>Structured container for directory analysis results.</p> <p>This is the canonical, dataclass-first return value for directory probes. Use :meth:<code>to_dict</code> to convert to a plain dict and :meth:<code>to_df</code> to access the optional DataFrame. The class exists to provide a typed, ergonomic API for programmatic consumption.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>@dataclass\nclass DirectoryAnalysis(Mapping):\n    \"\"\"Structured container for directory analysis results.\n\n    This is the canonical, dataclass-first return value for directory probes.\n    Use :meth:`to_dict` to convert to a plain dict and :meth:`to_df`\n    to access the optional DataFrame. The class exists to provide a typed,\n    ergonomic API for programmatic consumption.\n    \"\"\"\n\n    path: str\n    summary: Dict\n    file_extensions: Dict\n    common_folder_names: Dict\n    empty_folders: List[str]\n    top_folders_by_file_count: List\n    depth_distribution: Dict\n    dataframe: Optional[\"DataFrame\"] = None\n    timing: Optional[Dict] = None\n    dataframe_note: Optional[str] = None\n\n    @classmethod\n    def from_dict(cls, d: Dict) -&gt; \"DirectoryAnalysis\":\n        \"\"\"Create a :class:`DirectoryAnalysis` from a plain dict.\n\n        Parameters\n        ----------\n        d : dict\n            Dictionary in the shape produced by :meth:`DirectoryProfiler.probe`.\n\n        Returns\n        -------\n        DirectoryAnalysis\n            Constructed dataclass instance.\n\n        \"\"\"\n        return cls(\n            path=d.get(\"path\"),\n            summary=d.get(\"summary\", {}),\n            file_extensions=d.get(\"file_extensions\", {}),\n            common_folder_names=d.get(\"common_folder_names\", {}),\n            empty_folders=d.get(\"empty_folders\", []),\n            top_folders_by_file_count=d.get(\"top_folders_by_file_count\", []),\n            depth_distribution=d.get(\"depth_distribution\", {}),\n            dataframe=d.get(\"dataframe\"),\n            timing=d.get(\"timing\"),\n            dataframe_note=d.get(\"dataframe_note\"),\n        )\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Return a plain ``dict`` representation of this analysis.\"\"\"\n        # Convert to a plain dict shape\n        d = {\n            \"path\": self.path,\n            \"summary\": self.summary,\n            \"file_extensions\": self.file_extensions,\n            \"common_folder_names\": self.common_folder_names,\n            \"empty_folders\": self.empty_folders,\n            \"top_folders_by_file_count\": self.top_folders_by_file_count,\n            \"depth_distribution\": self.depth_distribution,\n        }\n        if self.dataframe is not None:\n            d[\"dataframe\"] = self.dataframe\n        if self.timing is not None:\n            d[\"timing\"] = self.timing\n        if self.dataframe_note is not None:\n            d[\"dataframe_note\"] = self.dataframe_note\n        return d\n\n    def to_df(self) -&gt; Optional[\"DataFrame\"]:\n        \"\"\"Return the attached DataFrame wrapper or log a helpful warning when absent.\n\n        This method used to silently return None when no DataFrame was built which\n        often confused interactive users calling ``analysis.to_df()``. We now log a\n        warning explaining the likely causes (DataFrame building disabled or polars\n        not installed) to surface actionable next steps.\n        \"\"\"\n        if self.dataframe is None:\n            # Emit a helpful, actionable warning rather than silently returning None\n            logger.warning(\n                \"No DataFrame available for analysis at path {path!s}. \"\n                \"DataFrame building is disabled by default or 'polars' is not installed. \"\n                \"Call DirectoryProfiler(build_dataframe=True) or use filoma.probe_to_df(...) to obtain a DataFrame.\",\n                path=self.path,\n            )\n        return self.dataframe\n\n    def as_dict(self) -&gt; Dict:\n        \"\"\"Alias for :meth:`to_dict`.\n\n        Provided for backward compatibility with dict-based APIs.\n        \"\"\"\n        return self.to_dict()\n\n    # Convenience printing helpers so callers can write `analysis.print_summary()`\n    # or `analysis.print_report()` without importing DirectoryProfiler. These\n    # delegate to the existing DirectoryProfiler rich printers for consistency.\n    def print_summary(self, profiler: \"DirectoryProfiler\" = None):\n        \"\"\"Pretty-print a short summary using the rich-based DirectoryProfiler printer.\n\n        If `profiler` is provided it will be used (useful to customize show_progress,\n        console, or other profiler settings); otherwise a default profiler is created.\n        \"\"\"\n        # Local import to avoid import cycles at module import time\n        if profiler is None:\n            profiler = DirectoryProfiler()\n        profiler.print_summary(self)\n\n    def print_report(self, profiler: \"DirectoryProfiler\" = None):\n        \"\"\"Pretty-print the full report (summary + extras) via DirectoryProfiler.\n\n        This is an alias for `print_summary` + additional report sections; kept\n        as a separate method name for discoverability and symmetry with other\n        profilers in the project.\n        \"\"\"\n        if profiler is None:\n            profiler = DirectoryProfiler()\n        profiler.print_report(self)\n\n    # Mapping protocol implementations so callers can still use dict-like access\n    # (e.g., result['summary']) even though the canonical return type is a dataclass.\n    def _as_dict(self) -&gt; Dict:\n        return self.to_dict()\n\n    def __getitem__(self, key):\n        \"\"\"Mapping-style access to analysis fields by key.\"\"\"\n        return self._as_dict()[key]\n\n    def __iter__(self):\n        \"\"\"Iterate over analysis mapping keys.\"\"\"\n        return iter(self._as_dict())\n\n    def __len__(self):\n        \"\"\"Return number of top-level fields in the analysis mapping.\"\"\"\n        return len(self._as_dict())\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Mapping-style access to analysis fields by key.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Mapping-style access to analysis fields by key.\"\"\"\n    return self._as_dict()[key]\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over analysis mapping keys.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over analysis mapping keys.\"\"\"\n    return iter(self._as_dict())\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.__len__","title":"<code>__len__()</code>","text":"<p>Return number of top-level fields in the analysis mapping.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def __len__(self):\n    \"\"\"Return number of top-level fields in the analysis mapping.\"\"\"\n    return len(self._as_dict())\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.as_dict","title":"<code>as_dict()</code>","text":"<p>Alias for :meth:<code>to_dict</code>.</p> <p>Provided for backward compatibility with dict-based APIs.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def as_dict(self) -&gt; Dict:\n    \"\"\"Alias for :meth:`to_dict`.\n\n    Provided for backward compatibility with dict-based APIs.\n    \"\"\"\n    return self.to_dict()\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Create a :class:<code>DirectoryAnalysis</code> from a plain dict.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.from_dict--parameters","title":"Parameters","text":"<p>d : dict     Dictionary in the shape produced by :meth:<code>DirectoryProfiler.probe</code>.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.from_dict--returns","title":"Returns","text":"<p>DirectoryAnalysis     Constructed dataclass instance.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Dict) -&gt; \"DirectoryAnalysis\":\n    \"\"\"Create a :class:`DirectoryAnalysis` from a plain dict.\n\n    Parameters\n    ----------\n    d : dict\n        Dictionary in the shape produced by :meth:`DirectoryProfiler.probe`.\n\n    Returns\n    -------\n    DirectoryAnalysis\n        Constructed dataclass instance.\n\n    \"\"\"\n    return cls(\n        path=d.get(\"path\"),\n        summary=d.get(\"summary\", {}),\n        file_extensions=d.get(\"file_extensions\", {}),\n        common_folder_names=d.get(\"common_folder_names\", {}),\n        empty_folders=d.get(\"empty_folders\", []),\n        top_folders_by_file_count=d.get(\"top_folders_by_file_count\", []),\n        depth_distribution=d.get(\"depth_distribution\", {}),\n        dataframe=d.get(\"dataframe\"),\n        timing=d.get(\"timing\"),\n        dataframe_note=d.get(\"dataframe_note\"),\n    )\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.print_report","title":"<code>print_report(profiler=None)</code>","text":"<p>Pretty-print the full report (summary + extras) via DirectoryProfiler.</p> <p>This is an alias for <code>print_summary</code> + additional report sections; kept as a separate method name for discoverability and symmetry with other profilers in the project.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_report(self, profiler: \"DirectoryProfiler\" = None):\n    \"\"\"Pretty-print the full report (summary + extras) via DirectoryProfiler.\n\n    This is an alias for `print_summary` + additional report sections; kept\n    as a separate method name for discoverability and symmetry with other\n    profilers in the project.\n    \"\"\"\n    if profiler is None:\n        profiler = DirectoryProfiler()\n    profiler.print_report(self)\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.print_summary","title":"<code>print_summary(profiler=None)</code>","text":"<p>Pretty-print a short summary using the rich-based DirectoryProfiler printer.</p> <p>If <code>profiler</code> is provided it will be used (useful to customize show_progress, console, or other profiler settings); otherwise a default profiler is created.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_summary(self, profiler: \"DirectoryProfiler\" = None):\n    \"\"\"Pretty-print a short summary using the rich-based DirectoryProfiler printer.\n\n    If `profiler` is provided it will be used (useful to customize show_progress,\n    console, or other profiler settings); otherwise a default profiler is created.\n    \"\"\"\n    # Local import to avoid import cycles at module import time\n    if profiler is None:\n        profiler = DirectoryProfiler()\n    profiler.print_summary(self)\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.to_df","title":"<code>to_df()</code>","text":"<p>Return the attached DataFrame wrapper or log a helpful warning when absent.</p> <p>This method used to silently return None when no DataFrame was built which often confused interactive users calling <code>analysis.to_df()</code>. We now log a warning explaining the likely causes (DataFrame building disabled or polars not installed) to surface actionable next steps.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def to_df(self) -&gt; Optional[\"DataFrame\"]:\n    \"\"\"Return the attached DataFrame wrapper or log a helpful warning when absent.\n\n    This method used to silently return None when no DataFrame was built which\n    often confused interactive users calling ``analysis.to_df()``. We now log a\n    warning explaining the likely causes (DataFrame building disabled or polars\n    not installed) to surface actionable next steps.\n    \"\"\"\n    if self.dataframe is None:\n        # Emit a helpful, actionable warning rather than silently returning None\n        logger.warning(\n            \"No DataFrame available for analysis at path {path!s}. \"\n            \"DataFrame building is disabled by default or 'polars' is not installed. \"\n            \"Call DirectoryProfiler(build_dataframe=True) or use filoma.probe_to_df(...) to obtain a DataFrame.\",\n            path=self.path,\n        )\n    return self.dataframe\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryAnalysis.to_dict","title":"<code>to_dict()</code>","text":"<p>Return a plain <code>dict</code> representation of this analysis.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Return a plain ``dict`` representation of this analysis.\"\"\"\n    # Convert to a plain dict shape\n    d = {\n        \"path\": self.path,\n        \"summary\": self.summary,\n        \"file_extensions\": self.file_extensions,\n        \"common_folder_names\": self.common_folder_names,\n        \"empty_folders\": self.empty_folders,\n        \"top_folders_by_file_count\": self.top_folders_by_file_count,\n        \"depth_distribution\": self.depth_distribution,\n    }\n    if self.dataframe is not None:\n        d[\"dataframe\"] = self.dataframe\n    if self.timing is not None:\n        d[\"timing\"] = self.timing\n    if self.dataframe_note is not None:\n        d[\"dataframe_note\"] = self.dataframe_note\n    return d\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler","title":"<code>DirectoryProfiler</code>","text":"<p>Analyzes directory structures for basic statistics and patterns.</p> <p>Provides file counts, folder patterns, empty directories, and extension analysis.</p> <p>Can use either a pure Python implementation or a faster Rust implementation when available. Supports both sequential and parallel Rust processing.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>class DirectoryProfiler:\n    \"\"\"Analyzes directory structures for basic statistics and patterns.\n\n    Provides file counts, folder patterns, empty directories, and extension analysis.\n\n    Can use either a pure Python implementation or a faster Rust implementation\n    when available. Supports both sequential and parallel Rust processing.\n\n    \"\"\"\n\n    def __init__(self, config: \"DirectoryProfilerConfig\"):\n        \"\"\"Initialize the directory profiler.\n\n        The profiler is configured with a `DirectoryProfilerConfig` instance which\n        holds options such as whether to use Rust acceleration, parallel processing,\n        fd integration, thresholding for parallelism, DataFrame building, and progress\n        reporting callbacks. Pass a `DirectoryProfilerConfig` object as the single\n        `config` argument. See `DirectoryProfilerConfig` for descriptions of each\n        configurable field.\n        \"\"\"\n        # Expect a DirectoryProfilerConfig object \u2014 no legacy kwargs supported.\n        if not hasattr(config, \"__class__\") or config.__class__.__name__ != \"DirectoryProfilerConfig\":\n            raise TypeError(\"DirectoryProfiler requires a DirectoryProfilerConfig instance as the sole argument\")\n\n        self.console = Console()\n        self.config = config\n\n        # Set simple aliases for common flags to preserve prior attribute names\n        # Internal availability checks are still performed below.\n        self.search_backend = config.search_backend\n        self.parallel_threshold = config.parallel_threshold\n        self._fast_path_only = config.fast_path_only\n        self.progress_callback = config.progress_callback\n\n        # Validate availability and enforce clear relationships\n        # Use explicit booleans from the config\n        if config.use_rust and not RUST_AVAILABLE:\n            raise RuntimeError(\"Rust implementation requested but not available in this build\")\n        if config.use_parallel and not RUST_PARALLEL_AVAILABLE:\n            raise RuntimeError(\"Parallel Rust requested but not available\")\n        if config.use_async and not RUST_ASYNC_AVAILABLE:\n            raise RuntimeError(\"Async Rust prober requested but not available in this build\")\n        if config.use_fd and not FD_AVAILABLE:\n            raise RuntimeError(\"fd integration requested but not available in this environment\")\n        if config.build_dataframe and not DATAFRAME_AVAILABLE:\n            raise RuntimeError(\"DataFrame building requested but Polars/DataFrame support is not available\")\n\n        # Network args only apply when use_async is True (explicit)\n        if not config.use_async and any(\n            (\n                config.network_concurrency != 64,\n                config.network_timeout_ms != 5000,\n                config.network_retries != 0,\n            )\n        ):\n            raise ValueError(\"Network tuning parameters only apply when use_async=True\")\n\n        # Threads only applies when use_fd is True\n        if config.threads is not None and not config.use_fd:\n            raise ValueError(\"'threads' setting only applies when use_fd=True\")\n\n        # Decide which implementation to use based on search_backend and availability\n        backend_choice = config.search_backend\n        if backend_choice == \"auto\":\n            # Honor explicit user preferences when provided.\n            # If both backends are explicitly requested and available, prefer fd\n            if config.use_fd and config.use_rust and FD_AVAILABLE and RUST_AVAILABLE:\n                backend_choice = \"fd\"\n            # If user explicitly requested Rust and it's available, use it\n            elif config.use_rust and RUST_AVAILABLE:\n                backend_choice = \"rust\"\n            # If user explicitly requested fd and it's available, use it\n            elif config.use_fd and FD_AVAILABLE:\n                backend_choice = \"fd\"\n            else:\n                # No explicit preference from user -&gt; auto-detect best available\n                if RUST_AVAILABLE:\n                    backend_choice = \"rust\"\n                elif FD_AVAILABLE:\n                    backend_choice = \"fd\"\n                else:\n                    backend_choice = \"python\"\n\n        if backend_choice == \"rust\":\n            self.use_rust = True\n            self.use_fd = False\n        elif backend_choice == \"fd\":\n            self.use_rust = False\n            self.use_fd = True\n        else:\n            self.use_rust = False\n            self.use_fd = False\n\n        # Parallel/async/other toggles come directly from config (already validated)\n        self.use_parallel = bool(config.use_parallel and self.use_rust)\n        self.use_async = bool(config.use_async and self.use_rust)\n\n        # Other instance-level flags\n        self.build_dataframe = bool(config.build_dataframe)\n        self.return_absolute_paths = bool(config.return_absolute_paths)\n        # Progress handling\n        if _is_interactive_environment() and config.show_progress:\n            logger.debug(\"Interactive environment detected, disabling progress bars to avoid conflicts\")\n            self.show_progress = False\n        else:\n            self.show_progress = bool(config.show_progress)\n\n        # Network tuning (only valid if use_async True)\n        self.network_concurrency = config.network_concurrency\n        self.network_timeout_ms = config.network_timeout_ms\n        self.network_retries = config.network_retries\n\n        # Threads forwarded to fd if using fd backend\n        self.threads = config.threads if self.use_fd else None\n\n        # Defer fd integration initialization until actually used\n        self.fd_integration = None\n\n    def is_rust_available(self) -&gt; bool:\n        \"\"\"Check if Rust implementation is available and being used.\n\n        Returns\n        -------\n            True if Rust implementation is available and enabled, False otherwise\n\n        \"\"\"\n        return self.use_rust and RUST_AVAILABLE\n\n    def is_parallel_available(self) -&gt; bool:\n        \"\"\"Check if parallel Rust implementation is available and being used.\n\n        Returns\n        -------\n            True if parallel Rust implementation is available and enabled, False otherwise\n\n        \"\"\"\n        return self.use_parallel and RUST_PARALLEL_AVAILABLE\n\n    def is_fd_available(self) -&gt; bool:\n        \"\"\"Check if fd integration is available and being used.\n\n        Returns\n        -------\n            True if fd is available and enabled, False otherwise\n\n        \"\"\"\n        # Use FD_AVAILABLE to reflect whether the fd integration package is importable\n        # Tests may monkeypatch FD_AVAILABLE without having the fd binary present.\n        return self.use_fd and FD_AVAILABLE\n\n    def get_implementation_info(self) -&gt; Dict[str, bool]:\n        \"\"\"Get information about which implementations are available and being used.\n\n        Returns\n        -------\n            Dictionary with implementation availability status\n\n        \"\"\"\n        return {\n            \"rust_available\": RUST_AVAILABLE,\n            \"rust_parallel_available\": RUST_PARALLEL_AVAILABLE,\n            \"rust_async_available\": RUST_ASYNC_AVAILABLE,\n            \"fd_available\": FD_AVAILABLE,\n            \"dataframe_available\": DATAFRAME_AVAILABLE,\n            \"using_rust\": self.use_rust,\n            \"using_parallel\": self.use_parallel,\n            \"using_async\": bool(self.use_async and RUST_ASYNC_AVAILABLE),\n            \"using_fd\": self.use_fd,\n            \"using_dataframe\": self.build_dataframe,\n            \"return_absolute_paths\": self.return_absolute_paths,\n            \"search_backend\": self.search_backend,\n            \"python_fallback\": not (self.use_rust or self.use_fd),\n        }\n\n    def probe(self, path: str, max_depth: Optional[int] = None, threads: Optional[int] = None) -&gt; \"DirectoryAnalysis\":\n        \"\"\"Analyze a directory tree and return comprehensive statistics.\n\n        Args:\n        ----\n            path: Path to the root directory to probe\n            max_depth: Maximum depth to traverse (None for unlimited)\n            threads: Optional override for number of threads when using fd backend\n\n        Returns:\n        -------\n            A :class:`DirectoryAnalysis` instance containing analysis results\n\n        \"\"\"\n        start_time = time.time()\n\n        # Choose the best backend\n        backend = self._choose_backend()\n\n        # Log the start of analysis\n        impl_type = self._get_impl_display_name(backend)\n        logger.info(f\"Starting directory analysis of '{path}' using {impl_type} implementation\")\n\n        try:\n            if backend == \"fd\":\n                # threads param overrides instance threads when provided\n                chosen_threads = threads if threads is not None else self.threads\n                result = self._probe_fd(path, max_depth, threads=chosen_threads)\n            elif backend == \"rust\":\n                result = self._probe_rust(path, max_depth, fast_path_only=self._fast_path_only)\n            else:\n                result = self._probe_python(path, max_depth)\n\n            # Calculate and log timing\n            elapsed_time = time.time() - start_time\n            total_items = result[\"summary\"][\"total_files\"] + result[\"summary\"][\"total_folders\"]\n\n            logger.success(\n                f\"Directory analysis completed in {elapsed_time:.2f}s - \"\n                f\"Found {total_items:,} items ({result['summary']['total_files']:,} files, \"\n                f\"{result['summary']['total_folders']:,} folders) using {impl_type}\"\n            )\n\n            # Add timing information to result\n            result[\"timing\"] = {\n                \"elapsed_seconds\": elapsed_time,\n                \"implementation\": impl_type,\n                \"items_per_second\": (total_items / elapsed_time if elapsed_time &gt; 0 else 0),\n            }\n\n            # Return a structured dataclass by default for easier programmatic use\n            return DirectoryAnalysis.from_dict(result)\n\n        except Exception as e:\n            elapsed_time = time.time() - start_time\n            logger.error(f\"Directory analysis failed after {elapsed_time:.2f}s: {str(e)}\")\n            raise\n\n    def _choose_backend(self) -&gt; str:\n        \"\"\"Choose the best available backend based on settings and availability.\n\n        Returns\n        -------\n            Backend name: \"fd\", \"rust\", or \"python\"\n\n        \"\"\"\n        # If search_backend is 'auto' and neither rust nor fd are requested\n        # by the resolved preferences, prefer the Python backend. This avoids\n        # forcing Python when the user specifically preferred fd.\n        if self.search_backend == \"auto\" and not (self.use_rust or self.use_fd):\n            return \"python\"\n\n        if self.search_backend == \"fd\":\n            if self.use_fd and FD_AVAILABLE:\n                return \"fd\"\n            else:\n                logger.warning(\"fd backend requested but not available, falling back to auto selection\")\n\n        elif self.search_backend == \"rust\":\n            if self.use_rust:\n                return \"rust\"\n            else:\n                logger.warning(\"Rust backend requested but not available, falling back to auto selection\")\n\n        elif self.search_backend == \"python\":\n            return \"python\"\n\n        # Auto selection logic\n        if self.search_backend == \"auto\":\n            # Based on cold cache benchmarks Rust tends to be the fastest\n            # general-purpose backend. Prefer Rust when available; fall back\n            # to fd when Rust is not enabled/available but fd is explicitly\n            # enabled by the user.\n            if self.use_rust and RUST_AVAILABLE:\n                return \"rust\"\n            elif self.use_fd and FD_AVAILABLE:\n                return \"fd\"\n            else:\n                return \"python\"\n\n        # Fallback to python if nothing else works\n        return \"python\"\n\n    def _get_impl_display_name(self, backend: str) -&gt; str:\n        \"\"\"Get display name for implementation type.\"\"\"\n        if backend == \"fd\":\n            return \"\ud83d\udd0d fd\"\n        elif backend == \"rust\":\n            if self.use_parallel and RUST_PARALLEL_AVAILABLE:\n                return \"\ud83e\udd80 Rust (Parallel)\"\n            else:\n                return \"\ud83e\udd80 Rust (Sequential)\"\n        else:\n            return \"\ud83d\udc0d Python\"\n\n    def _probe_fd(self, path: str, max_depth: Optional[int] = None, threads: Optional[int] = None) -&gt; Dict:\n        \"\"\"Use fd for file discovery + Python for analysis.\n\n        This hybrid approach leverages fd's ultra-fast file discovery\n        while using Python for statistical analysis to maintain\n        consistency with other backends.\n        \"\"\"\n        # Lazily initialize fd integration here. This ensures tests that\n        # monkeypatch FD_AVAILABLE can control availability without the\n        # constructor eagerly probing the environment.\n        if self.fd_integration is None:\n            # If the fd integration package wasn't importable at module\n            # import time, reflect that now.\n            if not FD_AVAILABLE:\n                raise RuntimeError(\"fd integration not available\")\n            try:\n                self.fd_integration = FdIntegration()\n                if not self.fd_integration.is_available():\n                    # fd binary is not usable on this system\n                    self.fd_integration = None\n                    raise RuntimeError(\"fd integration not available\")\n            except Exception:\n                self.fd_integration = None\n                raise RuntimeError(\"fd integration not available\")\n\n        progress = None\n        task_id = None\n\n        if self.show_progress:\n            progress = Progress(\n                SpinnerColumn(),\n                TextColumn(\"[bold blue]Discovering files with fd...\"),\n                BarColumn(),\n                TextColumn(\"[progress.percentage]{task.percentage:&gt;3.0f}%\"),\n                TimeElapsedColumn(),\n                console=self.console,\n                transient=True,\n            )\n            progress.start()\n            task_id = progress.add_task(\"Discovering...\", total=None)\n\n        # Run the fd discovery and analysis inside a try so we always stop\n        # the progress bar in the finally block below.\n        try:\n            # Use fd to get all files and directories rapidly\n            if progress and task_id is not None:\n                progress.update(task_id, description=\"[bold blue]Finding files...\")\n\n            # fd's --max-depth applies to the matched path; to match the\n            # Python/Rust semantics where files up to depth (max_depth + 1)\n            # are included, when a max_depth is provided for the probe we\n            # increase the file search depth by 1.\n            file_max_depth = None if max_depth is None else max_depth + 1\n            # When using fd in auto mode, prefer flags that match a raw\n            # traversal (include hidden files, don't honor ignore files, follow symlinks)\n            fd_find_kwargs = {\n                \"path\": path,\n                \"file_types\": [\"f\"],\n                \"max_depth\": file_max_depth,\n                \"absolute_paths\": self.return_absolute_paths,\n                \"threads\": threads,\n            }\n            if self.search_backend == \"auto\":\n                fd_find_kwargs.update({\"search_hidden\": True, \"no_ignore\": True, \"follow_links\": True})\n\n            all_files = self.fd_integration.find(**fd_find_kwargs)\n\n            if progress and task_id is not None:\n                progress.update(task_id, description=\"[bold blue]Finding directories...\")\n\n            all_dirs = self.fd_integration.find(\n                path=path,\n                file_types=[\"d\"],  # Directories only\n                max_depth=max_depth,\n                absolute_paths=self.return_absolute_paths,\n                threads=threads,\n                search_hidden=True if self.search_backend == \"auto\" else False,\n                no_ignore=True if self.search_backend == \"auto\" else False,\n                follow_links=True if self.search_backend == \"auto\" else False,\n            )\n\n            # Convert to Path objects for analysis\n            root_path_obj = Path(path).resolve()\n            all_paths = [Path(p) for p in all_files + all_dirs]\n\n            # If DataFrame building is enabled and DataFrame support is available,\n            # build a prebuilt DataFrame from the fd results and pass it to the\n            # Python probing logic to avoid rebuilding the DataFrame there.\n            prebuilt_df = None\n            if self.build_dataframe and DATAFRAME_AVAILABLE:\n                try:\n                    prebuilt_df = DataFrame([str(p) for p in all_paths])\n                except Exception:\n                    # If DataFrame construction fails for any reason, fall back\n                    # to letting _probe_paths_python collect paths itself.\n                    prebuilt_df = None\n\n            if progress and task_id is not None:\n                progress.update(task_id, description=\"[bold yellow]Analyzing discovered files...\")\n                progress.update(task_id, total=100, completed=50)\n\n                # Now probe the discovered paths using Python logic\n                # Pass the existing progress to avoid conflicts. If a prebuilt DataFrame\n                # exists, provide it to avoid rebuilding the DataFrame inside the probe.\n                result = self._probe_paths_python(\n                    root_path_obj,\n                    all_paths,\n                    max_depth,\n                    existing_progress=progress,\n                    existing_task_id=task_id,\n                    prebuilt_dataframe=prebuilt_df,\n                )\n            else:\n                # No progress provided; run probe without progress integration\n                result = self._probe_paths_python(\n                    root_path_obj,\n                    all_paths,\n                    max_depth,\n                    existing_progress=None,\n                    existing_task_id=None,\n                    prebuilt_dataframe=prebuilt_df,\n                )\n\n            if progress and task_id is not None:\n                progress.update(task_id, description=\"[bold green]Analysis complete!\")\n                progress.update(task_id, completed=100)\n\n            return result\n\n        finally:\n            if progress:\n                progress.stop()\n\n    def sample_paths(self, path: str, sample_size: int = 20) -&gt; Dict[str, List[str]]:\n        \"\"\"Return small samples of paths for quick backend-diffing.\n\n        Returns a dict with keys 'fd_files', 'fd_dirs', 'python_files'. Rust currently\n        does not expose a path list in the public API so it is omitted (you can\n        re-run the Rust prober separately if needed).\n        \"\"\"\n        samples = {\"fd_files\": [], \"fd_dirs\": [], \"python_files\": []}\n        try:\n            if FD_AVAILABLE:\n                fd = FdIntegration()\n                samples[\"fd_files\"] = fd.find(\n                    path=path,\n                    file_types=[\"f\"],\n                    max_results=sample_size,\n                    search_hidden=True,\n                    no_ignore=True,\n                    follow_links=True,\n                    absolute_paths=self.return_absolute_paths,\n                )\n                samples[\"fd_dirs\"] = fd.find(\n                    path=path,\n                    file_types=[\"d\"],\n                    max_results=sample_size,\n                    search_hidden=True,\n                    no_ignore=True,\n                    follow_links=True,\n                    absolute_paths=self.return_absolute_paths,\n                )\n        except Exception:\n            samples[\"fd_files\"] = []\n            samples[\"fd_dirs\"] = []\n\n        # Python sample\n        try:\n            root = Path(path)\n            python_files = []\n            for i, p in enumerate(root.rglob(\"*\")):\n                if p.is_file():\n                    python_files.append(str(p.resolve()))\n                if len(python_files) &gt;= sample_size:\n                    break\n            samples[\"python_files\"] = python_files\n        except Exception:\n            samples[\"python_files\"] = []\n\n        return samples\n\n    def _probe_paths_python(\n        self,\n        path_root: Path,\n        all_paths: List[Path],\n        max_depth: Optional[int] = None,\n        existing_progress=None,\n        existing_task_id=None,\n        prebuilt_dataframe=None,\n    ) -&gt; Dict:\n        \"\"\"Analyze pre-discovered paths using Python logic.\n\n        This method takes a list of paths (from fd or other source) and performs\n        the statistical analysis to maintain consistency with the Python backend.\n\n        Args:\n        ----\n            path: Root directory being probed\n            all_paths: List of paths to probe\n            max_depth: Maximum depth for analysis\n            existing_progress: Existing progress bar to reuse (avoids conflicts)\n            existing_task_id: Existing task ID to update\n            path_root: The resolved root Path for the probe (used for depth calculations)\n            prebuilt_dataframe: Optional DataFrame supplied to avoid rebuilding inside probe\n\n        \"\"\"\n        # Initialize counters and collections\n        file_count = 0\n        folder_count = 1  # Start with 1 to count the root directory itself\n        total_size = 0\n        empty_folders = []\n        file_extensions = Counter()\n        folder_names = Counter()\n        files_per_folder = defaultdict(int)\n        depth_stats = defaultdict(int)\n\n        # Count the root directory at depth 0\n        depth_stats[0] = 1\n\n        # Collection for DataFrame if enabled. If a prebuilt_dataframe is provided\n        # (e.g. from fd results), skip collecting paths and attach it at the end.\n        dataframe_paths = [] if (self.build_dataframe and prebuilt_dataframe is None) else None\n\n        # Sort paths for better progress indication (guard against None or unsortable lists)\n        if all_paths:\n            try:\n                all_paths.sort()\n            except Exception:\n                # If sorting fails (e.g., mixed types), ignore and proceed\n                pass\n\n        progress = existing_progress\n        task_id = existing_task_id\n        processed_items = 0\n        progress_owned = False  # Track if we own the progress bar\n\n        if self.show_progress and existing_progress is None:\n            # Only create new progress if none was provided\n            progress = Progress(\n                SpinnerColumn(),\n                TextColumn(\"[bold blue]Analyzing file metadata...\"),\n                BarColumn(),\n                TextColumn(\"[progress.percentage]{task.percentage:&gt;3.0f}%\"),\n                TextColumn(\"({task.completed:,}/{task.total:,} items)\"),\n                TimeElapsedColumn(),\n                console=self.console,\n                transient=True,\n            )\n            progress.start()\n            task_id = progress.add_task(\"Analyzing...\", total=len(all_paths))\n            progress_owned = True\n        elif existing_progress and existing_task_id:\n            # Update existing progress for the analysis phase\n            existing_progress.update(\n                existing_task_id,\n                description=\"[bold yellow]Analyzing file metadata...\",\n                total=len(all_paths),\n                completed=0,\n            )\n\n        try:\n            for current_path in all_paths:\n                processed_items += 1\n\n                # Update progress\n                if progress and task_id is not None:\n                    if processed_items % 100 == 0:\n                        progress.update(task_id, completed=processed_items)\n\n                    if self.progress_callback:\n                        self.progress_callback(\n                            f\"Processing: {current_path.name}\",\n                            processed_items,\n                            len(all_paths),\n                        )\n\n                # Calculate current depth\n                try:\n                    depth = len(current_path.relative_to(path_root).parts)\n                except ValueError:\n                    depth = 0\n\n                # Skip if beyond max depth (should not happen with fd filtering, but safety check)\n                if max_depth is not None:\n                    if current_path.is_dir() and depth &gt; max_depth:\n                        continue\n                    elif current_path.is_file() and depth &gt; max_depth + 1:\n                        continue\n\n                # Add to paths collection if DataFrame is enabled and we're collecting paths\n                if self.build_dataframe and dataframe_paths is not None:\n                    dataframe_paths.append(str(current_path))\n\n                if current_path.is_dir():\n                    depth_stats[depth] += 1\n                    folder_count += 1\n\n                    # Check for empty folders\n                    try:\n                        if not any(current_path.iterdir()):\n                            empty_folders.append(str(current_path))\n                    except (OSError, PermissionError):\n                        pass\n\n                    # Analyze folder names for patterns\n                    folder_names[current_path.name] += 1\n\n                elif current_path.is_file():\n                    file_count += 1\n\n                    # Count files in parent directory\n                    files_per_folder[str(current_path.parent)] += 1\n\n                    # Get file extension\n                    ext = current_path.suffix.lower()\n                    if ext:\n                        file_extensions[ext] += 1\n                    else:\n                        file_extensions[\"&lt;no extension&gt;\"] += 1\n\n                    # Add to total size\n                    try:\n                        total_size += current_path.stat().st_size\n                    except (OSError, IOError):\n                        pass\n\n            # Final progress update\n            if progress and task_id is not None:\n                progress.update(task_id, completed=processed_items)\n\n            # Calculate summary statistics\n            avg_files_per_folder = file_count / max(1, folder_count)\n\n            # Find folders with most files\n            top_folders_by_file_count = sorted(files_per_folder.items(), key=lambda x: x[1], reverse=True)[:10]\n\n            # Build result dictionary\n            result = {\n                \"path\": str(path_root),\n                \"summary\": {\n                    \"total_files\": file_count,\n                    \"total_folders\": folder_count,\n                    \"total_size_bytes\": total_size,\n                    \"total_size_mb\": round(total_size / (1024 * 1024), 2),\n                    \"avg_files_per_folder\": round(avg_files_per_folder, 2),\n                    \"max_depth\": max(depth_stats.keys()) if depth_stats else 0,\n                    \"empty_folder_count\": len(empty_folders),\n                },\n                \"file_extensions\": dict(file_extensions.most_common(20)),\n                \"common_folder_names\": dict(folder_names.most_common(20)),\n                \"empty_folders\": empty_folders,\n                \"top_folders_by_file_count\": top_folders_by_file_count,\n                \"depth_distribution\": dict(depth_stats),\n            }\n\n            # Add DataFrame if enabled\n            if self.build_dataframe and DATAFRAME_AVAILABLE:\n                if prebuilt_dataframe is not None:\n                    # Use prebuilt DataFrame supplied by caller (fd results)\n                    result[\"dataframe\"] = prebuilt_dataframe\n                else:\n                    result[\"dataframe\"] = DataFrame(dataframe_paths)\n\n            return result\n\n        finally:\n            if progress and progress_owned:\n                progress.stop()\n\n    def _probe_rust(self, path: str, max_depth: Optional[int] = None, fast_path_only: bool = False) -&gt; Dict:\n        \"\"\"Use the Rust implementation for analysis.\n\n        For performance, the main statistical analysis is done in Rust.\n        If DataFrame building is enabled, file paths are collected separately\n        using Python/pathlib to maintain consistency with the Python implementation.\n        \"\"\"\n        progress = None\n        task_id = None\n\n        if self.show_progress:\n            progress = Progress(\n                SpinnerColumn(),\n                TextColumn(\"[bold blue]Analyzing directory structure...\"),\n                BarColumn(),\n                TextColumn(\"[progress.percentage]{task.percentage:&gt;3.0f}%\"),\n                TimeElapsedColumn(),\n                console=self.console,\n                transient=True,  # Remove progress bar when done\n            )\n            progress.start()\n            task_id = progress.add_task(\"Analyzing...\", total=None)\n\n        try:\n            # Choose Rust variant: async for network filesystems, sync otherwise\n            try:\n                fs_type = self._detect_filesystem_type(path)\n            except Exception:\n                fs_type = None\n\n            is_network_fs = False\n            if fs_type:\n                # Common network FS types\n                if any(x in fs_type.lower() for x in (\"nfs\", \"cifs\", \"smb\", \"ceph\", \"gluster\", \"sshfs\")):\n                    is_network_fs = True\n\n            # If network FS choose async Rust prober which limits concurrency and uses tokio\n            # Only use the async Rust variant when the path looks like a network\n            # filesystem AND the user explicitly enabled async via `use_async`.\n            if is_network_fs and self.use_async:\n                # Default concurrency limit can be tuned; use configured values\n                if RUST_ASYNC_AVAILABLE:\n                    # Decide Rust flag defaults: when search_backend is 'auto', prefer fd-like semantics\n                    if self.search_backend == \"auto\":\n                        follow = True\n                        hidden = True\n                        no_ignore = True\n                    else:\n                        follow = None\n                        hidden = None\n                        no_ignore = None\n\n                    result = probe_directory_rust_async(\n                        path,\n                        max_depth,\n                        self.network_concurrency,\n                        self.network_timeout_ms,\n                        self.network_retries,\n                        fast_path_only,\n                        follow_links=follow,\n                        search_hidden=hidden,\n                        no_ignore=no_ignore,\n                    )\n                else:\n                    # Async variant not available; fall back to parallel or sequential Rust\n                    if self.use_parallel and RUST_PARALLEL_AVAILABLE:\n                        if self.search_backend == \"auto\":\n                            follow = True\n                            hidden = True\n                            no_ignore = True\n                        else:\n                            follow = None\n                            hidden = None\n                            no_ignore = None\n\n                        result = probe_directory_rust_parallel(\n                            path,\n                            max_depth,\n                            self.parallel_threshold,\n                            fast_path_only,\n                            follow_links=follow,\n                            search_hidden=hidden,\n                            no_ignore=no_ignore,\n                        )\n                    else:\n                        result = probe_directory_rust(path, max_depth, fast_path_only)\n            elif is_network_fs and not self.use_async:\n                # User explicitly disabled async; prefer parallel or sequential Rust\n                if self.use_parallel and RUST_PARALLEL_AVAILABLE:\n                    if self.search_backend == \"auto\":\n                        follow = True\n                        hidden = True\n                        no_ignore = True\n                    else:\n                        follow = None\n                        hidden = None\n                        no_ignore = None\n\n                    result = probe_directory_rust_parallel(\n                        path,\n                        max_depth,\n                        self.parallel_threshold,\n                        fast_path_only,\n                        follow_links=follow,\n                        search_hidden=hidden,\n                        no_ignore=no_ignore,\n                    )\n                else:\n                    if self.search_backend == \"auto\":\n                        follow = True\n                        hidden = True\n                        no_ignore = True\n                    else:\n                        follow = None\n                        hidden = None\n                        no_ignore = None\n\n                    result = probe_directory_rust(\n                        path,\n                        max_depth,\n                        fast_path_only,\n                        follow_links=follow,\n                        search_hidden=hidden,\n                        no_ignore=no_ignore,\n                    )\n            else:\n                if self.search_backend == \"auto\":\n                    follow = True\n                    hidden = True\n                    no_ignore = True\n                else:\n                    follow = None\n                    hidden = None\n                    no_ignore = None\n\n                if self.use_parallel and RUST_PARALLEL_AVAILABLE:\n                    result = probe_directory_rust_parallel(\n                        path,\n                        max_depth,\n                        self.parallel_threshold,\n                        fast_path_only,\n                        follow_links=follow,\n                        search_hidden=hidden,\n                        no_ignore=no_ignore,\n                    )\n                else:\n                    result = probe_directory_rust(\n                        path,\n                        max_depth,\n                        fast_path_only,\n                        follow_links=follow,\n                        search_hidden=hidden,\n                        no_ignore=no_ignore,\n                    )\n\n            # Update progress to show completion\n            if progress and task_id is not None:\n                progress.update(task_id, description=\"[bold green]Analysis complete!\")\n                progress.update(task_id, total=100, completed=100)\n\n            # Rust now returns absolute (or canonicalized when follow_links=True) paths,\n            # so Python-side normalization is no longer necessary here.\n\n            # If DataFrame building is enabled, we need to collect file paths\n            # since the Rust implementation doesn't return them\n            if self.build_dataframe and DATAFRAME_AVAILABLE:\n                if progress and task_id is not None:\n                    progress.update(task_id, description=\"[bold yellow]Building DataFrame...\")\n\n                root_path_obj = Path(path)\n                all_paths = []\n                permission_errors_encountered = False\n\n                # Collect paths using Python (pathlib) with error handling for system directories\n                try:\n                    for current_path in root_path_obj.rglob(\"*\"):\n                        try:\n                            # Calculate current depth\n                            depth = len(current_path.relative_to(root_path_obj).parts)\n\n                            # Skip if beyond max depth\n                            if max_depth is not None and depth &gt; max_depth:\n                                continue\n\n                            all_paths.append(str(current_path))\n                        except (ValueError, OSError, PermissionError):\n                            # Skip paths that can't be accessed or processed\n                            permission_errors_encountered = True\n                            continue\n                except (OSError, PermissionError, FileNotFoundError):\n                    # If rglob fails entirely, provide DataFrame with whatever we collected\n                    self.console.print(\"[yellow]Warning: Some paths couldn't be accessed for DataFrame building[/yellow]\")\n                    logger.warning(f\"DataFrame building encountered permission errors on {path}, providing partial results\")\n                    permission_errors_encountered = True\n\n                # Add DataFrame to the result (may be partial if there were permission errors)\n                result[\"dataframe\"] = DataFrame(all_paths)\n                if permission_errors_encountered:\n                    # Add a note only if we actually encountered permission errors\n                    result[\"dataframe_note\"] = \"DataFrame may be incomplete due to permission restrictions\"\n\n                if progress and task_id is not None:\n                    progress.update(task_id, description=\"[bold green]DataFrame built!\")\n\n            return result\n\n        finally:\n            if progress:\n                progress.stop()\n\n    def _probe_python(self, path: str, max_depth: Optional[int] = None) -&gt; Dict:\n        \"\"\"Pure Python implementation with enhanced DataFrame support and progress indication.\"\"\"\n        path_root = Path(path)\n        if not path_root.exists():\n            raise ValueError(f\"Path does not exist: {path_root}\")\n        if not path_root.is_dir():\n            raise ValueError(f\"Path is not a directory: {path_root}\")\n\n        # Initialize counters and collections\n        file_count = 0\n        folder_count = 1  # Start with 1 to count the root directory itself\n        total_size = 0\n        empty_folders = []\n        file_extensions = Counter()\n        folder_names = Counter()\n        files_per_folder = defaultdict(int)\n        depth_stats = defaultdict(int)\n\n        # Count the root directory at depth 0\n        depth_stats[0] = 1\n\n        # Collection for DataFrame if enabled\n        all_paths = [] if self.build_dataframe else None\n\n        # Estimate total items for progress tracking\n        progress = None\n        task_id = None\n        total_items = None\n        processed_items = 0\n\n        if self.show_progress:\n            # Quick estimation pass\n            total_items = sum(1 for _ in path_root.rglob(\"*\"))\n\n            progress = Progress(\n                SpinnerColumn(),\n                TextColumn(\"[bold blue]Analyzing directory structure...\"),\n                BarColumn(),\n                TextColumn(\"[progress.percentage]{task.percentage:&gt;3.0f}%\"),\n                TextColumn(\"({task.completed:,}/{task.total:,} items)\"),\n                TimeElapsedColumn(),\n                console=self.console,\n                transient=True,\n            )\n            progress.start()\n            task_id = progress.add_task(\"Analyzing...\", total=total_items)\n\n        try:\n            # Walk through directory tree using pathlib for consistency\n            try:\n                for current_path in path_root.rglob(\"*\"):\n                    try:\n                        processed_items += 1\n\n                        # Update progress\n                        if progress and task_id is not None:\n                            if processed_items % 100 == 0:  # Update every 100 items for performance\n                                progress.update(task_id, completed=processed_items)\n\n                            # Call custom progress callback if provided\n                            if self.progress_callback:\n                                self.progress_callback(\n                                    f\"Processing: {current_path.name}\",\n                                    processed_items,\n                                    total_items or 0,\n                                )\n\n                        # Calculate current depth\n                        try:\n                            depth = len(current_path.relative_to(path_root).parts)\n                        except ValueError:\n                            depth = 0\n\n                        # Skip if beyond max depth (match Rust implementation logic)\n                        if max_depth is not None:\n                            try:\n                                if current_path.is_dir() and depth &gt; max_depth:\n                                    continue\n                                elif current_path.is_file() and depth &gt; max_depth + 1:\n                                    continue\n                            except (OSError, PermissionError):\n                                # Skip paths we can't access for depth checking\n                                continue\n\n                        # Add to paths collection if DataFrame is enabled\n                        if self.build_dataframe:\n                            all_paths.append(str(current_path))\n\n                        try:\n                            is_dir = current_path.is_dir()\n                            is_file = current_path.is_file()\n                        except (OSError, PermissionError):\n                            # Skip paths we can't determine type for\n                            continue\n\n                        if is_dir:\n                            depth_stats[depth] += 1\n                            folder_count += 1\n\n                            # Check for empty folders\n                            try:\n                                if not any(current_path.iterdir()):\n                                    empty_folders.append(str(current_path))\n                            except (OSError, PermissionError):\n                                # Skip directories we can't read\n                                pass\n\n                            # Analyze folder names for patterns\n                            folder_names[current_path.name] += 1\n\n                        elif is_file:\n                            file_count += 1\n\n                            # Count files in parent directory\n                            files_per_folder[str(current_path.parent)] += 1\n\n                            # Get file extension\n                            ext = current_path.suffix.lower()\n                            if ext:\n                                file_extensions[ext] += 1\n                            else:\n                                file_extensions[\"&lt;no extension&gt;\"] += 1\n\n                            # Add to total size\n                            try:\n                                total_size += current_path.stat().st_size\n                            except (OSError, IOError):\n                                # Skip files we can't stat (permissions, broken symlinks, etc.)\n                                pass\n\n                    except (OSError, PermissionError):\n                        # Skip individual files/directories we can't access\n                        continue\n\n            except (OSError, PermissionError):\n                # If rglob fails entirely, we can't probe this directory\n                self.console.print(f\"[yellow]Warning: Cannot access directory {path_root} - insufficient permissions[/yellow]\")\n                # Return minimal result\n                return {\n                    \"path\": str(path_root),\n                    \"summary\": {\n                        \"total_files\": 0,\n                        \"total_folders\": 0,\n                        \"total_size_bytes\": 0,\n                        \"total_size_mb\": 0.0,\n                        \"avg_files_per_folder\": 0.0,\n                        \"max_depth\": 0,\n                        \"empty_folder_count\": 0,\n                    },\n                    \"file_extensions\": {},\n                    \"common_folder_names\": {},\n                    \"empty_folders\": [],\n                    \"top_folders_by_file_count\": [],\n                    \"depth_distribution\": {},\n                    \"timing\": {\"error\": \"Permission denied\"},\n                }\n\n            # Final progress update\n            if progress and task_id is not None:\n                progress.update(task_id, completed=processed_items)\n\n            # Calculate summary statistics\n            avg_files_per_folder = file_count / max(1, folder_count)\n\n            # Find folders with most files\n            top_folders_by_file_count = sorted(files_per_folder.items(), key=lambda x: x[1], reverse=True)[:10]\n\n            # Build result dictionary\n            result = {\n                \"path\": str(path_root),\n                \"summary\": {\n                    \"total_files\": file_count,\n                    \"total_folders\": folder_count,\n                    \"total_size_bytes\": total_size,\n                    \"total_size_mb\": round(total_size / (1024 * 1024), 2),\n                    \"avg_files_per_folder\": round(avg_files_per_folder, 2),\n                    \"max_depth\": max(depth_stats.keys()) if depth_stats else 0,\n                    \"empty_folder_count\": len(empty_folders),\n                },\n                \"file_extensions\": dict(file_extensions.most_common(20)),\n                \"common_folder_names\": dict(folder_names.most_common(20)),\n                \"empty_folders\": empty_folders,\n                \"top_folders_by_file_count\": top_folders_by_file_count,\n                \"depth_distribution\": dict(depth_stats),\n            }\n\n            # Add DataFrame if enabled\n            if self.build_dataframe and DATAFRAME_AVAILABLE:\n                result[\"dataframe\"] = DataFrame(all_paths)\n\n            return result\n\n        finally:\n            if progress:\n                progress.stop()\n\n    def print_summary(self, analysis: \"DirectoryAnalysis\"):\n        \"\"\"Print a summary of the directory analysis (expects DirectoryAnalysis).\"\"\"\n        if not isinstance(analysis, DirectoryAnalysis):\n            raise TypeError(\"print_summary expects a DirectoryAnalysis instance\")\n\n        summary = analysis.summary\n        timing = analysis.timing or {}\n\n        # Show which implementation was used with more detail\n        impl_type = timing.get(\"implementation\", \"Unknown\")\n\n        # Add DataFrame indicator\n        if self.build_dataframe and analysis.dataframe is not None:\n            impl_type += \" + \ud83d\udcca DataFrame\"\n\n        # Main summary table\n        title = f\"Directory Analysis: {analysis.path} ({impl_type})\"\n        if timing:\n            title += f\" - {timing.get('elapsed_seconds', 0):.2f}s\"\n\n        table = Table(title=title)\n        table.add_column(\"Metric\", style=\"bold cyan\")\n        table.add_column(\"Value\", style=\"white\")\n\n        table.add_row(\"Total Files\", f\"{summary['total_files']:,}\")\n        table.add_row(\"Total Folders\", f\"{summary['total_folders']:,}\")\n        table.add_row(\"Total Size\", f\"{summary['total_size_mb']:,} MB\")\n        table.add_row(\"Average Files per Folder\", str(summary[\"avg_files_per_folder\"]))\n        table.add_row(\"Maximum Depth\", str(summary[\"max_depth\"]))\n        table.add_row(\"Empty Folders\", str(summary[\"empty_folder_count\"]))\n\n        # Add DataFrame info if available\n        if self.build_dataframe and analysis.dataframe is not None:\n            df = analysis.dataframe\n            table.add_row(\"DataFrame Rows\", f\"{len(df):,}\")\n\n        # Add timing information if available\n        if timing:\n            table.add_row(\"Analysis Time\", f\"{timing['elapsed_seconds']:.2f}s\")\n            if timing.get(\"items_per_second\", 0) &gt; 0:\n                table.add_row(\"Processing Speed\", f\"{timing['items_per_second']:,.0f} items/sec\")\n\n        self.console.print(table)\n        self.console.print()\n\n    def get_dataframe(self, analysis: \"DirectoryAnalysis\") -&gt; Optional[\"DataFrame\"]:\n        \"\"\"Get the DataFrame from analysis results.\n\n        Args:\n        ----\n            analysis: :class:`DirectoryAnalysis` instance\n\n        Returns:\n        -------\n            DataFrame object if available, None otherwise\n\n        \"\"\"\n        if not isinstance(analysis, DirectoryAnalysis):\n            raise TypeError(\"get_dataframe expects a DirectoryAnalysis instance\")\n        return analysis.to_df()\n\n    def is_dataframe_enabled(self) -&gt; bool:\n        \"\"\"Check if DataFrame building is enabled and available.\n\n        Returns\n        -------\n            True if DataFrame building is enabled, False otherwise\n\n        \"\"\"\n        return self.build_dataframe and DATAFRAME_AVAILABLE\n\n    def _detect_filesystem_type(self, path: str) -&gt; Optional[str]:\n        \"\"\"Attempt to detect the filesystem type for a given path.\n\n        Returns the fs type string (e.g., 'nfs', 'ext4') or None if not detected.\n        \"\"\"\n        import os\n\n        try:\n            # Parse /proc/mounts for the mount containing the path\n            mounts = []\n            with open(\"/proc/mounts\", \"r\") as f:\n                for line in f:\n                    parts = line.split()\n                    if len(parts) &gt;= 3:\n                        mounts.append((parts[1], parts[2]))  # (mount_point, fs_type)\n\n            # Find best match by longest mount_point prefix\n            best = (\"\", None)\n            p = os.path.abspath(path)\n            for mnt, fst in mounts:\n                if p.startswith(mnt) and len(mnt) &gt; len(best[0]):\n                    best = (mnt, fst)\n\n            if best[1]:\n                return best[1]\n\n        except Exception:\n            pass\n\n        # Fallback: try os.statvfs and map f_fsid is not portable; return None\n        return None\n\n    def print_file_extensions(self, analysis: \"DirectoryAnalysis\", top_n: int = 10):\n        \"\"\"Print the most common file extensions (expects DirectoryAnalysis).\"\"\"\n        if not isinstance(analysis, DirectoryAnalysis):\n            raise TypeError(\"print_file_extensions expects a DirectoryAnalysis instance\")\n\n        extensions = analysis.file_extensions\n\n        if not extensions:\n            return\n\n        table = Table(title=\"File Extensions\")\n        table.add_column(\"Extension\", style=\"bold magenta\")\n        table.add_column(\"Count\", style=\"white\")\n        table.add_column(\"Percentage\", style=\"green\")\n        total_files = analysis.summary[\"total_files\"]\n\n        for ext, count in list(extensions.items())[:top_n]:\n            percentage = (count / total_files * 100) if total_files &gt; 0 else 0\n            table.add_row(ext, f\"{count:,}\", f\"{percentage:.1f}%\")\n\n        self.console.print(table)\n        self.console.print()\n\n    def print_folder_patterns(self, analysis: \"DirectoryAnalysis\", top_n: int = 10):\n        \"\"\"Print the most common folder names (expects DirectoryAnalysis).\"\"\"\n        if not isinstance(analysis, DirectoryAnalysis):\n            raise TypeError(\"print_folder_patterns expects a DirectoryAnalysis instance\")\n\n        folder_names = analysis.common_folder_names\n\n        if not folder_names:\n            return\n\n        table = Table(title=\"Common Folder Names\")\n        table.add_column(\"Folder Name\", style=\"bold blue\")\n        table.add_column(\"Occurrences\", style=\"white\")\n\n        for name, count in list(folder_names.items())[:top_n]:\n            table.add_row(name, f\"{count:,}\")\n\n        self.console.print(table)\n        self.console.print()\n\n    def print_empty_folders(self, analysis: \"DirectoryAnalysis\", max_show: int = 20):\n        \"\"\"Print empty folders found (expects DirectoryAnalysis).\"\"\"\n        if not isinstance(analysis, DirectoryAnalysis):\n            raise TypeError(\"print_empty_folders expects a DirectoryAnalysis instance\")\n\n        empty_folders = analysis.empty_folders\n\n        if not empty_folders:\n            self.console.print(\"[green]\u2713 No empty folders found![/green]\")\n            return\n\n        table = Table(title=f\"Empty Folders (showing {min(len(empty_folders), max_show)} of {len(empty_folders)})\")\n        table.add_column(\"Path\", style=\"yellow\")\n\n        for folder in empty_folders[:max_show]:\n            table.add_row(folder)\n\n        if len(empty_folders) &gt; max_show:\n            table.add_row(f\"... and {len(empty_folders) - max_show} more\")\n\n        self.console.print(table)\n        self.console.print()\n\n    def print_report(self, analysis: \"DirectoryAnalysis\"):\n        \"\"\"Print a comprehensive report of the directory analysis.\n\n        Expects a :class:`DirectoryAnalysis` instance. Use :meth:`to_dict`\n        if you need a plain dict shape for downstream tooling.\n        \"\"\"\n        if not isinstance(analysis, DirectoryAnalysis):\n            raise TypeError(\"print_report expects a DirectoryAnalysis instance\")\n\n        self.print_summary(analysis)\n        self.print_file_extensions(analysis)\n        self.print_folder_patterns(analysis)\n        self.print_empty_folders(analysis)\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the directory profiler.</p> <p>The profiler is configured with a <code>DirectoryProfilerConfig</code> instance which holds options such as whether to use Rust acceleration, parallel processing, fd integration, thresholding for parallelism, DataFrame building, and progress reporting callbacks. Pass a <code>DirectoryProfilerConfig</code> object as the single <code>config</code> argument. See <code>DirectoryProfilerConfig</code> for descriptions of each configurable field.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def __init__(self, config: \"DirectoryProfilerConfig\"):\n    \"\"\"Initialize the directory profiler.\n\n    The profiler is configured with a `DirectoryProfilerConfig` instance which\n    holds options such as whether to use Rust acceleration, parallel processing,\n    fd integration, thresholding for parallelism, DataFrame building, and progress\n    reporting callbacks. Pass a `DirectoryProfilerConfig` object as the single\n    `config` argument. See `DirectoryProfilerConfig` for descriptions of each\n    configurable field.\n    \"\"\"\n    # Expect a DirectoryProfilerConfig object \u2014 no legacy kwargs supported.\n    if not hasattr(config, \"__class__\") or config.__class__.__name__ != \"DirectoryProfilerConfig\":\n        raise TypeError(\"DirectoryProfiler requires a DirectoryProfilerConfig instance as the sole argument\")\n\n    self.console = Console()\n    self.config = config\n\n    # Set simple aliases for common flags to preserve prior attribute names\n    # Internal availability checks are still performed below.\n    self.search_backend = config.search_backend\n    self.parallel_threshold = config.parallel_threshold\n    self._fast_path_only = config.fast_path_only\n    self.progress_callback = config.progress_callback\n\n    # Validate availability and enforce clear relationships\n    # Use explicit booleans from the config\n    if config.use_rust and not RUST_AVAILABLE:\n        raise RuntimeError(\"Rust implementation requested but not available in this build\")\n    if config.use_parallel and not RUST_PARALLEL_AVAILABLE:\n        raise RuntimeError(\"Parallel Rust requested but not available\")\n    if config.use_async and not RUST_ASYNC_AVAILABLE:\n        raise RuntimeError(\"Async Rust prober requested but not available in this build\")\n    if config.use_fd and not FD_AVAILABLE:\n        raise RuntimeError(\"fd integration requested but not available in this environment\")\n    if config.build_dataframe and not DATAFRAME_AVAILABLE:\n        raise RuntimeError(\"DataFrame building requested but Polars/DataFrame support is not available\")\n\n    # Network args only apply when use_async is True (explicit)\n    if not config.use_async and any(\n        (\n            config.network_concurrency != 64,\n            config.network_timeout_ms != 5000,\n            config.network_retries != 0,\n        )\n    ):\n        raise ValueError(\"Network tuning parameters only apply when use_async=True\")\n\n    # Threads only applies when use_fd is True\n    if config.threads is not None and not config.use_fd:\n        raise ValueError(\"'threads' setting only applies when use_fd=True\")\n\n    # Decide which implementation to use based on search_backend and availability\n    backend_choice = config.search_backend\n    if backend_choice == \"auto\":\n        # Honor explicit user preferences when provided.\n        # If both backends are explicitly requested and available, prefer fd\n        if config.use_fd and config.use_rust and FD_AVAILABLE and RUST_AVAILABLE:\n            backend_choice = \"fd\"\n        # If user explicitly requested Rust and it's available, use it\n        elif config.use_rust and RUST_AVAILABLE:\n            backend_choice = \"rust\"\n        # If user explicitly requested fd and it's available, use it\n        elif config.use_fd and FD_AVAILABLE:\n            backend_choice = \"fd\"\n        else:\n            # No explicit preference from user -&gt; auto-detect best available\n            if RUST_AVAILABLE:\n                backend_choice = \"rust\"\n            elif FD_AVAILABLE:\n                backend_choice = \"fd\"\n            else:\n                backend_choice = \"python\"\n\n    if backend_choice == \"rust\":\n        self.use_rust = True\n        self.use_fd = False\n    elif backend_choice == \"fd\":\n        self.use_rust = False\n        self.use_fd = True\n    else:\n        self.use_rust = False\n        self.use_fd = False\n\n    # Parallel/async/other toggles come directly from config (already validated)\n    self.use_parallel = bool(config.use_parallel and self.use_rust)\n    self.use_async = bool(config.use_async and self.use_rust)\n\n    # Other instance-level flags\n    self.build_dataframe = bool(config.build_dataframe)\n    self.return_absolute_paths = bool(config.return_absolute_paths)\n    # Progress handling\n    if _is_interactive_environment() and config.show_progress:\n        logger.debug(\"Interactive environment detected, disabling progress bars to avoid conflicts\")\n        self.show_progress = False\n    else:\n        self.show_progress = bool(config.show_progress)\n\n    # Network tuning (only valid if use_async True)\n    self.network_concurrency = config.network_concurrency\n    self.network_timeout_ms = config.network_timeout_ms\n    self.network_retries = config.network_retries\n\n    # Threads forwarded to fd if using fd backend\n    self.threads = config.threads if self.use_fd else None\n\n    # Defer fd integration initialization until actually used\n    self.fd_integration = None\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.get_dataframe","title":"<code>get_dataframe(analysis)</code>","text":"<p>Get the DataFrame from analysis results.</p> <pre><code>analysis: :class:`DirectoryAnalysis` instance\n</code></pre> <pre><code>DataFrame object if available, None otherwise\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def get_dataframe(self, analysis: \"DirectoryAnalysis\") -&gt; Optional[\"DataFrame\"]:\n    \"\"\"Get the DataFrame from analysis results.\n\n    Args:\n    ----\n        analysis: :class:`DirectoryAnalysis` instance\n\n    Returns:\n    -------\n        DataFrame object if available, None otherwise\n\n    \"\"\"\n    if not isinstance(analysis, DirectoryAnalysis):\n        raise TypeError(\"get_dataframe expects a DirectoryAnalysis instance\")\n    return analysis.to_df()\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.get_implementation_info","title":"<code>get_implementation_info()</code>","text":"<p>Get information about which implementations are available and being used.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.get_implementation_info--returns","title":"Returns","text":"<pre><code>Dictionary with implementation availability status\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def get_implementation_info(self) -&gt; Dict[str, bool]:\n    \"\"\"Get information about which implementations are available and being used.\n\n    Returns\n    -------\n        Dictionary with implementation availability status\n\n    \"\"\"\n    return {\n        \"rust_available\": RUST_AVAILABLE,\n        \"rust_parallel_available\": RUST_PARALLEL_AVAILABLE,\n        \"rust_async_available\": RUST_ASYNC_AVAILABLE,\n        \"fd_available\": FD_AVAILABLE,\n        \"dataframe_available\": DATAFRAME_AVAILABLE,\n        \"using_rust\": self.use_rust,\n        \"using_parallel\": self.use_parallel,\n        \"using_async\": bool(self.use_async and RUST_ASYNC_AVAILABLE),\n        \"using_fd\": self.use_fd,\n        \"using_dataframe\": self.build_dataframe,\n        \"return_absolute_paths\": self.return_absolute_paths,\n        \"search_backend\": self.search_backend,\n        \"python_fallback\": not (self.use_rust or self.use_fd),\n    }\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_dataframe_enabled","title":"<code>is_dataframe_enabled()</code>","text":"<p>Check if DataFrame building is enabled and available.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_dataframe_enabled--returns","title":"Returns","text":"<pre><code>True if DataFrame building is enabled, False otherwise\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def is_dataframe_enabled(self) -&gt; bool:\n    \"\"\"Check if DataFrame building is enabled and available.\n\n    Returns\n    -------\n        True if DataFrame building is enabled, False otherwise\n\n    \"\"\"\n    return self.build_dataframe and DATAFRAME_AVAILABLE\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_fd_available","title":"<code>is_fd_available()</code>","text":"<p>Check if fd integration is available and being used.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_fd_available--returns","title":"Returns","text":"<pre><code>True if fd is available and enabled, False otherwise\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def is_fd_available(self) -&gt; bool:\n    \"\"\"Check if fd integration is available and being used.\n\n    Returns\n    -------\n        True if fd is available and enabled, False otherwise\n\n    \"\"\"\n    # Use FD_AVAILABLE to reflect whether the fd integration package is importable\n    # Tests may monkeypatch FD_AVAILABLE without having the fd binary present.\n    return self.use_fd and FD_AVAILABLE\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_parallel_available","title":"<code>is_parallel_available()</code>","text":"<p>Check if parallel Rust implementation is available and being used.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_parallel_available--returns","title":"Returns","text":"<pre><code>True if parallel Rust implementation is available and enabled, False otherwise\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def is_parallel_available(self) -&gt; bool:\n    \"\"\"Check if parallel Rust implementation is available and being used.\n\n    Returns\n    -------\n        True if parallel Rust implementation is available and enabled, False otherwise\n\n    \"\"\"\n    return self.use_parallel and RUST_PARALLEL_AVAILABLE\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_rust_available","title":"<code>is_rust_available()</code>","text":"<p>Check if Rust implementation is available and being used.</p>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.is_rust_available--returns","title":"Returns","text":"<pre><code>True if Rust implementation is available and enabled, False otherwise\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def is_rust_available(self) -&gt; bool:\n    \"\"\"Check if Rust implementation is available and being used.\n\n    Returns\n    -------\n        True if Rust implementation is available and enabled, False otherwise\n\n    \"\"\"\n    return self.use_rust and RUST_AVAILABLE\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.print_empty_folders","title":"<code>print_empty_folders(analysis, max_show=20)</code>","text":"<p>Print empty folders found (expects DirectoryAnalysis).</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_empty_folders(self, analysis: \"DirectoryAnalysis\", max_show: int = 20):\n    \"\"\"Print empty folders found (expects DirectoryAnalysis).\"\"\"\n    if not isinstance(analysis, DirectoryAnalysis):\n        raise TypeError(\"print_empty_folders expects a DirectoryAnalysis instance\")\n\n    empty_folders = analysis.empty_folders\n\n    if not empty_folders:\n        self.console.print(\"[green]\u2713 No empty folders found![/green]\")\n        return\n\n    table = Table(title=f\"Empty Folders (showing {min(len(empty_folders), max_show)} of {len(empty_folders)})\")\n    table.add_column(\"Path\", style=\"yellow\")\n\n    for folder in empty_folders[:max_show]:\n        table.add_row(folder)\n\n    if len(empty_folders) &gt; max_show:\n        table.add_row(f\"... and {len(empty_folders) - max_show} more\")\n\n    self.console.print(table)\n    self.console.print()\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.print_file_extensions","title":"<code>print_file_extensions(analysis, top_n=10)</code>","text":"<p>Print the most common file extensions (expects DirectoryAnalysis).</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_file_extensions(self, analysis: \"DirectoryAnalysis\", top_n: int = 10):\n    \"\"\"Print the most common file extensions (expects DirectoryAnalysis).\"\"\"\n    if not isinstance(analysis, DirectoryAnalysis):\n        raise TypeError(\"print_file_extensions expects a DirectoryAnalysis instance\")\n\n    extensions = analysis.file_extensions\n\n    if not extensions:\n        return\n\n    table = Table(title=\"File Extensions\")\n    table.add_column(\"Extension\", style=\"bold magenta\")\n    table.add_column(\"Count\", style=\"white\")\n    table.add_column(\"Percentage\", style=\"green\")\n    total_files = analysis.summary[\"total_files\"]\n\n    for ext, count in list(extensions.items())[:top_n]:\n        percentage = (count / total_files * 100) if total_files &gt; 0 else 0\n        table.add_row(ext, f\"{count:,}\", f\"{percentage:.1f}%\")\n\n    self.console.print(table)\n    self.console.print()\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.print_folder_patterns","title":"<code>print_folder_patterns(analysis, top_n=10)</code>","text":"<p>Print the most common folder names (expects DirectoryAnalysis).</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_folder_patterns(self, analysis: \"DirectoryAnalysis\", top_n: int = 10):\n    \"\"\"Print the most common folder names (expects DirectoryAnalysis).\"\"\"\n    if not isinstance(analysis, DirectoryAnalysis):\n        raise TypeError(\"print_folder_patterns expects a DirectoryAnalysis instance\")\n\n    folder_names = analysis.common_folder_names\n\n    if not folder_names:\n        return\n\n    table = Table(title=\"Common Folder Names\")\n    table.add_column(\"Folder Name\", style=\"bold blue\")\n    table.add_column(\"Occurrences\", style=\"white\")\n\n    for name, count in list(folder_names.items())[:top_n]:\n        table.add_row(name, f\"{count:,}\")\n\n    self.console.print(table)\n    self.console.print()\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.print_report","title":"<code>print_report(analysis)</code>","text":"<p>Print a comprehensive report of the directory analysis.</p> <p>Expects a :class:<code>DirectoryAnalysis</code> instance. Use :meth:<code>to_dict</code> if you need a plain dict shape for downstream tooling.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_report(self, analysis: \"DirectoryAnalysis\"):\n    \"\"\"Print a comprehensive report of the directory analysis.\n\n    Expects a :class:`DirectoryAnalysis` instance. Use :meth:`to_dict`\n    if you need a plain dict shape for downstream tooling.\n    \"\"\"\n    if not isinstance(analysis, DirectoryAnalysis):\n        raise TypeError(\"print_report expects a DirectoryAnalysis instance\")\n\n    self.print_summary(analysis)\n    self.print_file_extensions(analysis)\n    self.print_folder_patterns(analysis)\n    self.print_empty_folders(analysis)\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.print_summary","title":"<code>print_summary(analysis)</code>","text":"<p>Print a summary of the directory analysis (expects DirectoryAnalysis).</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def print_summary(self, analysis: \"DirectoryAnalysis\"):\n    \"\"\"Print a summary of the directory analysis (expects DirectoryAnalysis).\"\"\"\n    if not isinstance(analysis, DirectoryAnalysis):\n        raise TypeError(\"print_summary expects a DirectoryAnalysis instance\")\n\n    summary = analysis.summary\n    timing = analysis.timing or {}\n\n    # Show which implementation was used with more detail\n    impl_type = timing.get(\"implementation\", \"Unknown\")\n\n    # Add DataFrame indicator\n    if self.build_dataframe and analysis.dataframe is not None:\n        impl_type += \" + \ud83d\udcca DataFrame\"\n\n    # Main summary table\n    title = f\"Directory Analysis: {analysis.path} ({impl_type})\"\n    if timing:\n        title += f\" - {timing.get('elapsed_seconds', 0):.2f}s\"\n\n    table = Table(title=title)\n    table.add_column(\"Metric\", style=\"bold cyan\")\n    table.add_column(\"Value\", style=\"white\")\n\n    table.add_row(\"Total Files\", f\"{summary['total_files']:,}\")\n    table.add_row(\"Total Folders\", f\"{summary['total_folders']:,}\")\n    table.add_row(\"Total Size\", f\"{summary['total_size_mb']:,} MB\")\n    table.add_row(\"Average Files per Folder\", str(summary[\"avg_files_per_folder\"]))\n    table.add_row(\"Maximum Depth\", str(summary[\"max_depth\"]))\n    table.add_row(\"Empty Folders\", str(summary[\"empty_folder_count\"]))\n\n    # Add DataFrame info if available\n    if self.build_dataframe and analysis.dataframe is not None:\n        df = analysis.dataframe\n        table.add_row(\"DataFrame Rows\", f\"{len(df):,}\")\n\n    # Add timing information if available\n    if timing:\n        table.add_row(\"Analysis Time\", f\"{timing['elapsed_seconds']:.2f}s\")\n        if timing.get(\"items_per_second\", 0) &gt; 0:\n            table.add_row(\"Processing Speed\", f\"{timing['items_per_second']:,.0f} items/sec\")\n\n    self.console.print(table)\n    self.console.print()\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.probe","title":"<code>probe(path, max_depth=None, threads=None)</code>","text":"<p>Analyze a directory tree and return comprehensive statistics.</p> <pre><code>path: Path to the root directory to probe\nmax_depth: Maximum depth to traverse (None for unlimited)\nthreads: Optional override for number of threads when using fd backend\n</code></pre> <pre><code>A :class:`DirectoryAnalysis` instance containing analysis results\n</code></pre> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def probe(self, path: str, max_depth: Optional[int] = None, threads: Optional[int] = None) -&gt; \"DirectoryAnalysis\":\n    \"\"\"Analyze a directory tree and return comprehensive statistics.\n\n    Args:\n    ----\n        path: Path to the root directory to probe\n        max_depth: Maximum depth to traverse (None for unlimited)\n        threads: Optional override for number of threads when using fd backend\n\n    Returns:\n    -------\n        A :class:`DirectoryAnalysis` instance containing analysis results\n\n    \"\"\"\n    start_time = time.time()\n\n    # Choose the best backend\n    backend = self._choose_backend()\n\n    # Log the start of analysis\n    impl_type = self._get_impl_display_name(backend)\n    logger.info(f\"Starting directory analysis of '{path}' using {impl_type} implementation\")\n\n    try:\n        if backend == \"fd\":\n            # threads param overrides instance threads when provided\n            chosen_threads = threads if threads is not None else self.threads\n            result = self._probe_fd(path, max_depth, threads=chosen_threads)\n        elif backend == \"rust\":\n            result = self._probe_rust(path, max_depth, fast_path_only=self._fast_path_only)\n        else:\n            result = self._probe_python(path, max_depth)\n\n        # Calculate and log timing\n        elapsed_time = time.time() - start_time\n        total_items = result[\"summary\"][\"total_files\"] + result[\"summary\"][\"total_folders\"]\n\n        logger.success(\n            f\"Directory analysis completed in {elapsed_time:.2f}s - \"\n            f\"Found {total_items:,} items ({result['summary']['total_files']:,} files, \"\n            f\"{result['summary']['total_folders']:,} folders) using {impl_type}\"\n        )\n\n        # Add timing information to result\n        result[\"timing\"] = {\n            \"elapsed_seconds\": elapsed_time,\n            \"implementation\": impl_type,\n            \"items_per_second\": (total_items / elapsed_time if elapsed_time &gt; 0 else 0),\n        }\n\n        # Return a structured dataclass by default for easier programmatic use\n        return DirectoryAnalysis.from_dict(result)\n\n    except Exception as e:\n        elapsed_time = time.time() - start_time\n        logger.error(f\"Directory analysis failed after {elapsed_time:.2f}s: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfiler.sample_paths","title":"<code>sample_paths(path, sample_size=20)</code>","text":"<p>Return small samples of paths for quick backend-diffing.</p> <p>Returns a dict with keys 'fd_files', 'fd_dirs', 'python_files'. Rust currently does not expose a path list in the public API so it is omitted (you can re-run the Rust prober separately if needed).</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def sample_paths(self, path: str, sample_size: int = 20) -&gt; Dict[str, List[str]]:\n    \"\"\"Return small samples of paths for quick backend-diffing.\n\n    Returns a dict with keys 'fd_files', 'fd_dirs', 'python_files'. Rust currently\n    does not expose a path list in the public API so it is omitted (you can\n    re-run the Rust prober separately if needed).\n    \"\"\"\n    samples = {\"fd_files\": [], \"fd_dirs\": [], \"python_files\": []}\n    try:\n        if FD_AVAILABLE:\n            fd = FdIntegration()\n            samples[\"fd_files\"] = fd.find(\n                path=path,\n                file_types=[\"f\"],\n                max_results=sample_size,\n                search_hidden=True,\n                no_ignore=True,\n                follow_links=True,\n                absolute_paths=self.return_absolute_paths,\n            )\n            samples[\"fd_dirs\"] = fd.find(\n                path=path,\n                file_types=[\"d\"],\n                max_results=sample_size,\n                search_hidden=True,\n                no_ignore=True,\n                follow_links=True,\n                absolute_paths=self.return_absolute_paths,\n            )\n    except Exception:\n        samples[\"fd_files\"] = []\n        samples[\"fd_dirs\"] = []\n\n    # Python sample\n    try:\n        root = Path(path)\n        python_files = []\n        for i, p in enumerate(root.rglob(\"*\")):\n            if p.is_file():\n                python_files.append(str(p.resolve()))\n            if len(python_files) &gt;= sample_size:\n                break\n        samples[\"python_files\"] = python_files\n    except Exception:\n        samples[\"python_files\"] = []\n\n    return samples\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfilerConfig","title":"<code>DirectoryProfilerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for DirectoryProfiler (explicit, typed, no legacy kwargs).</p> <p>All fields are documented and validated in post_init.</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>@dataclass(frozen=True)\nclass DirectoryProfilerConfig:\n    \"\"\"Configuration for DirectoryProfiler (explicit, typed, no legacy kwargs).\n\n    All fields are documented and validated in __post_init__.\n    \"\"\"\n\n    # Backend selection\n    use_rust: bool = False\n    use_parallel: bool = True\n    use_async: bool = False\n    use_fd: bool = False\n    search_backend: str = \"auto\"  # 'rust' | 'fd' | 'python' | 'auto'\n\n    # General tuning\n    parallel_threshold: int = 1000\n    build_dataframe: bool = False\n    return_absolute_paths: bool = False\n    show_progress: bool = True\n    progress_callback: Optional[Callable[[str, int, int], None]] = None\n    fast_path_only: bool = False\n\n    # Network tuning (only valid when use_async is True)\n    network_concurrency: int = 64\n    network_timeout_ms: int = 5000\n    network_retries: int = 0\n\n    # fd-specific tuning\n    threads: Optional[int] = None\n\n    def __post_init__(self):\n        \"\"\"Validate configuration fields after initialization.\n\n        Ensures values are within acceptable ranges and relationships are\n        enforced (for example, network tuning only when async is enabled).\n        \"\"\"\n        # Basic validations\n        if self.search_backend not in (\"auto\", \"rust\", \"fd\", \"python\"):\n            raise ValueError(\"search_backend must be one of 'auto','rust','fd','python'\")\n        if not isinstance(self.parallel_threshold, int) or self.parallel_threshold &lt; 0:\n            raise ValueError(\"parallel_threshold must be a non-negative integer\")\n        if not isinstance(self.network_concurrency, int) or self.network_concurrency &lt;= 0:\n            raise ValueError(\"network_concurrency must be a positive integer\")\n        if self.network_timeout_ms &lt;= 0:\n            raise ValueError(\"network_timeout_ms must be positive\")\n        if self.network_retries &lt; 0:\n            raise ValueError(\"network_retries must be non-negative\")\n\n        # Relationship validations\n        if not self.use_async and (self.network_concurrency != 64 or self.network_timeout_ms != 5000 or self.network_retries != 0):\n            raise ValueError(\"Network tuning parameters only apply when use_async=True\")\n        if self.threads is not None and not self.use_fd:\n            raise ValueError(\"'threads' only applies when use_fd=True\")\n</code></pre>"},{"location":"api/#filoma.directories.directory_profiler.DirectoryProfilerConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration fields after initialization.</p> <p>Ensures values are within acceptable ranges and relationships are enforced (for example, network tuning only when async is enabled).</p> Source code in <code>src/filoma/directories/directory_profiler.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration fields after initialization.\n\n    Ensures values are within acceptable ranges and relationships are\n    enforced (for example, network tuning only when async is enabled).\n    \"\"\"\n    # Basic validations\n    if self.search_backend not in (\"auto\", \"rust\", \"fd\", \"python\"):\n        raise ValueError(\"search_backend must be one of 'auto','rust','fd','python'\")\n    if not isinstance(self.parallel_threshold, int) or self.parallel_threshold &lt; 0:\n        raise ValueError(\"parallel_threshold must be a non-negative integer\")\n    if not isinstance(self.network_concurrency, int) or self.network_concurrency &lt;= 0:\n        raise ValueError(\"network_concurrency must be a positive integer\")\n    if self.network_timeout_ms &lt;= 0:\n        raise ValueError(\"network_timeout_ms must be positive\")\n    if self.network_retries &lt; 0:\n        raise ValueError(\"network_retries must be non-negative\")\n\n    # Relationship validations\n    if not self.use_async and (self.network_concurrency != 64 or self.network_timeout_ms != 5000 or self.network_retries != 0):\n        raise ValueError(\"Network tuning parameters only apply when use_async=True\")\n    if self.threads is not None and not self.use_fd:\n        raise ValueError(\"'threads' only applies when use_fd=True\")\n</code></pre>"},{"location":"backends/","title":"Backend Architecture","text":"<p><code>filoma</code> provides three high-performance backends that automatically select the best option for your system.</p>"},{"location":"backends/#backend-overview","title":"Backend Overview","text":""},{"location":"backends/#python-backend-universal","title":"\ud83d\udc0d Python Backend (Universal)","text":"<ul> <li>Always available - works on any Python installation</li> <li>Full compatibility - complete feature set</li> <li>Reliable fallback - when other backends aren't available</li> </ul>"},{"location":"backends/#rust-backend-fastest","title":"\ud83e\udd80 Rust Backend (Fastest)","text":"<ul> <li>Best performance - 2.5x faster than alternatives</li> <li>Parallel processing - automatic multi-threading</li> <li>Auto-selected - chosen by default when available</li> <li>Same API - drop-in replacement with identical output</li> </ul>"},{"location":"backends/#fd-backend-competitive","title":"\ud83d\udd0d fd Backend (Competitive)","text":"<ul> <li>Fast file discovery - leverages the <code>fd</code> command-line tool</li> <li>Advanced patterns - supports regex and glob patterns</li> <li>Hybrid approach - fd for discovery + Python for analysis</li> <li>Network optimized - often best for NFS filesystems</li> </ul>"},{"location":"backends/#automatic-selection","title":"Automatic Selection","text":"<pre><code>from filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\n\n# Automatically uses fastest available backend\nprofiler = DirectoryProfiler(DirectoryProfilerConfig())\nresult = profiler.probe(\"/path/to/directory\")\n\n# Check which backend was used\nprofiler.print_summary(result)\n# Shows: \"Directory Analysis: /path (\ud83e\udd80 Rust)\" or \"\ud83d\udd0d fd\" or \"\ud83d\udc0d Python\"\n</code></pre>"},{"location":"backends/#rust-async-network-optimized","title":"\ud83e\udd80 Rust Async (Network-optimized)","text":"<ul> <li>When: Automatically selected for network-mounted filesystems (NFS/CIFS/SMB/Gluster/SSHFS) when available.</li> <li>Why: Uses a tokio-based scanner with bounded concurrency to hide network latency and avoid overwhelming remote servers.</li> <li>Tuning: <code>DirectoryProfiler</code> accepts network tuning parameters:<ul> <li><code>network_concurrency</code> (int): maximum outstanding directory ops (default 64)</li> <li><code>network_timeout_ms</code> (int): per-operation timeout in milliseconds (default 5000)</li> <li><code>network_retries</code> (int): number of retries on transient failures (default 0)</li> </ul> </li> </ul> <p>Use these to tune behavior on slow or flaky mounts. Example:</p> <pre><code>profiler = DirectoryProfiler(DirectoryProfilerConfig(network_concurrency=32, network_timeout_ms=2000, network_retries=1))\n</code></pre> <p>If the async Rust backend isn't compiled into your wheel, filoma will fall back to the existing Rust or fd backends.</p>"},{"location":"backends/#manual-backend-selection","title":"Manual Backend Selection","text":"<pre><code># Force specific backend\nprofiler_rust = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"rust\"))\nprofiler_fd = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"fd\"))\nprofiler_python = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"python\"))\n\n# Check availability\nprint(f\"Rust available: {profiler_rust.is_rust_available()}\")\nprint(f\"fd available: {profiler_fd.is_fd_available()}\")\nprint(f\"Python available: True\")  # Always available\n</code></pre>"},{"location":"backends/#backend-comparison","title":"Backend Comparison","text":"<pre><code>import time\n\nbackends = [\"rust\", \"fd\", \"python\"]\nfor backend in backends:\n    profiler = DirectoryProfiler(DirectoryProfilerConfig(search_backend=backend, show_progress=False))\n    # Check if the specific backend is available\n    if ((backend == \"rust\" and profiler.is_rust_available()) or\n        (backend == \"fd\" and profiler.is_fd_available()) or\n        (backend == \"python\")):  # Python always available\n        start = time.time()\n        result = profiler.probe(\"/test/directory\")\n        elapsed = time.time() - start\n        files_per_sec = result['summary']['total_files'] / elapsed\n        print(f\"{backend}: {elapsed:.3f}s ({files_per_sec:,.0f} files/sec)\")\n</code></pre>"},{"location":"backends/#when-to-use-each-backend","title":"When to Use Each Backend","text":"Use Case Recommended Backend Why Large directories Auto (Rust preferred) Best overall performance Network filesystems <code>fd</code> Optimized for network I/O CI/CD environments Auto Reliable with graceful fallbacks Maximum compatibility <code>python</code> Always works, no dependencies DataFrame analysis Auto (Rust preferred) Fastest DataFrame building Pattern matching <code>fd</code> Advanced regex/glob support"},{"location":"backends/#technical-details","title":"Technical Details","text":"<p>All backends provide: - Identical APIs - same function signatures and parameters - Same output format - consistent data structures - Progress bars - real-time feedback for large operations - Error handling - graceful fallbacks and error reporting</p>"},{"location":"backends/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Rust: Best for CPU-intensive operations, parallel processing</li> <li>fd: Best for I/O-intensive operations, pattern matching</li> <li>Python: Most compatible, good baseline performance</li> </ul>"},{"location":"backends/#backend-detection","title":"Backend Detection","text":"<pre><code>from filoma.directories import DirectoryProfiler\nfrom filoma.core import FdIntegration\n\n# Check what's available\nprofiler = DirectoryProfiler(DirectoryProfilerConfig())\nfd = FdIntegration()\n\nprint(\"Available backends:\")\nprint(f\"  \ud83d\udc0d Python: Always available\")\nprint(f\"  \ud83e\udd80 Rust: {'\u2705' if profiler.use_rust else '\u274c'}\")\nprint(f\"  \ud83d\udd0d fd: {'\u2705' if fd.is_available() else '\u274c'}\")\n\nif fd.is_available():\n    print(f\"  fd version: {fd.get_version()}\")\n</code></pre>"},{"location":"benchmarks/","title":"Performance Benchmarks","text":"<p><code>DISCLAIMER</code>: Benchmark results are illustrative and may vary based on your hardware, filesystem, and directory structure. Always run your own benchmarks on your target systems for accurate performance data. They were run during the early stages of <code>filoma</code> development and may not reflect the latest optimizations.  </p>"},{"location":"benchmarks/#test-environment","title":"Test Environment","text":"<p>All performance data measured on the following system:</p> <pre><code>OS:         Linux x86_64 (Ubuntu-based)\nStorage:    WD_BLACK SN770 2TB NVMe SSD (Sandisk Corp)\nFilesystem: ext4 (non-NFS, local storage)\nMemory:     High-speed access to NVMe storage\nCPU:        Multi-core with parallel processing support\n</code></pre> <p>\ud83d\udcca Why This Matters: SSD vs HDD performance can vary dramatically. NVMe SSDs provide  exceptional random I/O performance that benefits all backends. Network filesystems (NFS)  may show different characteristics. Your mileage may vary based on storage type.</p>"},{"location":"benchmarks/#benchmark-methodology","title":"Benchmark Methodology","text":""},{"location":"benchmarks/#cold-cache-testing","title":"\u2744\ufe0f Cold Cache Testing","text":"<p>Critical: All benchmarks use cold cache methodology to represent real-world performance:</p> <pre><code># Before each test:\nsync                                    # Flush buffers\necho 3 &gt; /proc/sys/vm/drop_caches      # Clear filesystem cache\n</code></pre> <p>\ud83d\udd25 Cache Impact: OS filesystem cache can make benchmarks 2-8x faster but unrealistic.  Warm cache results don't represent first-time directory access. Our cold cache benchmarks  show realistic performance for real-world usage.</p>"},{"location":"benchmarks/#test-dataset","title":"Test Dataset","text":"<ul> <li>Directory: <code>/usr</code> (system directory with diverse file types)</li> <li>Files: ~250,000 files</li> <li>Depth: Multiple levels of nested directories</li> <li>Size Range: Small config files to large binaries</li> </ul>"},{"location":"benchmarks/#performance-results","title":"Performance Results","text":""},{"location":"benchmarks/#file-discovery-performance-fast-path","title":"File Discovery Performance (Fast Path)","text":"<p>Cold cache benchmarks - File path discovery only</p> <pre><code>Backend      \u2502 Time      \u2502 Files/sec  \u2502 Relative Speed\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nRust         \u2502 3.16s     \u2502 70,367     \u2502 2.28x faster\nfd           \u2502 4.80s     \u2502 46,244     \u2502 1.50x faster  \nPython       \u2502 8.11s     \u2502 30,795     \u2502 1.00x (baseline)\n</code></pre>"},{"location":"benchmarks/#dataframe-building-performance","title":"DataFrame Building Performance","text":"<p>Cold cache benchmarks - Full metadata collection with DataFrame creation</p> <pre><code>Backend      \u2502 Time      \u2502 Files/sec  \u2502 Relative Speed\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nRust         \u2502 4.16s     \u2502 53,417     \u2502 1.95x faster\nfd           \u2502 4.80s     \u2502 46,219     \u2502 1.50x faster\nPython       \u2502 8.13s     \u2502 30,733     \u2502 1.00x (baseline)\n</code></pre>"},{"location":"benchmarks/#key-insights","title":"Key Insights","text":"<ul> <li>\ud83e\udd80 Rust fastest overall - Best performance for both file discovery and DataFrame building</li> <li>\ud83d\udd0d fd competitive - Close second, excellent alternative when Rust isn't available  </li> <li>\ud83d\udc0d Python most compatible - Works by default, reliable fallback option</li> <li>\ud83d\udcca Identical results - All backends produce the same analysis output and metadata</li> <li>\u2744\ufe0f Cold vs warm cache - Real performance is 2-8x slower than cached results</li> <li>\ud83c\udfaf Automatic selection - filoma chooses the optimal backend for your system</li> </ul>"},{"location":"benchmarks/#network-storage-performance","title":"Network Storage Performance","text":"<p>\ud83d\udcca Network Storage Note: In NFS environments, <code>fd</code> often outperforms other backends due to  optimized network I/O patterns. For network filesystems, consider forcing the <code>fd</code> backend  with <code>DirectoryProfiler(search_backend=\"fd\")</code> for optimal performance.</p>"},{"location":"benchmarks/#benchmarking-best-practices","title":"Benchmarking Best Practices","text":""},{"location":"benchmarks/#accurate-performance-testing","title":"Accurate Performance Testing","text":"<pre><code>import subprocess\nimport time\nfrom filoma.directories import DirectoryProfiler\n\ndef clear_filesystem_cache():\n    \"\"\"Clear OS filesystem cache for realistic benchmarks.\"\"\"\n    subprocess.run(['sync'], check=True)\n    subprocess.run(['sudo', 'tee', '/proc/sys/vm/drop_caches'], \n                   input='3\\n', text=True, stdout=subprocess.DEVNULL, check=True)\n    time.sleep(1)  # Let cache clear settle\n\ndef benchmark_backend(backend_name, path, iterations=3):\n    \"\"\"Benchmark a specific backend with cold cache.\"\"\"\n    profiler = DirectoryProfiler(DirectoryProfilerConfig(search_backend=backend_name, show_progress=False))\n\n    # Check if the specific backend is available\n    available = ((backend_name == \"rust\" and profiler.is_rust_available()) or\n                (backend_name == \"fd\" and profiler.is_fd_available()) or\n                (backend_name == \"python\"))  # Python always available\n    if not available:\n        return None\n\n    times = []\n    for i in range(iterations):\n        clear_filesystem_cache()\n        start = time.time()\n    result = profiler.probe(path)\n        elapsed = time.time() - start\n        times.append(elapsed)\n\n    avg_time = sum(times) / len(times)\n    files_per_sec = result['summary']['total_files'] / avg_time\n\n    return {\n        'backend': backend_name,\n        'avg_time': avg_time,\n        'files_per_sec': files_per_sec,\n        'total_files': result['summary']['total_files']\n    }\n\n# Example usage\nresults = []\nfor backend in ['rust', 'fd', 'python']:\n    result = benchmark_backend(backend, '/test/directory')\n    if result:\n        results.append(result)\n        print(f\"{backend}: {result['avg_time']:.3f}s ({result['files_per_sec']:.0f} files/sec)\")\n\n# Find fastest\nif results:\n    fastest = min(results, key=lambda x: x['avg_time'])\n    print(f\"\\n\ud83c\udfc6 Fastest: {fastest['backend']}\")\n</code></pre>"},{"location":"benchmarks/#performance-tips","title":"Performance Tips","text":"<ol> <li>Disable progress bars for benchmarking: <code>show_progress=False</code></li> <li>Use fast path only for discovery benchmarks: <code>fast_path_only=True</code></li> <li>Clear filesystem cache between runs for realistic results</li> <li>Run multiple iterations and average the results</li> <li>Test on your target storage - results vary by filesystem type</li> </ol>"},{"location":"benchmarks/#warm-vs-cold-cache-comparison","title":"Warm vs Cold Cache Comparison","text":"<pre><code># Cold cache (realistic)\nclear_filesystem_cache()\nstart = time.time()\nresult = profiler.probe(\"/test/directory\")\ncold_time = time.time() - start\n\n# Warm cache (for comparison only)\nstart = time.time()\nresult = profiler.probe(\"/test/directory\")  \nwarm_time = time.time() - start\n\nprint(f\"Cold cache: {cold_time:.3f}s (realistic)\")\nprint(f\"Warm cache: {warm_time:.3f}s (cached, {cold_time/warm_time:.1f}x slower when cold)\")\n</code></pre> <p>\u26a0\ufe0f Important: Always use cold cache for realistic benchmarks. Warm cache results can be  2-8x faster but don't represent real-world performance for first-time directory access.</p>"},{"location":"benchmarks/#backend-selection-recommendations","title":"Backend Selection Recommendations","text":"Use Case Recommended Backend Why Large directories Auto (Rust if available) Best overall performance Network filesystems <code>fd</code> Optimized for network I/O CI/CD environments Auto Reliable with graceful fallbacks Maximum compatibility <code>python</code> Always works, no dependencies DataFrame analysis Auto (Rust if available) Fastest DataFrame building Pattern matching <code>fd</code> Advanced regex/glob support"},{"location":"benchmarks/#your-results-may-vary","title":"Your Results May Vary","text":"<p>Performance depends on: - Storage type - NVMe SSD &gt; SATA SSD &gt; HDD - Filesystem - ext4, NTFS, APFS, NFS all behave differently - Directory structure - Deep vs wide, file size distribution - System load - CPU, memory, I/O contention - Network latency - Critical for NFS/network storage</p> <p>Run your own benchmarks on your target systems for accurate performance data.</p>"},{"location":"concepts/","title":"Core Concepts","text":"<p><code>filoma</code> focuses on fast, ergonomic filesystem analysis. Four high-level helpers cover 90% of use cases:</p> Helper Purpose Returns <code>probe(path)</code> Analyze a directory (or dispatch to file) DirectoryAnalysis or File dataclass <code>probe_to_df(path)</code> Analyze + return filoma.DataFrame wrapper filoma.DataFrame <code>probe_file(path)</code> Single file metadata File dataclass <code>probe_image(path|ndarray)</code> Image stats/metadata ImageReport <p>Key object types: - DirectoryAnalysis: structured dict-like result with summary + counts. - filoma.DataFrame: thin wrapper over Polars with filesystem helpers. - File dataclass (Filo): metadata (size, ownership, times, hash, etc.). - ImageReport: metadata + numeric stats for images / arrays.</p> <p>Backends (auto-selected): Rust &gt; fd &gt; Python. You usually don\u2019t choose manually.</p> <p>Design goals: - Minimal surface: few verbs, predictable results. - Opt-in cost: heavy metadata/hash only if you ask. - DataFrame-first: move seamlessly into Polars for analysis. - Deterministic ML splits: grouping-aware to prevent leakage.</p> <p>When in doubt: start with <code>probe('.')</code>, then <code>probe_to_df('.')</code> if you need tabular work.</p>"},{"location":"cookbook/","title":"Cookbook","text":"<p>Practical, copy\u2011paste recipes for common tasks.</p>"},{"location":"cookbook/#top-n-largest-files","title":"Top N Largest Files","text":"<pre><code>from filoma import probe_to_df\nimport polars as pl\n\n# filoma-first: probe_to_df returns a filoma.DataFrame wrapper\ndfw = probe_to_df('.')\n# If you need the raw polars.DataFrame use `dfw.df` or `dfw.to_polars()`\nlargest = dfw.df.select(['path','size_bytes']).sort('size_bytes', descending=True).head(10)\nprint(largest)\n</code></pre>"},{"location":"cookbook/#extension-distribution","title":"Extension Distribution","text":"<pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')\n(by_ext := dfw.df.groupby('suffix').count().sort('count', descending=True).head(15))\n</code></pre>"},{"location":"cookbook/#count-files-per-directory-depth-1","title":"Count Files per Directory (Depth 1)","text":"<pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')\n# add parent using the raw polars DataFrame via `dfw.df`\ncounts = dfw.df.with_columns(dfw.df['path'].str.split('/').list.slice(-2,1).alias('parent')).groupby('parent').count().sort('count', descending=True)\n</code></pre>"},{"location":"cookbook/#filter-only-python-sources","title":"Filter Only Python Sources","text":"<pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')\n# polars-style filter on the underlying DataFrame\npy = dfw.df.filter(dfw.df['path'].str.ends_with('.py'))\n</code></pre>"},{"location":"cookbook/#add-file-metadata-later-lazy-enrichment","title":"Add File Metadata Later (Lazy Enrichment)","text":"<pre><code>from filoma import probe_to_df\nfrom filoma.dataframe import DataFrame\n# If probe_to_df returns a wrapper, pass its raw polars via .df when needed\nbase = DataFrame(probe_to_df('.', enrich=False).df)\nwith_stats = base.add_file_stats_cols()  # adds size, times, owner, etc.\n</code></pre>"},{"location":"cookbook/#fast-path-discovery-skip-metadata","title":"Fast Path Discovery (Skip Metadata)","text":"<pre><code>from filoma.directories import DirectoryProfiler\n# Prefer the convenience helper when you want a filoma.DataFrame wrapper:\ndfw = probe_to_df('.', enrich=False)\n\n# Or explicitly enable DataFrame building when using DirectoryProfiler:\nanalysis = DirectoryProfiler(DirectoryProfilerConfig(fast_path_only=True, build_dataframe=True)).probe('.')\npaths_df = analysis.to_df().df\n</code></pre>"},{"location":"cookbook/#detect-recently-modified-files-last-24h","title":"Detect Recently Modified Files (last 24h)","text":"<pre><code>from datetime import datetime, timedelta\nfrom filoma import probe_to_df\ndfw = probe_to_df('.')\ncutoff = datetime.utcnow() - timedelta(hours=24)\n# Work on the polars DataFrame via `dfw.df`\nrecent = dfw.df.filter(dfw.df['modified_time'] &gt; cutoff.isoformat())\n</code></pre>"},{"location":"cookbook/#trainvaltest-split-701515","title":"Train/Val/Test Split (70/15/15)","text":"<pre><code>from filoma import probe_to_df, ml\ndfw = probe_to_df('.')\n# preferred: pass the filoma.DataFrame wrapper to ml.split_data\ntrain, val, test = ml.split_data(dfw, train_val_test=(70,15,15), seed=42)\n</code></pre>"},{"location":"cookbook/#group-split-by-parent-folder","title":"Group Split by Parent Folder","text":"<pre><code>from filoma import probe_to_df, ml\ndfw = probe_to_df('.')\ntrain, val, test = ml.split_data(dfw, how='parts', parts=(-2,), seed=42)\n</code></pre>"},{"location":"cookbook/#discover-filename-tokens-then-split","title":"Discover Filename Tokens Then Split","text":"<pre><code>from filoma import probe_to_df\nfrom filoma.dataframe import DataFrame\nbase = DataFrame(probe_to_df('.').df)\n# Preferred: use the DataFrame method\ndf = base.add_filename_features(sep='_')\ntrain, val, test = df.split_data(feature=('feat1',))\n</code></pre> <p>Use <code>DataFrame.add_filename_features(...)</code> to discover filename tokens; it returns a <code>filoma.DataFrame</code> wrapper.</p>"},{"location":"cookbook/#export-for-downstream-processing","title":"Export for Downstream Processing","text":"<pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')\n# Use wrapper's convenience save methods or raw polars via `dfw.df`\ndfw.save_parquet('files.parquet')\ndfw.save_csv('files.csv')\n</code></pre>"},{"location":"cookbook/#profile-subset-eg-only-large-images","title":"Profile Subset (e.g., Only Large Images)","text":"<pre><code>from filoma import probe_to_df, probe_image\nimport polars as pl\ndfw = probe_to_df('.')\nimages = dfw.df.filter(dfw.df['suffix'].is_in(['.png','.tif','.npy']))\nlarge = images.filter(images['size_bytes'] &gt; 5_000_000)\nreports = [probe_image(p) for p in large['path'].to_list()]\n</code></pre>"},{"location":"cookbook/#compute-sha256-for-selected-files","title":"Compute SHA256 for Selected Files","text":"<pre><code>from filoma import probe_file\nimport hashlib\npaths = ['README.md','pyproject.toml']\nrows = []\nfor p in paths:\n    filo = probe_file(p, compute_hash=True)\n    rows.append({'path': filo.path, 'sha256': filo.sha256})\n</code></pre>"},{"location":"cookbook/#simple-duplicate-finder-by-size-then-hash","title":"Simple Duplicate Finder (by size then hash)","text":"<pre><code>from filoma import probe_to_df, probe_file\nimport collections\ndfw = probe_to_df('.')\n# coarse group by file size (use raw polars via `dfw.df`)\ncand = dfw.df.groupby('size_bytes').count().filter(dfw.df['count']&gt;1)['size_bytes'].to_list()\nsubset = dfw.df.filter(dfw.df['size_bytes'].is_in(cand))\n# compute hashes only for candidates\nhash_map = collections.defaultdict(list)\nfor path in subset['path'].to_list():\n    filo = probe_file(path, compute_hash=True)\n    hash_map[filo.sha256].append(path)\nduplicates = [v for v in hash_map.values() if len(v) &gt; 1]\n</code></pre>"},{"location":"cookbook/#depth-histogram","title":"Depth Histogram","text":"<pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')\ndfw.df.groupby('depth').count().sort('depth')\n</code></pre>"},{"location":"cookbook/#largest-directories-by-file-count","title":"Largest Directories by File Count","text":"<pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')\n# parent column (quick derivation) using the raw polars DataFrame\nparents = dfw.df.with_columns(dfw.df['path'].str.split('/').list.slice(-2,1).alias('parent'))\nparents.groupby('parent').count().sort('count', descending=True).head(20)\n</code></pre>"},{"location":"cookbook/#smart-file-search-with-fdfinder","title":"Smart File Search with FdFinder","text":"<p>The <code>FdFinder</code> class provides a powerful way to search for files using regular expressions and glob patterns.</p> <pre><code>from filoma.directories import FdFinder\n\nfinder = FdFinder()\n\n# Find all Python files\npython_files = finder.find_files(pattern=r\"\\.py$\")\n\n# Find files by multiple extensions\ncode_files = finder.find_by_extension(['py', 'rs', 'js'])\n\n# Find files using a glob pattern\nconfig_files = finder.find_files(pattern=\"*.{json,yaml}\", use_glob=True)\n\nprint(f\"Found {len(python_files)} Python files.\")\n</code></pre> <p>Add a recipe request via an issue if something common is missing.</p>"},{"location":"data-quality/","title":"Data Quality","text":"<p>This page documents the duplicate-detection and near-duplicate matching utilities available in <code>filoma</code>. Use these helpers to evaluate dataset quality, find exact duplicates, and detect near-duplicates in text and images before training or analysis.</p>"},{"location":"data-quality/#notebook-tutorial","title":"Notebook tutorial","text":"<p>Open the rendered demo in a new tab </p>"},{"location":"dataframe/","title":"DataFrame Workflow","text":"<p>Get a Polars DataFrame directly: <pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')  # filoma.DataFrame wrapper\nprint(dfw.head())\n</code></pre></p> <p>Wrap existing analysis: <pre><code>from filoma import probe\nanalysis = probe('.')\n# Prefer using probe_to_df() to get a DataFrame in one step. If you already\n# have an analysis object, request a DataFrame explicitly when probing and\n# access the raw Polars DataFrame via `wrapper.df` when needed.\nanalysis = probe('.', build_dataframe=True)\nwrapper = analysis.to_df()\n# access raw polars via wrapper.df\n</code></pre></p> <p>Enrichment helpers (chainable): <pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.', enrich=True)  # depth, path parts, file stats\n</code></pre></p> <p>Manual enrichment: <pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.', enrich=False)\nfrom filoma.dataframe import DataFrame\nwrapper = DataFrame(dfw.df)\nwrapper = wrapper.add_depth_col().add_path_components().add_file_stats_cols()\n</code></pre></p> <p>Filtering &amp; grouping: <pre><code>wrapper.filter_by_extension('.py')\nwrapper.group_by_extension()\nwrapper.group_by_directory()\n</code></pre></p> <p>Export: <pre><code>wrapper.save_csv('files.csv')\nwrapper.save_parquet('files.parquet')\n</code></pre></p> <p>Convert to pandas: <pre><code>pandas_df = probe_to_df('.', to_pandas=True)\n</code></pre></p> <p>Tips: - Use <code>.add_file_stats_cols()</code> sparingly on huge trees (it touches filesystem for each path). Pandas conversions and caching</p> <p>filoma is Polars-first internally. For pandas interop use the following:</p> <ul> <li><code>df.pandas</code> \u2014 always returns a fresh pandas.DataFrame conversion.</li> <li><code>df.pandas_cached</code> or <code>df.to_pandas(force=False)</code> \u2014 returns a cached pandas     conversion (converted once). Use for repeated reads.</li> <li><code>df.to_pandas(force=True)</code> \u2014 force reconversion and update the cache.</li> <li><code>df.invalidate_pandas_cache()</code> \u2014 clear the cached pandas object.</li> </ul> <p>The wrapper automatically invalidates the cached pandas conversion on assignments (<code>df[...] = ...</code>) and when delegated Polars methods appear to mutate in-place (Polars commonly returns <code>None</code> or the same DataFrame object). For complex external mutations call <code>invalidate_pandas_cache()</code> or <code>to_pandas(force=True)</code> to ensure freshness.</p> <ul> <li>Combine with Polars expressions for advanced analysis.</li> </ul>"},{"location":"dedup/","title":"Dedup","text":"<p>Duplicate Detection</p> <p>This project includes a lightweight duplicate-detection helper available at <code>src/filoma/dedup.py</code>.</p> <ul> <li>Exact duplicates: <code>compute_sha256(path)</code> and <code>find_duplicates(paths)</code> detect byte-for-byte duplicates.</li> <li>Text near-duplicates: uses k-shingles and Jaccard similarity. Configure <code>text_k</code> and <code>text_threshold</code>.</li> <li>Image near-duplicates: basic perceptual hashing (aHash/dHash) using Pillow. Configure <code>image_hash</code> and <code>image_max_distance</code>.</li> </ul> <p>Examples:</p> <pre><code>from filoma import dedup\n\nfiles = [\"data/a.jpg\", \"data/b.jpg\", \"data/c.txt\"]\nres = dedup.find_duplicates(files, text_threshold=0.8, image_max_distance=6)\nprint(res[\"exact\"])  # exact matches\nprint(res[\"text\"])   # near-duplicate text groups\nprint(res[\"image\"])  # near-duplicate image groups\n</code></pre> <p>Optional dependencies:</p> <ul> <li><code>Pillow</code> \u2014 recommended for image hashing.</li> <li><code>datasketch</code> \u2014 optional for MinHash acceleration on large text datasets.</li> </ul>"},{"location":"demo/","title":"filoma demo","text":"<p>Explore the interactive demo notebook showing common <code>filoma</code> workflows (file probing, image profiling, directory analysis, DataFrame conversion, and ML-ready splits).</p> <ul> <li>Notebook source: <code>notebooks/demo.ipynb</code></li> <li>Rendered view: <code>docs/demo.html</code></li> </ul> <p>Below is the rendered notebook \u2014 scroll through the notebook in-place or open it in a new tab using the link under the frame.</p> <p>Open the rendered demo in a new tab</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#quick-start","title":"Quick Start","text":"<pre><code># \ud83d\ude80 RECOMMENDED: Using uv (modern, fast Python package manager)\nuv add filoma\n\n# Traditional method\npip install filoma\n</code></pre>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#for-uv-projects-recommended","title":"For uv Projects (Recommended)","text":"<pre><code># Install uv first if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add to your project\nuv add filoma\n</code></pre>"},{"location":"installation/#for-scripts-or-standalone-use","title":"For Scripts or Standalone Use","text":"<pre><code>uv pip install filoma\n</code></pre>"},{"location":"installation/#traditional-pip","title":"Traditional pip","text":"<pre><code>pip install filoma\n</code></pre>"},{"location":"installation/#performance-optimization-optional","title":"Performance Optimization (Optional)","text":""},{"location":"installation/#option-1-rust-backend-25x-faster","title":"Option 1: Rust Backend (2.5x faster)","text":"<pre><code># Install Rust toolchain\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource ~/.cargo/env\n\n# Reinstall to build Rust extension\nuv add filoma --force  # or: pip install --force-reinstall filoma\n</code></pre>"},{"location":"installation/#option-2-fd-command-competitive-alternative","title":"Option 2: fd Command (Competitive Alternative)","text":"<pre><code># Ubuntu/Debian\nsudo apt install fd-find\n\n# macOS\nbrew install fd\n\n# Other systems: https://github.com/sharkdp/fd#installation\n</code></pre>"},{"location":"installation/#performance-tiers","title":"Performance Tiers","text":"<ul> <li>Basic: Pure Python (works everywhere, ~30K files/sec)</li> <li>Fast: + fd command (competitive alternative, ~46K files/sec)</li> <li>Fastest: + Rust backend (best performance, ~70K files/sec, auto-selected)</li> </ul>"},{"location":"installation/#verification","title":"Verification","text":"<pre><code>import filoma\nfrom filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\n\nprint(f\"filoma version: {filoma.__version__}\")\n\n# Check available backends via a typed profiler\nprofiler = DirectoryProfiler(DirectoryProfilerConfig())\nprint(f\"\ud83e\udd80 Rust: {'\u2705' if profiler.use_rust else '\u274c'}\")\nprint(f\"\ud83d\udd0d fd: {'\u2705' if profiler.use_fd else '\u274c'}\")\n\n# Quick test using the top-level helper\nfrom filoma import probe\nresult = probe('.')\nprint(f\"\u2705 Found {result['summary']['total_files']} files\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#system-directory-issues","title":"System Directory Issues","text":"<p>When analyzing system directories (like <code>/</code>, <code>/proc</code>, <code>/sys</code>), you might encounter permission errors. filoma handles this gracefully:</p> <pre><code>from filoma.directories import DirectoryProfiler\n\n# Safe analysis with automatic fallbacks\nprofiler = DirectoryProfiler(DirectoryProfilerConfig())\n\n# This will automatically fall back to Python implementation if Rust fails\nresult = profiler.probe(\"/proc\", max_depth=2)\n\n# For maximum compatibility with system directories, use Python backend\nprofiler_safe = DirectoryProfiler(DirectoryProfilerConfig(search_backend=\"python\"))\nresult = profiler_safe.probe(\"/\", max_depth=3)\n</code></pre>"},{"location":"installation/#common-issues","title":"Common Issues","text":"<p>Permission denied errors: <pre><code># Run with limited depth to avoid deep system directories\npython -c \"from filoma import probe; print(probe('/', max_depth=2)['summary'])\"\n</code></pre></p> <p>Memory issues with large directories: <pre><code># Use fast_path_only for path discovery without metadata\nprofiler = DirectoryProfiler(DirectoryProfilerConfig(fast_path_only=True, build_dataframe=False))\nresult = profiler.probe(\"/large/directory\")\n</code></pre></p> <p>Progress bar issues in Jupyter: Progress bars are automatically disabled in interactive environments (IPython/Jupyter) to avoid conflicts.</p>"},{"location":"life-after-filoma/","title":"Life after filoma","text":"<p>Here are some tools that work well with <code>filoma</code> for further data analysis and machine learning tasks.  </p> <ol> <li>ydata-profiling: Generate detailed profiling reports from <code>filoma</code> dataframes to understand data distributions, correlations, and potential issues through interactive HTML reports with visualizations and statistics.  Open demo in a new tab </li> <li>More to be added soon!</li> </ol>"},{"location":"ml/","title":"ML Splits","text":"<p>Deterministic grouping-aware splits to avoid leakage.</p> <p>Basic usage: <pre><code>from filoma import probe_to_df, ml\ndfw = probe_to_df('.')  # filoma.DataFrame wrapper\n# preferred: pass the filoma.DataFrame wrapper\ntrain, val, test = ml.split_data(dfw, train_val_test=(70,15,15), feature='path_parts')\n</code></pre></p> <p>Group by filename tokens: <pre><code># Preferred: use the DataFrame method which discovers filename tokens\n# and returns a filoma.DataFrame (or use `inplace=True`).\ndf = df.add_filename_features(sep='_')\ntrain, val, test = df.split_data(feature=('feat1',))\n</code></pre></p> <p>Use the <code>DataFrame.add_filename_features(...)</code> instance method to discover filename tokens; it returns a <code>filoma.DataFrame</code> wrapper.</p> <p>Group by path parts (e.g., parent folder): <pre><code>train, val, test = ml.split_data(dfw, feature='path_parts', path_parts=(-2,))\n</code></pre></p> <p>Return different types: <pre><code>train_f, val_f, test_f = ml.split_data(dfw, return_type='filoma')\n</code></pre></p> <p>Tips: - Provide a seed to stabilize: <code>seed=42</code>. - Ratios may slightly drift; warnings explain adjustments. - Use <code>return_type='pandas'</code> if you prefer pandas downstream.</p>"},{"location":"profiling/","title":"File &amp; Image Profiling","text":"<p>Single file: <pre><code>from filoma import probe_file\ninfo = probe_file('README.md')\nprint(info.size, info.modified)\n</code></pre></p> <p>Image: <pre><code>from filoma import probe_image\nimg = probe_image('images/logo.png')\nprint(img.file_type, getattr(img, 'shape', None))\n</code></pre></p> <p>Numpy array: <pre><code>import numpy as np\nfrom filoma import probe_image\narr = np.zeros((64,64), dtype=np.uint8)\nrep = probe_image(arr)\nprint(rep.mean, rep.max)\n</code></pre></p> <p>Disable hash for speed: <pre><code>probe_file('big.bin', compute_hash=False)\n</code></pre></p> <p>Batch profile selected files via DataFrame: <pre><code>from filoma import probe_to_df\ndfw = probe_to_df('.')  # returns filoma.DataFrame wrapper\nwrapper = dfw.filter_by_extension('.py').add_file_stats_cols()\n</code></pre></p> <p>What you get (file dataclass key fields): - path, size, owner, group, mode_str, created, modified, is_file, is_dir, sha256 (optional), inode</p> <p>ImageReport common fields: - path, file_type, shape, dtype, min, max, mean, nans, infs</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p><code>filoma</code> is a fast and flexible Python tool for filesystem analysis. It helps you understand the contents of your directories, profile individual files, and prepare datasets for machine learning.</p>"},{"location":"quickstart/#getting-started-the-interactive-demo","title":"Getting Started: The Interactive Demo","text":"<p>The best way to get started with <code>filoma</code> is to run the interactive demo notebook. It covers the most common workflows in a hands-on way.</p> <ul> <li>View the Interactive Demo</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install <code>filoma</code> and its dependencies using <code>uv</code> or <code>pip</code>:</p> <pre><code># Recommended: using uv\nuv pip install filoma\n\n# Or with pip\npip install filoma\n</code></pre>"},{"location":"quickstart/#basic-usage-scan-a-directory","title":"Basic Usage: Scan a Directory","text":"<p>The most common use case is scanning a directory to see what's inside. The <code>probe_to_df</code> function scans a path and returns a Polars DataFrame, which is great for interactive analysis.</p> <pre><code>from filoma import probe_to_df\n\n# Scan the current directory and get a DataFrame\ndf = probe_to_df('.')\n\n# Print the first few rows\nprint(df.head())\n</code></pre> <p>This will give you a table with information about each file, like its path, size, and modification time.</p>"},{"location":"quickstart/#profile-a-single-file","title":"Profile a Single File","text":"<p>You can also get detailed information about a single file using <code>probe_file</code>:</p> <pre><code>from filoma import probe_file\n\n# Profile the README.md file\nfile_info = probe_file('README.md')\n\n# Print the file's properties\nprint(file_info.as_dict())\n</code></pre>"},{"location":"quickstart/#key-features","title":"Key Features","text":"<ul> <li>Fast Scans: Uses a Rust backend and <code>fd</code> for high-performance directory traversal.</li> <li>DataFrame-First: Easily integrates with the Polars for powerful data manipulation and analysis.</li> <li>Image Profiling: Extracts metadata from images.</li> <li>ML-Ready: Provides tools for creating deterministic train/validation/test splits from your file data.</li> <li>Lazy Loading: <code>import filoma</code> is fast and lightweight. Dependencies like <code>polars</code> and <code>Pillow</code> are loaded on demand.</li> </ul>"},{"location":"quickstart/#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>Cookbook: Find copy-paste recipes for common tasks.</li> <li>Concepts: Learn about the core ideas behind <code>filoma</code>.</li> <li>DataFrame Workflow: See how to work with <code>filoma</code>'s DataFrame output.</li> </ul>"},{"location":"scanning/","title":"Directory Scanning","text":"<p>Basic scan: <pre><code>from filoma import probe\nanalysis = probe('.')\nanalysis.print_summary()\n</code></pre></p> <p>Report: <pre><code>analysis.print_report()\n</code></pre></p> <p>Limit depth &amp; hide progress: <pre><code>analysis = probe('.', max_depth=3, show_progress=False)\n</code></pre></p> <p>Select backend explicitly (rarely needed): <pre><code>from filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\nDirectoryProfiler(DirectoryProfilerConfig(search_backend='fd')).probe('.')\n</code></pre></p> <p>Fast path only (paths without metadata): <pre><code>from filoma.directories import DirectoryProfiler, DirectoryProfilerConfig\nfast = DirectoryProfiler(DirectoryProfilerConfig(fast_path_only=True)).probe('.')\n</code></pre></p> <p>Network tuning example: <pre><code>DirectoryProfiler(DirectoryProfilerConfig(network_concurrency=32, network_timeout_ms=2000)).probe('/mnt/nfs')\n</code></pre></p> <p>Common flags: - <code>max_depth</code>: limit recursion. - <code>search_hidden</code>: include dotfiles. - <code>follow_links</code>: follow symlinks. - <code>fast_path_only</code>: skip metadata.</p> <p>Inspect raw structure (dict-like): <pre><code>print(analysis['summary'])\nprint(list(analysis['file_extensions'].items())[:5])\n</code></pre></p>"}]}